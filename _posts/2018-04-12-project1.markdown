---
layout: post
title:  Welcome!
date:   2018-04-12
author: Miguel
categories: projects
---

# Deadline Project1 02-05-2018

Tutorial for theory of [IBM](https://uva-slpl.github.io/nlp2/resources/papers/Schulz-IBM12-Tutorial.pdf)

# Tips

If you are using dense arrays and are running out of memory, sort your vocabulary by frequency and discard all but the top K types by mapping the least frequent ones to some symbol e.g. -UNK-, this way you get to reduce your vocabulary size.

If the validation (or test) set contains words that were never seen at training time, there are two solutions:
You can map all words that occurred a single time at training time to a symbol, e.g. -LOW-, then whenever you encounter an unseen word in the future, replace it by that symbol (also note that if you employed strategy 1 above, you already have a solution to this problem, simply map it to -UNK-);

Alternatively, note that word alignment is a fully unsupervised problem, thus we typically do not make a distinction between training/validation/test sets, this time we are doing it, just so you can track AER scores. This amounts to saying that it is not cheating to concatenate the validation set and the training set when inducing your unsupervised alignments. If still that makes you uneasy, then go with the suggestion above.

In IBM2, one can have a special jump event for alignments to NULL, this should lead to some small improvements (in log-likelihood and AER). The intuition is that alignments to NULL may imply very long jumps, collapsing all alignments to NULL reduce sparsity.

Leave out NULL-alignments from your output, since they are not included in the annotated (test) data. You will only get penalized for them.

If your log-likelihoods are different (e.g. lower) than the ones reported here, you might want to check if you are aligning in the right direction. We are assuming that French words are generated by English words, and that the NULL word is therefore on the English side.

## IBM2 jump function

If you want to implement a cheaper parametrization of the jump distribution [Vogel et al., 1996](http://www.aclweb.org/anthology/C96-2141):

Init of jump distribution:

Array jump_dist[0, 2*max_jump]

with e.g. max_jump = 100

	jump_dist = 1. / (2 * max_jump) * ones([(1, 2 * max_jump)])

```
jump_func(i, j, l, m):
		"""
		Alignment of french word j to english word i. 
		i = 0, to ,l
		j = 1, to ,m
		That is: a_j = i
		with e.g. max_jump = 100
		from[-max_jump, max_jump] to [0, 2*max_jump] 
		"""
		jump = int(i - floor(j * l / m)) + max_jump 
		if jump >= 2 * max_jump:
			return max_jump - 1
		if jump < 0:
			return 0
		else:
			return jump
```     
Example of use:

wth, jump_dist[0,..,2*max_jump]

...

delta = t[(e,f)] * jump_dist[0, jump_func(j, i, l, m)]

...



## Log likelihood
Formula (14) in [tutorial](https://uva-slpl.github.io/nlp2/resources/papers/Schulz-IBM12-Tutorial.pdf) of dataset for current  parameters:

	log p(f_1^m, a_1^m, m | e_0^l) \propto sum_{j=1}^m log p(i | j, m, l) + log p(f_j | e_{a_j})

For example the alignment distributon in IBM 1:

	p(a_j | m, l)  = log(1 / (l+1)^m) 

	= -m * log(l + 1)

Aligment distributon IBM 2:

	p(i | j, m, l) = jump(i, j, l, m) 

## Bayesian IBM

Bayesian IBM [Tutorial](https://uva-slpl.github.io/nlp2/resources/papers/Schulz-BayesIBM1-tutorial.pdf)

Dirichlet [notebook](https://github.com/uva-slpl/nlp2/tree/gh-pages/resources/notebooks/Dirichlet.ipynb)

# NAACL alignment format

## Output format

The results file should include one line for each word-to-word alignment 
identified by the system. The lines in the results file should follow the 
format below:     

sentence_no position_L1 position_L2 [S|P] 

where:
- sentence_no represents the id of the sentence within the test file. 
Sentences in the test data already have an id assigned. (see the examples 
below)    

- position_L1 represents the position of the token that is aligned from 
the text in language L1; the first token in each sentence is token 1. (not 0)    

- position_L2 represents the position of the token that is aligned from the 
text in language L2; again, the first token is token 1.    

- S|P can be either S or P, representing a Sure or Probable alignment. All 
alignments that are tagged as S are also considered to be part of the P 
alignments set (that is, all alignments that are considered "Sure" alignments 
are also part of the "Probable" alignments set). If the S|P field is missing, 
a value of S will be assumed by default.

The S|P field overlap is optional. 

## Running example

Consider the two following aligned sentences:

[from the English file]

They had gone . 

[from the French file]

Ils etaient alles .

A correct word alignment that will be produced for this sentence is

18 1 1

18 2 2 

18 3 3

18 4 4

Which states that all these alignments are from sentence 18, and the English 
token 1 ("They") aligns with the French token 1 ("Ils"), the English token 2 
("had"), aligns with the French token 2 ("etaient"), and so on. Note that the 
punctuation is also aligned (English token 4 (".") align with French token 
(".")), and will count towards the final scoring figures. 

With missing S|P fields considered by default to be S. 




