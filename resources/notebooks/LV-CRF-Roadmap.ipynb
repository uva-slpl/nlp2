{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss what you need to know and do for project 2 :)\n",
    "\n",
    "**Important:** formulas are better viewed if you start `jupyter notebook` (as opposed to use github's visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import libitg\n",
    "from libitg import Symbol, Terminal, Nonterminal, Span\n",
    "from libitg import Rule, CFG\n",
    "from libitg import FSA\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon\n",
    "\n",
    "I am going to use a very simple lexicon as running example, you would instead use the translation pairs we provided, but note that\n",
    "\n",
    "* we provided you with [all translation pairs under IBM1 in both directions](https://uva-slpl.github.io/nlp2/resources/project_crf/lexicon.tgz), thus there are many pairs in there\n",
    "* you should retain only the top scoring translation pairs (this will make your lexicon smaller and your forests more manageable)\n",
    "* we suggest 5 translations per source word\n",
    "* you need to explicitly encode insertion and deletion in your lexicon, you can bootstrap that from alignments to NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = defaultdict(set)\n",
    "lexicon['le'].update(['the', '-EPS-'])  # we will assume that `le` can be deleted\n",
    "lexicon['-EPS-'].update(['a', 'the'])  # we will assume that `the` and `a` can be inserted\n",
    "lexicon['e'].add('and')\n",
    "lexicon['chien'].add('dog')\n",
    "lexicon['noir'].update(['black', 'noir'])  \n",
    "lexicon['blanc'].add('white')\n",
    "lexicon['petit'].update(['small', 'little'])\n",
    "lexicon['petite'].update(['small', 'little'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with the parser\n",
    "\n",
    "You should play with the basic data structures and algorithms we provided you with, namely\n",
    "\n",
    "* Symbol: Terminal, Nonterminal, Span\n",
    "* Rule\n",
    "* CFG\n",
    "* FSA, make_fsa, LengthConstraint\n",
    "* ITG-related functions: make_source_itg, make_target_side_itg\n",
    "* [Earley intersection](https://uva-slpl.github.io/nlp2/resources/papers/Aziz-Earley.pdf): earley\n",
    "\n",
    "you need to know what they do and when to use them to obtain something you need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITG\n",
    "\n",
    "We deal with ITGs by\n",
    "\n",
    "* constructing a source-language CFG\n",
    "* parsing a source string\n",
    "* projecting the resulting forest onto the target-language vocabulary through the rules of an ITG and the lexicon\n",
    "\n",
    "\n",
    "**Note on performance:** we could in principle instantiate a different source-side CFG for each source sentence by constraining the lexicon to source words that occur in the sentence we are translating.\n",
    "That is to say that if our sentence does not contain a word (e.g. *petite*) there is no point in including that word in the CFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function uses the entire lexicon\n",
    "src_cfg = libitg.make_source_side_itg(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X] ||| [X] [X]\n",
      "[X] ||| 'petit'\n",
      "[X] ||| 'e'\n",
      "[X] ||| 'le'\n",
      "[X] ||| 'chien'\n",
      "[X] ||| 'blanc'\n",
      "[X] ||| 'noir'\n",
      "[X] ||| 'petite'\n",
      "[X] ||| '-EPS-'\n",
      "[S] ||| [X]\n"
     ]
    }
   ],
   "source": [
    "print(src_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSA\n",
    "\n",
    "We represent a sentence as a linear-chain FSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=3\n",
      "arcs=3\n",
      "origin=0 destination=1 label=le\n",
      "origin=1 destination=2 label=chien\n",
      "origin=2 destination=3 label=noir\n"
     ]
    }
   ],
   "source": [
    "src_fsa = libitg.make_fsa('le chien noir')\n",
    "print(src_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forests\n",
    "\n",
    "Forests represent sets of derivations, they are represented by CFGs where symbols have been decorated with spans.\n",
    "We are working with Earley parser, which takes a CFG, an FSA, the CFG's starting symbol, and the symbol that should be used as the starting symbol of the resulting CFG.\n",
    "Note that Earley takes a CFG and returns another!\n",
    "\n",
    "The most basic forest we need is one that contains all derivations of the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:1-3 ||| [X]:1-1 [X]:1-3\n",
      "[X]:1-3 ||| [X]:1-3 [X]:3-3\n",
      "[X]:1-3 ||| [X]:1-2 [X]:2-3\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:3-3 ||| '-EPS-':3-3\n",
      "[X]:3-3 ||| [X]:3-3 [X]:3-3\n",
      "[X]:0-3 ||| [X]:0-1 [X]:1-3\n",
      "[X]:0-3 ||| [X]:0-2 [X]:2-3\n",
      "[X]:0-3 ||| [X]:0-0 [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-3 [X]:3-3\n",
      "[X]:2-3 ||| [X]:2-2 [X]:2-3\n",
      "[X]:2-3 ||| 'noir':2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:3-3\n",
      "[D(x)] ||| [S]:0-3\n",
      "[X]:1-1 ||| '-EPS-':1-1\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[S]:0-3 ||| [X]:0-3\n",
      "[X]:0-0 ||| '-EPS-':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-2 ||| 'chien':1-2\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| 'le':0-1\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:2-2 ||| '-EPS-':2-2\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n"
     ]
    }
   ],
   "source": [
    "src_forest = libitg.earley(src_cfg, src_fsa, \n",
    "                           start_symbol=Nonterminal('S'), \n",
    "                           sprime_symbol=Nonterminal(\"D(x)\"))\n",
    "print(src_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to project this forest to the target vocabulary by using the lexicon and the ITG template rules.\n",
    "This is precisely the set \\\\(\\mathcal D(x)\\\\) of our lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:0-0 ||| 'a':0-0\n",
      "[X]:0-0 ||| 'the':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-2 [X]:0-0\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n",
      "[X]:0-2 ||| [X]:1-2 [X]:0-1\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:0-2 ||| [X]:2-2 [X]:0-2\n",
      "[X]:3-3 ||| 'a':3-3\n",
      "[X]:3-3 ||| 'the':3-3\n",
      "[X]:3-3 ||| [X]:3-3 [X]:3-3\n",
      "[X]:0-3 ||| [X]:0-1 [X]:1-3\n",
      "[X]:0-3 ||| [X]:1-3 [X]:0-1\n",
      "[X]:0-3 ||| [X]:0-2 [X]:2-3\n",
      "[X]:0-3 ||| [X]:2-3 [X]:0-2\n",
      "[X]:0-3 ||| [X]:0-0 [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-3 [X]:0-0\n",
      "[X]:0-3 ||| [X]:0-3 [X]:3-3\n",
      "[X]:0-3 ||| [X]:3-3 [X]:0-3\n",
      "[X]:2-3 ||| [X]:2-2 [X]:2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:2-2\n",
      "[X]:2-3 ||| 'black':2-3\n",
      "[X]:2-3 ||| 'noir':2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:3-3\n",
      "[X]:2-3 ||| [X]:3-3 [X]:2-3\n",
      "[D(x)] ||| [S]:0-3\n",
      "[X]:1-1 ||| 'a':1-1\n",
      "[X]:1-1 ||| 'the':1-1\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[S]:0-3 ||| [X]:0-3\n",
      "[X]:1-3 ||| [X]:1-1 [X]:1-3\n",
      "[X]:1-3 ||| [X]:1-3 [X]:1-1\n",
      "[X]:1-3 ||| [X]:1-3 [X]:3-3\n",
      "[X]:1-3 ||| [X]:3-3 [X]:1-3\n",
      "[X]:1-3 ||| [X]:1-2 [X]:2-3\n",
      "[X]:1-3 ||| [X]:2-3 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:1-1\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-2 ||| [X]:2-2 [X]:1-2\n",
      "[X]:1-2 ||| 'dog':1-2\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| [X]:1-1 [X]:0-1\n",
      "[X]:0-1 ||| 'the':0-1\n",
      "[X]:0-1 ||| '-EPS-':0-1\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:0-1 ||| [X]:0-1 [X]:0-0\n",
      "[X]:2-2 ||| 'a':2-2\n",
      "[X]:2-2 ||| 'the':2-2\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n"
     ]
    }
   ],
   "source": [
    "Dx = libitg.make_target_side_itg(src_forest, lexicon)\n",
    "print(Dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with a translation observation\n",
    "\n",
    "In training, we can observe translations, thus we need to constrain \\\\(\\mathcal D(x)\\\\) to the set of derivations that in addition to \\\\(x\\\\) also produce our target observation \\\\(y\\\\).\n",
    "\n",
    "We do that by intersecting the forest for \\\\(\\mathcal D(x)\\\\) with an automaton that represents \\\\(y\\\\).\n",
    "\n",
    "That is, we first make an FSA for the target sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=3\n",
      "arcs=3\n",
      "origin=0 destination=1 label=the\n",
      "origin=1 destination=2 label=black\n",
      "origin=2 destination=3 label=dog\n"
     ]
    }
   ],
   "source": [
    "tgt_fsa = libitg.make_fsa('the black dog')\n",
    "print(tgt_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then parse this FSA with the projected forest we got earlier from Earley.\n",
    "The resulting forest will represent the set \\\\(\\mathcal D(x,y)\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "[D(x,y)] ||| [D(x)]:0-3\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n"
     ]
    }
   ],
   "source": [
    "Dxy = libitg.earley(Dx, tgt_fsa,\n",
    "                    start_symbol=Nonterminal(\"D(x)\"), \n",
    "                    sprime_symbol=Nonterminal('D(x,y)'))\n",
    "print(Dxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the black dog'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is just to illustrate that ref_forest accepts a single target (English) string\n",
    "libitg.language_of_fsa(libitg.forest_to_fsa(Dxy, Nonterminal('D(x,y)')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length constraint\n",
    "\n",
    "In order to learn our CRF for maximum likelihood we need to compute the gradient of the log-likelihood.\n",
    "\n",
    "The gradient invoves two expectations (see lecture notes and project description). One expectation summarises all ways in which we can derive the joint observation \\\\((x, y)\\\\), the other summarises all ways in which we can derive the incomplete observation \\\\(x, n\\\\). The former is computed out of the forest that represents \\\\(\\mathcal D(x, y)\\\\) and the latter is computed out of the forest that represents \\\\(\\mathcal D_n(x)\\\\).\n",
    "\n",
    "Here we will show you how to get \\\\(\\mathcal D_n(x)\\\\) by parsing a special automaton that accepts the language \\\\(\\Delta^n\\\\) where \\\\(\\Delta\\\\) is the vocabulary of the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `strict` controls whether the constraint is |yield(d)| == n (strict=True) or |yield(d)| <= n (strict=False)\n",
    "length_fsa = libitg.LengthConstraint(4, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This special automaton accepts strings containing 1 to 4 words, no matter which words. The label -WILDCARD- is a special label such that `x == -WILDCARD-` evaluates to `True` no matter the value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=5\n",
      "initial=0\n",
      "final=1 2 3 4\n",
      "arcs=4\n",
      "origin=0 destination=1 label=-WILDCARD-\n",
      "origin=1 destination=2 label=-WILDCARD-\n",
      "origin=2 destination=3 label=-WILDCARD-\n",
      "origin=3 destination=4 label=-WILDCARD-\n"
     ]
    }
   ],
   "source": [
    "print(length_fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:1-2:1-3 ||| [X]:1-2:1-2 [X]:2-2:2-3\n",
      "[X]:1-2:1-3 ||| [X]:2-2:1-2 [X]:1-2:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-2:1-2 [X]:1-1:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-1:1-2 [X]:1-2:2-3\n",
      "[X]:1-2:0-2 ||| [X]:1-1:0-1 [X]:1-2:1-2\n",
      "[X]:1-2:0-2 ||| [X]:2-2:0-1 [X]:1-2:1-2\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-1 [X]:1-1:1-2\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-1 [X]:2-2:1-2\n",
      "[X]:1-1:1-2 ||| 'a':1-1:1-2\n",
      "[X]:1-1:1-2 ||| 'the':1-1:1-2\n",
      "[X]:3-3:2-4 ||| [X]:3-3:2-3 [X]:3-3:3-4\n",
      "[X]:0-1:2-4 ||| [X]:1-1:2-4 [X]:0-1:4-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-3 [X]:0-0:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-0:2-3 [X]:0-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-3 [X]:1-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:1-1:2-3 [X]:0-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-0:2-4 [X]:0-1:4-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-2 [X]:0-0:2-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-2 [X]:1-1:2-4\n",
      "[X]:3-3:1-2 ||| 'a':3-3:1-2\n",
      "[X]:3-3:1-2 ||| 'the':3-3:1-2\n",
      "[X]:1-2:0-3 ||| [X]:1-1:0-2 [X]:1-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:2-2:0-2 [X]:1-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-2 [X]:1-1:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-2 [X]:2-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-1 [X]:2-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-1:0-1 [X]:1-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:2-2:0-1 [X]:1-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-1 [X]:1-1:1-3\n",
      "[X]:3-3:2-3 ||| 'the':3-3:2-3\n",
      "[X]:3-3:2-3 ||| 'a':3-3:2-3\n",
      "[X]:3-3:1-3 ||| [X]:3-3:1-2 [X]:3-3:2-3\n",
      "[X]:2-2:0-2 ||| [X]:2-2:0-1 [X]:2-2:1-2\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "[X]:1-2:3-4 ||| 'dog':1-2:3-4\n",
      "[X]:0-1:1-3 ||| [X]:1-1:1-2 [X]:0-1:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-1 [X]:1-1:1-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-1 [X]:0-0:1-3\n",
      "[X]:0-1:1-3 ||| [X]:0-0:1-2 [X]:0-1:2-3\n",
      "[X]:0-1:1-3 ||| [X]:1-1:1-3 [X]:0-1:3-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-2 [X]:0-0:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-0:1-3 [X]:0-1:3-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-2 [X]:1-1:2-3\n",
      "[D(x)]:0-4 ||| [S]:0-3:0-4\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "[X]:2-2:0-1 ||| 'a':2-2:0-1\n",
      "[X]:2-3:0-1 ||| 'noir':2-3:0-1\n",
      "[X]:2-3:0-1 ||| 'black':2-3:0-1\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-3 [X]:2-2:3-4\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-2 [X]:2-2:2-4\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-1 [X]:2-2:1-4\n",
      "[X]:0-1:1-2 ||| [X]:0-1:1-1 [X]:1-1:1-2\n",
      "[X]:0-1:1-2 ||| [X]:1-1:1-2 [X]:0-1:2-2\n",
      "[X]:0-1:1-2 ||| 'the':0-1:1-2\n",
      "[X]:0-1:1-2 ||| [X]:0-0:1-2 [X]:0-1:2-2\n",
      "[X]:0-1:1-2 ||| [X]:0-1:1-1 [X]:0-0:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-1 [X]:3-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-1 [X]:2-2:1-2\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-3 [X]:1-1:3-4\n",
      "[X]:1-2:2-4 ||| [X]:1-1:2-3 [X]:1-2:3-4\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-3 [X]:2-2:3-4\n",
      "[X]:1-2:2-4 ||| [X]:2-2:2-3 [X]:1-2:3-4\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-2 [X]:3-3:2-3\n",
      "[X]:2-3:0-3 ||| [X]:3-3:0-1 [X]:2-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-2 [X]:2-2:2-3\n",
      "[X]:2-3:0-3 ||| [X]:2-2:0-1 [X]:2-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-1 [X]:2-2:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-1 [X]:3-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-2:0-2 [X]:2-3:2-3\n",
      "[X]:2-3:0-3 ||| [X]:3-3:0-2 [X]:2-3:2-3\n",
      "[X]:1-1:1-3 ||| [X]:1-1:1-2 [X]:1-1:2-3\n",
      "[D_n(x)] ||| [D(x)]:0-2\n",
      "[D_n(x)] ||| [D(x)]:0-4\n",
      "[D_n(x)] ||| [D(x)]:0-3\n",
      "[X]:0-3:0-2 ||| [X]:2-3:0-1 [X]:0-2:1-2\n",
      "[X]:0-3:0-2 ||| [X]:1-3:0-2 [X]:0-1:2-2\n",
      "[X]:0-3:0-2 ||| [X]:0-2:0-1 [X]:2-3:1-2\n",
      "[X]:0-3:0-2 ||| [X]:0-1:0-0 [X]:1-3:0-2\n",
      "[X]:2-2:3-4 ||| 'the':2-2:3-4\n",
      "[X]:2-2:3-4 ||| 'a':2-2:3-4\n",
      "[X]:0-2:1-2 ||| [X]:1-2:1-2 [X]:0-1:2-2\n",
      "[X]:0-2:1-2 ||| [X]:0-1:1-1 [X]:1-2:1-2\n",
      "[X]:0-0:2-3 ||| 'a':0-0:2-3\n",
      "[X]:0-0:2-3 ||| 'the':0-0:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-2 [X]:0-0:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-2 [X]:0-1:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-2 [X]:2-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-1 [X]:2-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-2 [X]:3-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-1 [X]:0-2:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "[X]:0-2:1-3 ||| [X]:2-2:1-2 [X]:0-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:1-2:1-2 [X]:0-1:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-1:1-2 [X]:1-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-2 [X]:0-0:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-0:1-2 [X]:0-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-2 [X]:2-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-1:1-1 [X]:1-2:1-3\n",
      "[X]:0-2:1-3 ||| [X]:1-2:1-3 [X]:0-1:3-3\n",
      "[X]:1-2:1-2 ||| 'dog':1-2:1-2\n",
      "[X]:1-1:2-4 ||| [X]:1-1:2-3 [X]:1-1:3-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-3 [X]:0-0:3-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-2 [X]:0-0:2-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-1 [X]:0-0:1-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-3 [X]:2-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-3 [X]:2-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-1 [X]:2-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-1 [X]:2-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-2 [X]:2-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-3 [X]:3-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-2 [X]:2-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-3 [X]:2-2:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-1 [X]:2-2:1-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-1 [X]:3-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-2 [X]:3-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-2 [X]:2-2:2-4\n",
      "[X]:2-3:1-2 ||| 'noir':2-3:1-2\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-3 [X]:2-2:3-4\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-3 [X]:3-3:3-4\n",
      "[X]:2-3:2-4 ||| [X]:2-2:2-3 [X]:2-3:3-4\n",
      "[X]:2-3:2-4 ||| [X]:3-3:2-3 [X]:2-3:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-3 [X]:0-0:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-3 [X]:1-1:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-2 [X]:0-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-2 [X]:0-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-3 [X]:0-1:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-2 [X]:0-0:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-2 [X]:1-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-4 [X]:0-1:4-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-3 [X]:0-1:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-1 [X]:1-1:1-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-4 [X]:0-1:4-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-1 [X]:0-0:1-4\n",
      "[X]:0-0:2-4 ||| [X]:0-0:2-3 [X]:0-0:3-4\n",
      "[X]:2-2:2-4 ||| [X]:2-2:2-3 [X]:2-2:3-4\n",
      "[X]:0-3:2-4 ||| [X]:1-3:2-4 [X]:0-1:4-4\n",
      "[X]:0-3:2-4 ||| [X]:2-3:2-3 [X]:0-2:3-4\n",
      "[X]:0-3:2-4 ||| [X]:0-1:2-2 [X]:1-3:2-4\n",
      "[X]:0-3:2-4 ||| [X]:0-2:2-3 [X]:2-3:3-4\n",
      "[X]:0-1:4-4 ||| '-EPS-':0-1:4-4\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-2 [X]:2-2:2-3\n",
      "[X]:2-3:1-3 ||| [X]:2-2:1-2 [X]:2-3:2-3\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-2 [X]:3-3:2-3\n",
      "[X]:2-3:1-3 ||| [X]:3-3:1-2 [X]:2-3:2-3\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-2 [X]:0-1:2-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-3 [X]:0-1:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-2 [X]:2-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-2 [X]:0-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-1 [X]:0-2:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-1 [X]:0-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-4 [X]:0-1:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-1 [X]:1-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-1 [X]:0-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-3 [X]:3-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-2 [X]:1-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-2 [X]:0-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-2 [X]:3-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-2 [X]:0-2:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-1 [X]:2-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-3 [X]:0-2:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-3 [X]:0-0:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-2 [X]:0-0:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-0 [X]:1-3:0-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-3 [X]:2-3:3-4\n",
      "[X]:2-3:2-3 ||| 'noir':2-3:2-3\n",
      "[X]:2-3:2-3 ||| 'black':2-3:2-3\n",
      "[X]:0-1:3-4 ||| [X]:0-1:3-3 [X]:0-0:3-4\n",
      "[X]:0-1:3-4 ||| [X]:1-1:3-4 [X]:0-1:4-4\n",
      "[X]:0-1:3-4 ||| 'the':0-1:3-4\n",
      "[X]:0-1:3-4 ||| [X]:0-0:3-4 [X]:0-1:4-4\n",
      "[X]:0-1:3-4 ||| [X]:0-1:3-3 [X]:1-1:3-4\n",
      "[X]:2-2:1-4 ||| [X]:2-2:1-3 [X]:2-2:3-4\n",
      "[X]:2-2:1-4 ||| [X]:2-2:1-2 [X]:2-2:2-4\n",
      "[X]:3-3:0-1 ||| 'a':3-3:0-1\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "[X]:0-2:2-4 ||| [X]:1-2:2-3 [X]:0-1:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-1:2-3 [X]:1-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:2-2:2-3 [X]:0-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:1-2:2-4 [X]:0-1:4-4\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-3 [X]:2-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-3 [X]:0-0:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-0:2-3 [X]:0-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-1:2-2 [X]:1-2:2-4\n",
      "[X]:1-3:2-4 ||| [X]:2-3:2-3 [X]:1-2:3-4\n",
      "[X]:1-3:2-4 ||| [X]:1-2:2-3 [X]:2-3:3-4\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "[X]:3-3:1-4 ||| [X]:3-3:1-3 [X]:3-3:3-4\n",
      "[X]:3-3:1-4 ||| [X]:3-3:1-2 [X]:3-3:2-4\n",
      "[X]:2-2:2-3 ||| 'the':2-2:2-3\n",
      "[X]:2-2:2-3 ||| 'a':2-2:2-3\n",
      "[X]:3-3:0-2 ||| [X]:3-3:0-1 [X]:3-3:1-2\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-1 [X]:0-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-3 [X]:0-0:3-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-2 [X]:0-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-2 [X]:0-1:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-2 [X]:1-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-2 [X]:0-0:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-3 [X]:0-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-4 [X]:0-1:4-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-1 [X]:0-0:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-0 [X]:1-2:0-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-1 [X]:0-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-1 [X]:0-1:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-1 [X]:1-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-3 [X]:2-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-2 [X]:0-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-2 [X]:2-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-3 [X]:0-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-3 [X]:0-1:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-3 [X]:1-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-1 [X]:2-2:1-4\n",
      "[X]:1-2:1-4 ||| [X]:2-2:1-3 [X]:1-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-2 [X]:2-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-3 [X]:1-1:3-4\n",
      "[X]:1-2:1-4 ||| [X]:2-2:1-2 [X]:1-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-2 [X]:1-1:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-1:1-3 [X]:1-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:1-1:1-2 [X]:1-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-3 [X]:2-2:3-4\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "[X]:2-3:1-4 ||| [X]:2-2:1-3 [X]:2-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-2 [X]:2-2:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-2:1-2 [X]:2-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-3 [X]:3-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-2 [X]:3-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:3-3:1-3 [X]:2-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:3-3:1-2 [X]:2-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-3 [X]:2-2:3-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-2 [X]:3-3:2-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-3 [X]:3-3:3-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-1 [X]:3-3:1-4\n",
      "[X]:0-2:0-1 ||| [X]:0-1:0-0 [X]:1-2:0-1\n",
      "[X]:0-2:0-1 ||| [X]:1-2:0-1 [X]:0-1:1-1\n",
      "[X]:0-0:1-2 ||| 'the':0-0:1-2\n",
      "[X]:0-0:1-2 ||| 'a':0-0:1-2\n",
      "[X]:0-0:1-3 ||| [X]:0-0:1-2 [X]:0-0:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-2:1-2 [X]:2-3:2-3\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-3 [X]:1-1:3-4\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-2 [X]:1-1:2-4\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-1 [X]:1-1:1-4\n",
      "[S]:0-3:0-4 ||| [X]:0-3:0-4\n",
      "[X]:2-2:0-3 ||| [X]:2-2:0-2 [X]:2-2:2-3\n",
      "[X]:2-2:0-3 ||| [X]:2-2:0-1 [X]:2-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-0:0-1 [X]:0-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-3 [X]:0-1:3-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-1 [X]:2-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-2 [X]:0-1:2-3\n",
      "[X]:0-2:0-3 ||| [X]:2-2:0-2 [X]:0-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-2 [X]:1-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-0 [X]:1-2:0-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-2 [X]:0-0:2-3\n",
      "[X]:0-2:0-3 ||| [X]:2-2:0-1 [X]:0-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-1 [X]:0-1:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-1 [X]:1-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-1 [X]:0-0:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-0:0-2 [X]:0-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-2 [X]:2-2:2-3\n",
      "[X]:0-0:1-4 ||| [X]:0-0:1-3 [X]:0-0:3-4\n",
      "[X]:0-0:1-4 ||| [X]:0-0:1-2 [X]:0-0:2-4\n",
      "[D(x)]:0-2 ||| [S]:0-3:0-2\n",
      "[X]:0-2:0-2 ||| [X]:0-0:0-1 [X]:0-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-1 [X]:0-0:1-2\n",
      "[X]:0-2:0-2 ||| [X]:1-2:0-2 [X]:0-1:2-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-0 [X]:1-2:0-2\n",
      "[X]:0-2:0-2 ||| [X]:2-2:0-1 [X]:0-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:1-2:0-1 [X]:0-1:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-1 [X]:1-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-1 [X]:2-2:1-2\n",
      "[X]:0-1:2-3 ||| [X]:0-1:2-2 [X]:0-0:2-3\n",
      "[X]:0-1:2-3 ||| 'the':0-1:2-3\n",
      "[X]:0-1:2-3 ||| [X]:0-1:2-2 [X]:1-1:2-3\n",
      "[X]:0-1:2-3 ||| [X]:0-0:2-3 [X]:0-1:3-3\n",
      "[X]:0-1:2-3 ||| [X]:1-1:2-3 [X]:0-1:3-3\n",
      "[X]:1-1:1-4 ||| [X]:1-1:1-2 [X]:1-1:2-4\n",
      "[X]:1-1:1-4 ||| [X]:1-1:1-3 [X]:1-1:3-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-3 [X]:2-3:3-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-2 [X]:2-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-4 [X]:0-1:4-4\n",
      "[X]:0-3:1-4 ||| [X]:3-3:1-2 [X]:0-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-2 [X]:1-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-3 [X]:0-2:3-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-3 [X]:0-1:3-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-2 [X]:0-2:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-3 [X]:0-0:3-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-1 [X]:1-3:1-4\n",
      "[X]:0-3:1-4 ||| [X]:0-0:1-2 [X]:0-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-3 [X]:3-3:3-4\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "[X]:1-1:0-1 ||| 'a':1-1:0-1\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-3 [X]:1-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-1 [X]:0-0:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-1 [X]:1-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-2 [X]:1-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-2 [X]:0-0:2-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-4 [X]:0-1:4-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-3 [X]:0-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-4 [X]:0-1:4-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-0 [X]:0-0:0-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-2 [X]:0-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-0 [X]:1-1:0-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-1 [X]:0-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-3 [X]:0-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-2 [X]:0-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-1 [X]:0-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-3 [X]:0-0:3-4\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n",
      "[X]:1-3:1-3 ||| [X]:1-2:1-2 [X]:2-3:2-3\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-3 [X]:2-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-3 [X]:1-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-3 [X]:1-1:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-2 [X]:1-2:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-1 [X]:2-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-1 [X]:1-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-1 [X]:1-1:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-3 [X]:1-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-2 [X]:1-2:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-2 [X]:1-1:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-1 [X]:1-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-2 [X]:2-2:2-4\n",
      "[X]:1-1:0-3 ||| [X]:1-1:0-2 [X]:1-1:2-3\n",
      "[X]:1-1:0-3 ||| [X]:1-1:0-1 [X]:1-1:1-3\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-3 [X]:1-2:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-3 [X]:2-3:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-3 [X]:1-1:3-4\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-2 [X]:1-2:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-2 [X]:2-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-3 [X]:3-3:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-1:1-2 [X]:1-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:3-3:1-2 [X]:1-3:2-4\n",
      "[X]:2-3:3-4 ||| 'noir':2-3:3-4\n",
      "[X]:2-3:3-4 ||| 'black':2-3:3-4\n",
      "[X]:1-1:0-2 ||| [X]:1-1:0-1 [X]:1-1:1-2\n",
      "[S]:0-3:0-2 ||| [X]:0-3:0-2\n",
      "[X]:0-0:0-3 ||| [X]:0-0:0-2 [X]:0-0:2-3\n",
      "[X]:0-0:0-3 ||| [X]:0-0:0-1 [X]:0-0:1-3\n",
      "[X]:3-3:0-3 ||| [X]:3-3:0-2 [X]:3-3:2-3\n",
      "[X]:3-3:0-3 ||| [X]:3-3:0-1 [X]:3-3:1-3\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "[X]:0-0:0-2 ||| [X]:0-0:0-1 [X]:0-0:1-2\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "[X]:2-2:1-2 ||| 'a':2-2:1-2\n",
      "[X]:2-2:1-2 ||| 'the':2-2:1-2\n",
      "[X]:0-2:3-4 ||| [X]:0-1:3-3 [X]:1-2:3-4\n",
      "[X]:0-2:3-4 ||| [X]:1-2:3-4 [X]:0-1:4-4\n",
      "[X]:0-0:0-1 ||| 'a':0-0:0-1\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "[X]:2-2:1-3 ||| [X]:2-2:1-2 [X]:2-2:2-3\n",
      "[X]:0-1:0-2 ||| [X]:0-0:0-2 [X]:0-1:2-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-0 [X]:1-1:0-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-1 [X]:1-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-1 [X]:0-0:1-2\n",
      "[X]:0-1:0-2 ||| [X]:1-1:0-1 [X]:0-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:1-1:0-2 [X]:0-1:2-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-0 [X]:0-0:0-2\n",
      "[X]:0-1:0-2 ||| [X]:0-0:0-1 [X]:0-1:1-2\n",
      "[X]:3-3:3-4 ||| 'the':3-3:3-4\n",
      "[X]:3-3:3-4 ||| 'a':3-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-3 [X]:1-2:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-3 [X]:2-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-2 [X]:1-1:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-2 [X]:3-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-1 [X]:1-2:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-1 [X]:2-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-3 [X]:1-1:3-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-2 [X]:1-2:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-2 [X]:2-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-1 [X]:1-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-2 [X]:1-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-3 [X]:3-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-2 [X]:1-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-1 [X]:1-3:1-4\n",
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-1 [X]:0-0:1-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-2 [X]:0-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-2 [X]:0-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-1 [X]:1-1:1-3\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-3 [X]:0-1:3-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-1 [X]:0-1:1-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-0 [X]:0-0:0-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-2 [X]:1-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-2 [X]:0-0:2-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-3 [X]:0-1:3-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-0 [X]:1-1:0-3\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-1 [X]:0-1:1-3\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-2 [X]:1-1:2-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-1 [X]:1-2:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-1 [X]:2-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-2 [X]:3-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-2 [X]:2-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "[X]:0-2:1-4 ||| [X]:2-2:1-3 [X]:0-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-3 [X]:0-1:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-3 [X]:1-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:2-2:1-2 [X]:0-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-2 [X]:0-1:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-2 [X]:1-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-3 [X]:0-0:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-2 [X]:2-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-0:1-3 [X]:0-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-0:1-2 [X]:0-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-1 [X]:1-2:1-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-4 [X]:0-1:4-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-3 [X]:2-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-2 [X]:0-0:2-4\n",
      "[X]:0-0:3-4 ||| 'a':0-0:3-4\n",
      "[X]:0-0:3-4 ||| 'the':0-0:3-4\n",
      "[X]:1-3:0-2 ||| [X]:2-3:0-1 [X]:1-2:1-2\n",
      "[X]:1-3:0-2 ||| [X]:1-2:0-1 [X]:2-3:1-2\n",
      "[X]:1-1:3-4 ||| 'a':1-1:3-4\n",
      "[X]:1-1:3-4 ||| 'the':1-1:3-4\n",
      "[X]:1-1:2-3 ||| 'a':1-1:2-3\n",
      "[X]:1-1:2-3 ||| 'the':1-1:2-3\n",
      "[X]:1-2:0-1 ||| 'dog':1-2:0-1\n"
     ]
    }
   ],
   "source": [
    "Dnx = libitg.earley(Dx, length_fsa,\n",
    "                    start_symbol=Nonterminal(\"D(x)\"), \n",
    "                    sprime_symbol=Nonterminal(\"D_n(x)\"))\n",
    "print(Dnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this produces a super large FSA which enumerates the strings in the forest\n",
    "Dnx_as_fsa = libitg.forest_to_fsa(Dnx, Nonterminal('D_n(x)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a\n",
      "a a black\n",
      "a a black dog\n",
      "a a dog\n",
      "a a dog black\n",
      "a a dog noir\n",
      "a a noir\n",
      "a a noir dog\n",
      "a black\n",
      "a black a\n",
      "a black a dog\n",
      "a black dog\n",
      "a black dog a\n",
      "a black dog the\n",
      "a black the\n",
      "a black the dog\n",
      "a dog\n",
      "a dog a\n",
      "a dog a black\n",
      "a dog a noir\n",
      "a dog black\n",
      "a dog black a\n",
      "a dog black the\n",
      "a dog noir\n",
      "a dog noir a\n",
      "a dog noir the\n",
      "a dog the\n",
      "a dog the black\n",
      "a dog the noir\n",
      "a noir\n",
      "a noir a\n",
      "a noir a dog\n",
      "a noir dog\n",
      "a noir dog a\n",
      "a noir dog the\n",
      "a noir the\n",
      "a noir the dog\n",
      "a the\n",
      "a the black\n",
      "a the black dog\n",
      "a the dog\n",
      "a the dog black\n",
      "a the dog noir\n",
      "a the noir\n",
      "a the noir dog\n",
      "black a\n",
      "black a a\n",
      "black a a dog\n",
      "black a dog\n",
      "black a dog a\n",
      "black a dog the\n",
      "black a the\n",
      "black a the dog\n",
      "black dog\n",
      "black dog a\n",
      "black dog a a\n",
      "black dog a the\n",
      "black dog the\n",
      "black dog the a\n",
      "black dog the the\n",
      "black the\n",
      "black the a\n",
      "black the a dog\n",
      "black the dog\n",
      "black the dog a\n",
      "black the dog the\n",
      "black the the\n",
      "black the the dog\n",
      "dog a\n",
      "dog a a\n",
      "dog a a black\n",
      "dog a a noir\n",
      "dog a black\n",
      "dog a black a\n",
      "dog a black the\n",
      "dog a noir\n",
      "dog a noir a\n",
      "dog a noir the\n",
      "dog a the\n",
      "dog a the black\n",
      "dog a the noir\n",
      "dog black\n",
      "dog black a\n",
      "dog black a a\n",
      "dog black a the\n",
      "dog black the\n",
      "dog black the a\n",
      "dog black the the\n",
      "dog noir\n",
      "dog noir a\n",
      "dog noir a a\n",
      "dog noir a the\n",
      "dog noir the\n",
      "dog noir the a\n",
      "dog noir the the\n",
      "dog the\n",
      "dog the a\n",
      "dog the a black\n",
      "dog the a noir\n",
      "dog the black\n",
      "dog the black a\n",
      "dog the black the\n",
      "dog the noir\n",
      "dog the noir a\n",
      "dog the noir the\n",
      "dog the the\n",
      "dog the the black\n",
      "dog the the noir\n",
      "noir a\n",
      "noir a a\n",
      "noir a a dog\n",
      "noir a dog\n",
      "noir a dog a\n",
      "noir a dog the\n",
      "noir a the\n",
      "noir a the dog\n",
      "noir dog\n",
      "noir dog a\n",
      "noir dog a a\n",
      "noir dog a the\n",
      "noir dog the\n",
      "noir dog the a\n",
      "noir dog the the\n",
      "noir the\n",
      "noir the a\n",
      "noir the a dog\n",
      "noir the dog\n",
      "noir the dog a\n",
      "noir the dog the\n",
      "noir the the\n",
      "noir the the dog\n",
      "the a\n",
      "the a black\n",
      "the a black dog\n",
      "the a dog\n",
      "the a dog black\n",
      "the a dog noir\n",
      "the a noir\n",
      "the a noir dog\n",
      "the black\n",
      "the black a\n",
      "the black a dog\n",
      "the black dog\n",
      "the black dog a\n",
      "the black dog the\n",
      "the black the\n",
      "the black the dog\n",
      "the dog\n",
      "the dog a\n",
      "the dog a black\n",
      "the dog a noir\n",
      "the dog black\n",
      "the dog black a\n",
      "the dog black the\n",
      "the dog noir\n",
      "the dog noir a\n",
      "the dog noir the\n",
      "the dog the\n",
      "the dog the black\n",
      "the dog the noir\n",
      "the noir\n",
      "the noir a\n",
      "the noir a dog\n",
      "the noir dog\n",
      "the noir dog a\n",
      "the noir dog the\n",
      "the noir the\n",
      "the noir the dog\n",
      "the the\n",
      "the the black\n",
      "the the black dog\n",
      "the the dog\n",
      "the the dog black\n",
      "the the dog noir\n",
      "the the noir\n",
      "the the noir dog\n"
     ]
    }
   ],
   "source": [
    "# here we get the strings in a normal python set\n",
    "candidates = libitg.language_of_fsa(Dnx_as_fsa)\n",
    "for candidate in sorted(candidates):\n",
    "    print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings in D_n(x): 176\n",
      "Does D_n(x) contain the reference (it should!)? yes\n"
     ]
    }
   ],
   "source": [
    "# Let's check how many string we got\n",
    "print('Strings in D_n(x): %d' % len(candidates))\n",
    "# and check whether the gold-standard reference is still in the set of candidates (it should be!)\n",
    "print('Does D_n(x) contain the reference (it should!)? %s' % ('yes' if 'the black dog' in candidates else 'no'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Earley parsing helped us instatiated the sets which will be support to our probability distribution, in particular,\n",
    "\n",
    "* \\\\(\\mathcal D_n(x)\\\\) will be support to our joint distribution \\\\(P(Y, D|X=x, N=n)\\\\)\n",
    "* \\\\(\\mathcal D(x, y)\\\\) will be support to our posterior distribution \\\\(P(D|X=x, Y=y, N=n)\\\\)\n",
    "\n",
    "and our joint distribution corresponds to \n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y, D=d|X=x, N=n) &= \\frac{\\exp(w^\\top \\Phi(y, d, x, n))}{\\sum_{y' \\in \\mathcal Y_n(x)} \\sum_{d' \\in \\mathcal D(x, y)} \\exp(w^\\top \\Phi(y', d', x, n))} \\\\\n",
    " &= \\frac{\\exp(w^\\top \\Phi(y, d, x, n))}{\\sum_{d' \\in \\mathcal D_n(x)} \\exp(w^\\top \\Phi(y', d', x, n))}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "* \\\\(\\Phi\\\\) is a feature function that maps any tuple \\\\(y, d, x, n\\\\) to \\\\(\\mathbb R^d\\\\)\n",
    "* \\\\(\\theta\\\\) is a weight vector in \\\\(\\mathbb R^d\\\\)\n",
    "* \\\\(\\mathcal Y_n(x)\\\\) is the set of translations of \\\\(x\\\\) that are at most \\\\(n\\\\)-words long\n",
    "* and the last equality is due to the fact that there is a deterministic mapping between a derivation and the string it projects on either language, that is, \\\\(y' = \\text{yield}_\\Delta(d')\\\\).\n",
    "\n",
    "In order to instantiate the joint distribution for the entire space \\\\(\\mathcal D_n(x)\\\\) we must use features that can be locally assigned to the steps of a derivation. This corresponds to the idea of having *local features* or *edge potentials*:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y, D=d|X=x, N=n) \n",
    " &= \\frac{\\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right)}{\\sum_{r_{s, t} \\in d' \\in \\mathcal D_n(x)} \\exp\\left( \\sum_c w^\\top \\phi(r, s, t, x, n) \\right)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* a derivation is seen as a sequence of rules decorated with spans\n",
    "* \\\\(r_{s,t}\\\\) is a rule decorated with a source and a target span (in the code sometimes we call this an *edge*)\n",
    "* \\\\(\\phi\\\\) is a (local) feature function that maps any tuple \\\\((r, s, t, x, n)\\\\) to \\\\(\\mathbb R^d\\\\)\n",
    "* note that \\\\( \\Phi(y, d, x, n) = \\sum_{r_{s, t} \\in d} \\phi(r, s, t, x, n) \\\\)\n",
    "\n",
    "The formulation in terms of local potentials make it clear that in designing feature representations for derivations we have unrestricted access to \\\\(x\\\\), \\\\(n\\\\), the rule identiy \\\\(r\\\\), the source span \\\\(s\\\\) and the target span \\\\(t\\\\), but note that because we do not have unrestricted access to \\\\(y\\\\), the information that \\\\(t\\\\) provides is rather limited.\n",
    "\n",
    "* For example, knowing the rule `X_{1-3,1-3} -> X_{2-3,1-2} X_{1-2,2-3}` tells us that the LHS spans from 1 to 3 on the source. Suppose, we are translating \\\\(x\\\\) = `le chien noir`, then we know the the LHS projects onto `chien noir`, that it is prefixed by `BOS le` and followed by `EOS` (end-of-sentence). On the other hand, if we focus on the target span, we only know that we will produce up 2 words (because `3-1=2`) but we dont't know which words those are;\n",
    "* Another rule, `X_{2-3,1-2} -> noir/black` tells us a different story, it tells us that the lexical entry `noir` has been aligned to the lexical entry `black`, it also tell us about the positions where these words are, so we could for example compute a distortion feature similar to IBM2's jump value;\n",
    "* A rule, `X_{0-1,0-0} -> le/-EPS-` tells us that a deletion has happened;\n",
    "* Insertions can for example appear like this `X_{1-2,1-3} -> X_{1-1,1-2} X_{1-2,2-3}`, note that the first RHS symbol has an empty source span `1-1`, but the target span is longer than 0, this means an insertion has happened;\n",
    "* The rule `X_{1-3,1-3} -> X_{2-3,1-2} X_{1-2,2-3}` also tells us that we are making an inversion, note that the source spans are not adjacent, but rather flipped, e.g. 2-3 for the first RHS symbol and 1-2 for the second RHS symbol, we could have a feature capture that;\n",
    "\n",
    "After undestanding how to use the parser to get the relevant sets, you should focus on implement a feature function that can convert the information available on an edge into a sparse feature vector.\n",
    "\n",
    "We suggest you start with a very simple feature function that is mostly *dense*. This will make development a lot easier in the beginning. Here we show a minimal version of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some auxiliary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_word(fsa: FSA, origin: int, destination: int) -> str:\n",
    "    \"\"\"Returns the python string representing a source word from origin to destination (assuming there's a single one)\"\"\"\n",
    "    labels = list(fsa.labels(origin, destination))\n",
    "    assert len(labels) == 1, 'Use this function only when you know the path is unambiguous, found %d labels %s for (%d, %d)' % (len(labels), labels, origin, destination)\n",
    "    return labels[0]\n",
    "\n",
    "def get_target_word(symbol: Symbol):\n",
    "    \"\"\"Returns the python string underlying a certain terminal (thus unwrapping all span annotations)\"\"\"\n",
    "    if not symbol.is_terminal():\n",
    "        raise ValueError('I need a terminal, got %s of type %s' % (symbol, type(symbol)))\n",
    "    return symbol.root().obj()\n",
    "\n",
    "def get_bispans(symbol: Span):\n",
    "    \"\"\"\n",
    "    Returns the bispans associated with a symbol. \n",
    "    \n",
    "    The first span returned corresponds to paths in the source FSA (typically a span in the source sentence),\n",
    "     the second span returned corresponds to either\n",
    "        a) paths in the target FSA (typically a span in the target sentence)\n",
    "        or b) paths in the length FSA\n",
    "    depending on the forest where this symbol comes from.\n",
    "    \"\"\"\n",
    "    if not isinstance(symbol, Span):\n",
    "        raise ValueError('I need a span, got %s of type %s' % (symbol, type(symbol)))\n",
    "    s, start2, end2 = symbol.obj()  # this unwraps the target or length annotation\n",
    "    _, start1, end1 = s.obj()  # this unwraps the source annotation\n",
    "    return (start1, end1), (start2, end2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and define our prototype feature function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features(edge: Rule, src_fsa: FSA, eps=Terminal('-EPS-'), \n",
    "                    sparse_del=False, sparse_ins=False, sparse_trans=False) -> dict:\n",
    "    \"\"\"\n",
    "    Featurises an edge given\n",
    "        * rule and spans\n",
    "        * src sentence as an FSA\n",
    "        * TODO: target sentence length n\n",
    "        * TODO: extract IBM1 dense features\n",
    "    crucially, note that the target sentence y is not available!    \n",
    "    \"\"\"\n",
    "    fmap = defaultdict(float)\n",
    "    if len(edge.rhs) == 2:  # binary rule\n",
    "        fmap['type:binary'] += 1.0\n",
    "        # here we could have sparse features of the source string as a function of spans being concatenated\n",
    "        (ls1, ls2), (lt1, lt2) = get_bispans(edge.rhs[0])  # left of RHS\n",
    "        (rs1, rs2), (rt1, rt2) = get_bispans(edge.rhs[1])  # right of RHS        \n",
    "        # TODO: double check these, assign features, add some more\n",
    "        if ls1 == ls2:  # deletion of source left child\n",
    "            pass\n",
    "        if rs1 == rs2:  # deletion of source right child\n",
    "            pass\n",
    "        if ls2 == rs1:  # monotone\n",
    "            pass\n",
    "        if ls1 == rs2:  # inverted\n",
    "            pass        \n",
    "    else:  # unary\n",
    "        symbol = edge.rhs[0]\n",
    "        if symbol.is_terminal():  # terminal rule\n",
    "            fmap['type:terminal'] += 1.0\n",
    "            # we could have IBM1 log probs for the traslation pair or ins/del\n",
    "            (s1, s2), (t1, t2) = get_bispans(symbol)            \n",
    "            if symbol.root() == eps:  # symbol.root() gives us a Terminal free of annotation\n",
    "                # for sure there is a source word\n",
    "                src_word = get_source_word(src_fsa, s1, s2)                \n",
    "                fmap['type:deletion'] += 1.0\n",
    "                # dense versions (for initial development phase)\n",
    "                # TODO: use IBM1 prob\n",
    "                #ff['ibm1:del:logprob'] += \n",
    "                # sparse version\n",
    "                if sparse_del:\n",
    "                    fmap['del:%s' % src_word] += 1.0\n",
    "            else:                  \n",
    "                # for sure there's a target word\n",
    "                tgt_word = get_target_word(symbol)\n",
    "                if s1 == s2:  # has not consumed any source word, must be an eps rule                    \n",
    "                    fmap['type:insertion'] += 1.0\n",
    "                    # dense version\n",
    "                    # TODO: use IBM1 prob\n",
    "                    #ff['ibm1:ins:logprob'] += \n",
    "                    # sparse version\n",
    "                    if sparse_ins:\n",
    "                        fmap['ins:%s' % tgt_word] += 1.0\n",
    "                else:\n",
    "                    # for sure there's a source word\n",
    "                    src_word = get_source_word(src_fsa, s1, s2)\n",
    "                    fmap['type:translation'] += 1.0\n",
    "                    # dense version\n",
    "                    # TODO: use IBM1 prob\n",
    "                    #ff['ibm1:x2y:logprob'] += \n",
    "                    #ff['ibm1:y2x:logprob'] += \n",
    "                    # sparse version                    \n",
    "                    if sparse_trans:\n",
    "                        fmap['trans:%s/%s' % (src_word, tgt_word)] += 1.0\n",
    "        else:  # S -> X\n",
    "            fmap['top'] += 1.0\n",
    "    return fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply featurize edges, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_edges(forest, src_fsa, \n",
    "                    sparse_del=False, sparse_ins=False, sparse_trans=False,\n",
    "                    eps=Terminal('-EPS-')) -> dict:\n",
    "    edge2fmap = dict()\n",
    "    for edge in forest:\n",
    "        edge2fmap[edge] = simple_features(edge, src_fsa, eps, sparse_del, sparse_ins, sparse_trans)\n",
    "    return edge2fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what we get if we use some sparse features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "defaultdict(<class 'float'>, {'type:deletion': 1.0, 'del:le': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "defaultdict(<class 'float'>, {'type:deletion': 1.0, 'del:le': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "defaultdict(<class 'float'>, {'type:insertion': 1.0, 'ins:the': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "defaultdict(<class 'float'>, {'type:insertion': 1.0, 'ins:the': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "defaultdict(<class 'float'>, {'type:deletion': 1.0, 'del:le': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "defaultdict(<class 'float'>, {'type:translation': 1.0, 'trans:le/the': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "defaultdict(<class 'float'>, {'top': 1.0})\n",
      "\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "defaultdict(<class 'float'>, {'type:insertion': 1.0, 'ins:the': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[D(x,y)] ||| [D(x)]:0-3\n",
      "defaultdict(<class 'float'>, {'top': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "defaultdict(<class 'float'>, {'type:insertion': 1.0, 'ins:the': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "defaultdict(<class 'float'>, {'trans:noir/black': 1.0, 'type:translation': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "defaultdict(<class 'float'>, {'trans:chien/dog': 1.0, 'type:translation': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "defaultdict(<class 'float'>, {'top': 1.0})\n",
      "\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "defaultdict(<class 'float'>, {'type:deletion': 1.0, 'del:le': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for edge, fmap in featurize_edges(Dxy, src_fsa, True, True, True).items():\n",
    "    print(edge)\n",
    "    print(fmap)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recall the definition of the joint distribution\n",
    "\n",
    "\\begin{align}\n",
    "P(y,d|x, n) &= \\frac{\\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right)}{Z_n(x)}\n",
    "\\end{align}\n",
    "\n",
    "for convenience, let's work with log probabilities, \n",
    "\n",
    "\\begin{align}\n",
    "\\log P(y,d|x,n) &= \\log \\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right) - \\log Z_n(x) \\\\\n",
    "  &= \\underbrace{\\sum_{r_{s,t} \\in d} \\underbrace{w^\\top \\phi(r, s, t, x, n)}_{\\text{local log-potential}}}_{\\text{log of unnormalised distribution}} - \\log Z_n(x)\n",
    "\\end{align}\n",
    "\n",
    "Now, in the log-domain, we can compute the logarithm of unnormalised probabilities by summing along the edges in a derivation.\n",
    "\n",
    "This is great news! We can express the log of the unnormalised distribution by decorating edges with local log-potentials, that is, \n",
    "\n",
    "\\begin{equation}\n",
    "\\omega(r_{s,t}) = w^\\top \\phi(r, s, t, x, n)\n",
    "\\end{equation}\n",
    "\n",
    "where \\\\(\\omega\\\\) takes an edge in the hypergraph and returns its local log-potential (dot product of feature weights and feature vector).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to define a weight function now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_function(edge, fmap, wmap) -> float:\n",
    "    pass  # dot product of fmap and wmap  (working in log-domain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "Working with hypergraphs can be tedious, they can be quite large, a few tips:\n",
    "\n",
    "* do not parse very long sentences: discard training instances where either string is longer than say 10 words in development phase, for the final report you can try to stretch all the way to 30 or so;\n",
    "* parse once: with a fixed lexicon and grammar, the parse forests will never change, thus produce them one at a time and pickle them to disk, so that whenever you need the forest during SGD it will be pre-computed\n",
    "* featurise once: same applies to feature vectors, if you do not change your feature function, feature vectors will remain the same, thus, extract them once and pickle them to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE\n",
    "\n",
    "Now it's time you optimise your likelihood\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L(w|x, y) &= \\log P(y|x, n) \\\\\n",
    " &= \\log \\sum_{d \\in \\mathcal D(x, y)} P(y, d|x, n) \\\\\n",
    " &= \\log \\sum_{d \\in \\mathcal D(x, y)} \\frac{\\exp(w^\\top \\Phi(y, d|x, n))}{Z_n(x)} \\\\\n",
    " &= \\log \\frac{\\sum_{d \\in \\mathcal D(x, y)} \\exp(w^\\top \\Phi(y, d|x, n))}{Z_n(x)} \\\\\n",
    " &= \\log \\frac{Z(x, y)}{Z_n(x)} \\\\\n",
    " &= \\log Z(x, y) - \\log Z_n(x)\n",
    "\\end{align}\n",
    "\n",
    "First of all, we need to efficiently compute\n",
    "* \\\\(\\log Z(x,y)\\\\) the sum of all unnormalised probabilities for derivations that explain the translation pair \\\\(x, y\\\\)\n",
    "* and \\\\(\\log Z_n(x)\\\\) the sum of all unnormalised probabilities for derivations that explain the incomplete observation \\\\(x, n\\\\)\n",
    "now note that we have acyclic hypergraphs whose edges represent log-potentials for a model which is log-linear. This means that we can efficiently compute the global log-normalisers with a recursive formula that visits each node in the forest once. This recursion is called the `Inside algorithm`.\n",
    "\n",
    "The *inside weight* of a node corresponds to the sum of the weights of all paths under this node in the forest.\n",
    "\n",
    "\\begin{equation}\n",
    "    I(v) = \n",
    "    \\begin{cases}\n",
    "        1 \\quad \\text{if } v \\text{ is terminal }\\\\\n",
    "        \\bigoplus_{e \\in BS(v)} \\omega(e) \\bigotimes_{u \\in tail(e)} I(u) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* \\\\(u\\\\) and \\\\(v\\\\) are nodes (these are annotated symbols in the forest)\n",
    "* \\\\(e\\\\) is an edge (this is an annotated rule in the forest)\n",
    "* \\\\(\\text{tail}(e)\\\\) is a sequence of children nodes (this is the RHS of an annotated rule in the forest)\n",
    "* \\\\(BS(v)\\\\) corresponds to the set of edges that are incoming to v (this is the set of annotated rules for which v is LHS in the forest)\n",
    "* \\\\(\\otimes\\\\) corresponds to product of probabilities or equivalently sum of log-probabilities\n",
    "* \\\\(\\otimes\\\\) corresponds to sum of probabilities or equivalently log-of-sum-of-exponentiated-log-probabilities, i.e. \\\\(\\log (\\exp(a) + \\exp(b))\\\\) where \\\\(a\\\\) and \\\\(b\\\\) are log-probabilities\n",
    "* if \\\\(\\otimes\\\\) and \\\\(\\oplus\\\\) confused you, then you need to learn a little more about [semirings](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf)\n",
    "\n",
    "The inside recursion can be implemented iteratively by visiting the nodes in [top-sorted order](https://en.wikipedia.org/wiki/Topological_sorting).\n",
    "              \n",
    "Check the lecture notes and [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a pseudo-code of the Inside algorithm.\n",
    "\n",
    "The inside at the root of the forest represents the log-normaliser of the forest.\n",
    "\n",
    "Now you should be ready to implement topsort and the inside algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_sort(forest: CFG) -> list:\n",
    "    \"\"\"Returns ordered list of nodes according to topsort order in an acyclic forest\"\"\"\n",
    "    pass\n",
    "\n",
    "def inside_algorithm(forest: CFG, tsort: list, edge_weights: dict) -> dict:\n",
    "    \"\"\"Returns the inside weight of each node\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we will approach the optimisation of the log-likelihood via SGD, that is, \n",
    "we will do so by taking steps towards the steepest ascent at each training instance (or mini-batch).\n",
    "\n",
    "Taking derivatives with respect to \\\\(w\\\\) we get\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w \\mathcal L(w|x, y, n) &= \\mathbb E_{P(D|Y=y, X=x, N=n)}[\\Phi(Y, D, X, N)] - \\mathbb E_{P(Y, D|X=x, N=n)}[\\Phi(Y, D, X, N)]\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* the first expectation uses the forest \\\\(\\mathcal D(x, y)\\\\)\n",
    "* the second expectation uses the forest \\\\(\\mathcal D_n(x)\\\\)\n",
    "\n",
    "again because our CRF is edge-factored, we can compute this rather efficiently combining inside weights with *outside* weights, for which we need the *outside algorithm*. If you heard of forward-backward, then inside-outside is just a generalisation for hypergraphs (instead of graphs). You can learn more about expectations from forest from [Li and Eisner](http://www.aclweb.org/anthology/D09-1005).\n",
    "\n",
    "Whereas the inside was defined easily on its own, the outside will be defined differently so that\n",
    "\\begin{align}\n",
    "    I(u) \\otimes O(u)\n",
    "\\end{align}\n",
    "is proportional to the marginal probability of node \\\\(u\\\\), where the missing proportionality constant corresponds to the log-normaliser of the distribution (which is precisely the Inside weight of the root of the forest).\n",
    "\n",
    "\\begin{equation}\n",
    "    O(v) = \n",
    "    \\begin{cases}\n",
    "        1 \\quad \\text{if } FS(v) =\\emptyset \\\\\n",
    "        \\bigoplus_{e \\in FS(v)} O(\\text{head}(e)) \\bigotimes_{s \\in tail(e)\\setminus\\{v\\}} I(s) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* \\\\(s\\\\) and \\\\(v\\\\) are nodes (these are annotated symbols in the forest)\n",
    "* \\\\(e\\\\) is an edge (this is an annotated rule in the forest)\n",
    "* \\\\(\\text{head}(e)\\\\) is a the edge's head node (this is the annotated LHS of an annotated rule in the forest)\n",
    "* \\\\(FS(v)\\\\) corresponds to the set of edges that are outgoing from v (this is the set of annotated rules for which v appears in the RHS)\n",
    "* \\\\(I(s)\\\\) is the inside weight of node \\\\(s\\\\)\n",
    "* \\\\(\\otimes\\\\) corresponds to product of probabilities or equivalently sum of log-probabilities\n",
    "* \\\\(\\otimes\\\\) corresponds to sum of probabilities or equivalently log-of-sum-of-exponentiated-log-probabilities, i.e. \\\\(\\log (\\exp(a) + \\exp(b))\\\\) where \\\\(a\\\\) and \\\\(b\\\\) are log-probabilities\n",
    "* if \\\\(\\otimes\\\\) and \\\\(\\oplus\\\\) confused you, then you need to learn a little more about [semirings](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf)\n",
    "\n",
    "\n",
    "Again, check the [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a pseu-code for the outside algorithm.\n",
    "\n",
    "Now you should be ready to implement the *outside algorithm*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outside_algorithm(forest: CFG, tsort:list, edge_weights: dict, inside: dict) -> dict:\n",
    "    \"\"\"Returns the outside weight of each node\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to combine inside and outside and efficiently compute the expected feature vector under each distribution.\n",
    "\n",
    "This takes a linear pass over the edges of the forest, and [Li and Eisner](http://www.aclweb.org/anthology/D09-1005) taught us how to do that very efficiently (see figure 4, or our [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a clean pseudo-code).\n",
    "\n",
    "You should be ready to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expected_feature_vector(forest: CFG, inside: dict, outside: dict, edge_features: dict) -> dict:\n",
    "    \"\"\"Returns an expected feature vector (here a sparse python dictionary)\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Now it's time to take gradient steps,\n",
    "\n",
    "\\begin{align}\n",
    "    w = w + \\delta \\nabla_w \\mathcal L(w|x, y, n)\n",
    "\\end{align}\n",
    "\n",
    "where for a certain observation \\\\(x, n, y\\\\) we compute the expected feature vectors, subtract them, scale by a learning rate and get a parameter update.\n",
    "If we have a mini-batch, we accumulate the gradients.\n",
    "\n",
    "In the beginning of the project you will be working with the simple (mostly dense) feature function, thus this should work reasonably well. At some point you will model sparse features and then you will most likely need a [regulariser](http://www.aclweb.org/anthology/P08-1024).\n",
    "\n",
    "To diagonose convergence we typically track log-likelihood of training data, with online learning or mini-batching is typically too expensive to go over the entire trainin set computing log-likelihood, thus you can do that on each batch individually and track both the batch log-likelihood and a running average.\n",
    "\n",
    "Lastly, you might want to experiment with averaged parameters where in parallel to the parameters \\\\(w\\\\) that you actually use, you can keep a running average of parameters \\\\(w_{\\text{avg}}\\\\), at then end of learning, this running average typically shows better performance at prediction time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Finally, at prediction time we have a big problem, because\n",
    "\\begin{align}\n",
    "y^\\star =    \\arg\\max_y P(y|x, n)\n",
    "\\end{align}\n",
    "is intractable as proven by [Sima'an (1996)](http://www.aclweb.org/anthology/C/C96/C96-2215.pdf).\n",
    "Thus we typically approximate that criterion by\n",
    "\\begin{align}\n",
    "y^\\star =  \\text{yield}_\\Delta \\left(  \\arg\\max_d P(y, d|x, n) \\right)\n",
    "\\end{align}\n",
    "which is often termed *Viterbi decoding*.\n",
    "\n",
    "All this takes is a forest respresenting all translation candidates, i.e. \\\\(\\mathcal D_n(x)\\\\) and a the Viterbi algorithm. Because in this project you need to implement *inside*, there's little point in programming Viterbi, you can simply use Inside for that. Check our [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
