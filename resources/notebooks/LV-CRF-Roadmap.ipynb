{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss what you need to know and do for project 2 :)\n",
    "\n",
    "**Important:** formulas are better viewed if you start `jupyter notebook` (as opposed to use github's visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import libitg\n",
    "from libitg import Symbol, Terminal, Nonterminal, Span\n",
    "from libitg import Rule, CFG\n",
    "from libitg import FSA\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon\n",
    "\n",
    "I am going to use a very simple lexicon as running example, you would instead use the translation pairs we provided, but note that\n",
    "\n",
    "* we provided you with [all translation pairs under IBM1 in both directions](https://uva-slpl.github.io/nlp2/resources/project_crf/lexicon.tgz), thus there are many pairs in there\n",
    "* you should retain only the top scoring translation pairs (this will make your lexicon smaller and your forests more manageable)\n",
    "* we suggest 5 translations per source word\n",
    "* you need to explicitly encode insertion and deletion in your lexicon, you can bootstrap that from alignments to NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test lexicon\n",
    "lexicon = defaultdict(set)\n",
    "lexicon['le'].update(['-EPS-', 'the', 'some', 'a', 'an'])  # we will assume that `le` can be deleted\n",
    "lexicon['e'].update(['-EPS-', 'and', '&', 'also', 'as'])\n",
    "lexicon['chien'].update(['-EPS-', 'dog', 'canine', 'wolf', 'puppy'])\n",
    "lexicon['noir'].update(['-EPS-', 'black', 'noir', 'dark', 'void'])\n",
    "lexicon['blanc'].update(['-EPS-', 'white', 'blank', 'clear', 'flash'])\n",
    "lexicon['petit'].update(['-EPS-', 'small', 'little', 'mini', 'junior'])\n",
    "lexicon['petite'].update(['-EPS-', 'small', 'little', 'mini', 'junior'])\n",
    "lexicon['.'].update(['-EPS-', '.', '!', '?', ','])\n",
    "#lexicon['-EPS-'].update(['.', ',', 'a', 'the', 'some', 'of', 'bit', \"'s\", \"'m\", \"'ve\"])  # we will assume that `the` and `a` can be inserted\n",
    "lexicon['-EPS-'].update(['.', 'a', 'the', 'some', 'of'])  # we will assume that `the` and `a` can be inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This lexicon makes more insertions\n",
    "lexicon2 = defaultdict(set)\n",
    "lexicon2['le'].update(['-EPS-', 'the', 'some', 'a', 'an'])  # we will assume that `le` can be deleted\n",
    "lexicon2['e'].update(['-EPS-', 'and', '&', 'also', 'as'])\n",
    "lexicon2['chien'].update(['-EPS-', 'dog', 'canine', 'wolf', 'puppy'])\n",
    "lexicon2['noir'].update(['-EPS-', 'black', 'noir', 'dark', 'void'])\n",
    "lexicon2['blanc'].update(['-EPS-', 'white', 'blank', 'clear', 'flash'])\n",
    "lexicon2['petit'].update(['-EPS-', 'small', 'little', 'mini', 'junior'])\n",
    "lexicon2['petite'].update(['-EPS-', 'small', 'little', 'mini', 'junior'])\n",
    "lexicon2['.'].update(['-EPS-', '.', '!', '?', ','])\n",
    "lexicon2['-EPS-'].update(['.', ',', 'a', 'the', 'some', 'of', 'bit', \"'s\", \"'m\", \"'ve\"])  # we will assume that `the` and `a` can be inserted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with the parser\n",
    "\n",
    "You should play with the basic data structures and algorithms we provided you with, namely\n",
    "\n",
    "* Symbol: Terminal, Nonterminal, Span\n",
    "* Rule\n",
    "* CFG\n",
    "* FSA, make_fsa, LengthConstraint\n",
    "* ITG-related functions: make_source_itg, make_target_side_itg\n",
    "* [Earley intersection](https://uva-slpl.github.io/nlp2/resources/papers/Aziz-Earley.pdf): earley\n",
    "\n",
    "you need to know what they do and when to use them to obtain something you need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITG\n",
    "\n",
    "We deal with ITGs by\n",
    "\n",
    "* constructing a source-language CFG\n",
    "* parsing a source string\n",
    "* projecting the resulting forest onto the target-language vocabulary through the rules of an ITG and the lexicon\n",
    "\n",
    "\n",
    "**Note on performance:** we could in principle instantiate a different source-side CFG for each source sentence by constraining the lexicon to source words that occur in the sentence we are translating.\n",
    "That is to say that if our sentence does not contain a word (e.g. *petite*) there is no point in including that word in the CFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function uses the entire lexicon\n",
    "src_cfg = libitg.make_source_side_itg(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S] ||| [X]\n",
      "[X] ||| [X] [X]\n",
      "[X] ||| '.'\n",
      "[X] ||| '-EPS-'\n",
      "[X] ||| 'le'\n",
      "[X] ||| 'noir'\n",
      "[X] ||| 'e'\n",
      "[X] ||| 'petite'\n",
      "[X] ||| 'petit'\n",
      "[X] ||| 'blanc'\n",
      "[X] ||| 'chien'\n"
     ]
    }
   ],
   "source": [
    "print(src_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our `new` **finite ITG**.\n",
    "\n",
    "This ITG constrains insertions through the grammar definition, thus the set \\\\(D(x)\\\\) that you get out of it is already finite by definition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S] ||| [X]\n",
      "[X] ||| [X] [X]\n",
      "[X] ||| [T]\n",
      "[X] ||| [D]\n",
      "[X] ||| [T] [I]\n",
      "[X] ||| [I] [T]\n",
      "[T] ||| '.'\n",
      "[T] ||| 'le'\n",
      "[T] ||| 'noir'\n",
      "[T] ||| 'e'\n",
      "[T] ||| 'petite'\n",
      "[T] ||| 'petit'\n",
      "[T] ||| 'blanc'\n",
      "[T] ||| 'chien'\n",
      "[I] ||| '-EPS-'\n",
      "[D] ||| [D] [D]\n",
      "[D] ||| '.'\n",
      "[D] ||| 'le'\n",
      "[D] ||| 'noir'\n",
      "[D] ||| 'e'\n",
      "[D] ||| 'petite'\n",
      "[D] ||| 'petit'\n",
      "[D] ||| 'blanc'\n",
      "[D] ||| 'chien'\n"
     ]
    }
   ],
   "source": [
    "print(libitg.make_source_side_finite_itg(lexicon))\n",
    "# of course we also have a function: make_target_side_finite_itg \n",
    "# check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSA\n",
    "\n",
    "We represent a sentence as a linear-chain FSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=3\n",
      "arcs=3\n",
      "origin=0 destination=1 label=le\n",
      "origin=1 destination=2 label=chien\n",
      "origin=2 destination=3 label=noir\n"
     ]
    }
   ],
   "source": [
    "src_fsa = libitg.make_fsa('le chien noir')\n",
    "print(src_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forests\n",
    "\n",
    "Forests represent sets of derivations, they are represented by CFGs where symbols have been decorated with spans.\n",
    "We are working with Earley parser, which takes a CFG, an FSA, the CFG's starting symbol, and the symbol that should be used as the starting symbol of the resulting CFG.\n",
    "Note that Earley takes a CFG and returns another!\n",
    "\n",
    "The most basic forest we need is one that contains all derivations of the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S]:0-3 ||| [X]:0-3\n",
      "[X]:0-1 ||| 'le':0-1\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:1-1 ||| '-EPS-':1-1\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[X]:2-2 ||| '-EPS-':2-2\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n",
      "[X]:2-3 ||| 'noir':2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:3-3\n",
      "[X]:2-3 ||| [X]:2-2 [X]:2-3\n",
      "[X]:1-2 ||| 'chien':1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:3-3 ||| '-EPS-':3-3\n",
      "[X]:3-3 ||| [X]:3-3 [X]:3-3\n",
      "[X]:0-0 ||| '-EPS-':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[X]:1-3 ||| [X]:1-2 [X]:2-3\n",
      "[X]:1-3 ||| [X]:1-3 [X]:3-3\n",
      "[X]:1-3 ||| [X]:1-1 [X]:1-3\n",
      "[X]:0-3 ||| [X]:0-2 [X]:2-3\n",
      "[X]:0-3 ||| [X]:0-3 [X]:3-3\n",
      "[X]:0-3 ||| [X]:0-1 [X]:1-3\n",
      "[X]:0-3 ||| [X]:0-0 [X]:0-3\n",
      "[D(x)] ||| [S]:0-3\n",
      "29 rules\n"
     ]
    }
   ],
   "source": [
    "# I am going to call this _Dx because it is almost D(x), it just misses a projection step\n",
    "_Dx = libitg.earley(src_cfg, src_fsa, \n",
    "                           start_symbol=Nonterminal('S'), \n",
    "                           sprime_symbol=Nonterminal(\"D(x)\"), \n",
    "                           clean=False)\n",
    "print(_Dx)\n",
    "print('%d rules' % len(_Dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earley now can clean the forest so that every edge is guaranteed to be part of at least one complete derivation.\n",
    "In some cases the default Earley algorithm already produces clean forests, but when the input automaton is more complex, setting the flag `clean=True` is a good idea! In general, it doesn't take too long to clean the forest, so by default Earley assumes clean is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 rules in clean forest\n"
     ]
    }
   ],
   "source": [
    "print('%d rules in clean forest' % len(libitg.earley(src_cfg, src_fsa, \n",
    "                                                     start_symbol=Nonterminal('S'), \n",
    "                                                     sprime_symbol=Nonterminal(\"D(x)\"), \n",
    "                                                     clean=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to project this forest to the target vocabulary by using the lexicon and the ITG template rules.\n",
    "This is precisely the set \\\\(\\mathcal D(x)\\\\) of our lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S]:0-3 ||| [X]:0-3\n",
      "[X]:0-1 ||| 'the':0-1\n",
      "[X]:0-1 ||| 'some':0-1\n",
      "[X]:0-1 ||| '-EPS-':0-1\n",
      "[X]:0-1 ||| 'a':0-1\n",
      "[X]:0-1 ||| 'an':0-1\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| [X]:1-1 [X]:0-1\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:0-1 ||| [X]:0-1 [X]:0-0\n",
      "[X]:0-3 ||| [X]:0-2 [X]:2-3\n",
      "[X]:0-3 ||| [X]:2-3 [X]:0-2\n",
      "[X]:0-3 ||| [X]:0-3 [X]:3-3\n",
      "[X]:0-3 ||| [X]:3-3 [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-1 [X]:1-3\n",
      "[X]:0-3 ||| [X]:1-3 [X]:0-1\n",
      "[X]:0-3 ||| [X]:0-0 [X]:0-3\n",
      "[X]:0-3 ||| [X]:0-3 [X]:0-0\n",
      "[X]:2-2 ||| 'the':2-2\n",
      "[X]:2-2 ||| '.':2-2\n",
      "[X]:2-2 ||| 'some':2-2\n",
      "[X]:2-2 ||| 'a':2-2\n",
      "[X]:2-2 ||| 'of':2-2\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n",
      "[X]:1-2 ||| 'canine':1-2\n",
      "[X]:1-2 ||| '-EPS-':1-2\n",
      "[X]:1-2 ||| 'wolf':1-2\n",
      "[X]:1-2 ||| 'puppy':1-2\n",
      "[X]:1-2 ||| 'dog':1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-2 ||| [X]:2-2 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:1-1\n",
      "[X]:3-3 ||| 'the':3-3\n",
      "[X]:3-3 ||| '.':3-3\n",
      "[X]:3-3 ||| 'some':3-3\n",
      "[X]:3-3 ||| 'a':3-3\n",
      "[X]:3-3 ||| 'of':3-3\n",
      "[X]:3-3 ||| [X]:3-3 [X]:3-3\n",
      "[X]:2-3 ||| 'dark':2-3\n",
      "[X]:2-3 ||| '-EPS-':2-3\n",
      "[X]:2-3 ||| 'black':2-3\n",
      "[X]:2-3 ||| 'void':2-3\n",
      "[X]:2-3 ||| 'noir':2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:3-3\n",
      "[X]:2-3 ||| [X]:3-3 [X]:2-3\n",
      "[X]:2-3 ||| [X]:2-2 [X]:2-3\n",
      "[X]:2-3 ||| [X]:2-3 [X]:2-2\n",
      "[X]:1-1 ||| 'the':1-1\n",
      "[X]:1-1 ||| '.':1-1\n",
      "[X]:1-1 ||| 'some':1-1\n",
      "[X]:1-1 ||| 'a':1-1\n",
      "[X]:1-1 ||| 'of':1-1\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n",
      "[X]:0-2 ||| [X]:1-2 [X]:0-1\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:0-2 ||| [X]:2-2 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-2 [X]:0-0\n",
      "[X]:1-3 ||| [X]:1-2 [X]:2-3\n",
      "[X]:1-3 ||| [X]:2-3 [X]:1-2\n",
      "[X]:1-3 ||| [X]:1-3 [X]:3-3\n",
      "[X]:1-3 ||| [X]:3-3 [X]:1-3\n",
      "[X]:1-3 ||| [X]:1-1 [X]:1-3\n",
      "[X]:1-3 ||| [X]:1-3 [X]:1-1\n",
      "[X]:0-0 ||| 'the':0-0\n",
      "[X]:0-0 ||| '.':0-0\n",
      "[X]:0-0 ||| 'some':0-0\n",
      "[X]:0-0 ||| 'a':0-0\n",
      "[X]:0-0 ||| 'of':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[D(x)] ||| [S]:0-3\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "# Now we have D(x) expressed over the target vocabulary\n",
    "Dx = libitg.make_target_side_itg(_Dx, lexicon)\n",
    "print(Dx)\n",
    "print(len(Dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with a translation observation\n",
    "\n",
    "In training, we can observe translations, thus we need to constrain \\\\(\\mathcal D(x)\\\\) to the set of derivations that in addition to \\\\(x\\\\) also produce our target observation \\\\(y\\\\).\n",
    "\n",
    "We do that by intersecting the forest for \\\\(\\mathcal D(x)\\\\) with an automaton that represents \\\\(y\\\\).\n",
    "\n",
    "That is, we first make an FSA for the target sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=3\n",
      "arcs=3\n",
      "origin=0 destination=1 label=the\n",
      "origin=1 destination=2 label=black\n",
      "origin=2 destination=3 label=dog\n"
     ]
    }
   ],
   "source": [
    "tgt_fsa = libitg.make_fsa('the black dog')\n",
    "print(tgt_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then parse this FSA with the projected forest we got earlier from Earley.\n",
    "The resulting forest will represent the set \\\\(\\mathcal D(x,y)\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "[D(x,y)] ||| [D(x)]:0-3\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n",
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "Dxy = libitg.earley(Dx, tgt_fsa,\n",
    "                    start_symbol=Nonterminal(\"D(x)\"), \n",
    "                    sprime_symbol=Nonterminal('D(x,y)'))\n",
    "print(Dxy)\n",
    "print(len(Dxy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the black dog'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is just to illustrate that ref_forest accepts a single target (English) string\n",
    "libitg.language_of_fsa(libitg.forest_to_fsa(Dxy, Nonterminal('D(x,y)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length constraint\n",
    "\n",
    "In order to learn our CRF for maximum likelihood we need to compute the gradient of the log-likelihood.\n",
    "\n",
    "The gradient invoves two expectations (see lecture notes and project description). One expectation summarises all ways in which we can derive the joint observation \\\\((x, y)\\\\), the other summarises all ways in which we can derive the incomplete observation \\\\(x, n\\\\). The former is computed out of the forest that represents \\\\(\\mathcal D(x, y)\\\\) and the latter is computed out of the forest that represents \\\\(\\mathcal D_n(x)\\\\).\n",
    "\n",
    "Here we will show you how to get \\\\(\\mathcal D_n(x)\\\\) by parsing a special automaton that accepts the language \\\\(\\Delta^n\\\\) where \\\\(\\Delta\\\\) is the vocabulary of the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `strict` controls whether the constraint is |yield(d)| == n (strict=True) or |yield(d)| <= n (strict=False)\n",
    "length_fsa = libitg.LengthConstraint(4, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This special automaton accepts strings containing 1 to 4 words, no matter which words. The label -WILDCARD- is a special label such that `x == -WILDCARD-` evaluates to `True` no matter the value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=5\n",
      "initial=0\n",
      "final=0 1 2 3 4\n",
      "arcs=4\n",
      "origin=0 destination=1 label=-WILDCARD-\n",
      "origin=1 destination=2 label=-WILDCARD-\n",
      "origin=2 destination=3 label=-WILDCARD-\n",
      "origin=3 destination=4 label=-WILDCARD-\n"
     ]
    }
   ],
   "source": [
    "print(length_fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:1-3:1-4 ||| [X]:1-1:1-4 [X]:1-3:4-4\n",
      "[X]:1-3:1-4 ||| [X]:1-1:1-3 [X]:1-3:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-1:1-2 [X]:1-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-2 [X]:1-1:2-4\n",
      "[X]:1-3:1-4 ||| [X]:3-3:1-3 [X]:1-3:3-4\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-3 [X]:1-2:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-1 [X]:3-3:1-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-3 [X]:2-3:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-3 [X]:1-1:3-4\n",
      "[X]:1-3:1-4 ||| [X]:3-3:1-4 [X]:1-3:4-4\n",
      "[X]:1-3:1-4 ||| [X]:3-3:1-2 [X]:1-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-3 [X]:3-3:3-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-1 [X]:1-1:1-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-2 [X]:2-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-1 [X]:2-3:1-4\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-4 [X]:1-2:4-4\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-2 [X]:1-2:2-4\n",
      "[X]:1-3:1-4 ||| [X]:1-2:1-4 [X]:2-3:4-4\n",
      "[X]:1-3:1-4 ||| [X]:1-3:1-2 [X]:3-3:2-4\n",
      "[X]:1-3:1-4 ||| [X]:2-3:1-1 [X]:1-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-4 [X]:0-2:4-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-3 [X]:0-0:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-1 [X]:2-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-2 [X]:0-0:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-3 [X]:1-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-1 [X]:1-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-4 [X]:1-2:4-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-2 [X]:0-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-2 [X]:1-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-1 [X]:0-0:1-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-3 [X]:0-1:3-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-3 [X]:0-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-1 [X]:0-1:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-0 [X]:0-0:0-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-1 [X]:0-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-4 [X]:0-1:4-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-3 [X]:0-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-4 [X]:0-2:4-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-2 [X]:2-2:2-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-0 [X]:0-1:0-4\n",
      "[X]:0-2:0-4 ||| [X]:0-0:0-1 [X]:0-2:1-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-3 [X]:2-2:3-4\n",
      "[X]:0-2:0-4 ||| [X]:1-2:0-2 [X]:0-1:2-4\n",
      "[X]:0-2:0-4 ||| [X]:0-2:0-0 [X]:2-2:0-4\n",
      "[X]:0-2:0-4 ||| [X]:0-1:0-0 [X]:1-2:0-4\n",
      "[X]:0-2:0-4 ||| [X]:2-2:0-2 [X]:0-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-1:1-4 [X]:1-2:4-4\n",
      "[X]:1-2:1-4 ||| [X]:1-1:1-3 [X]:1-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:2-2:1-3 [X]:1-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-3 [X]:1-1:3-4\n",
      "[X]:1-2:1-4 ||| [X]:2-2:1-2 [X]:1-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-3 [X]:2-2:3-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-1 [X]:2-2:1-4\n",
      "[X]:1-2:1-4 ||| [X]:1-1:1-2 [X]:1-2:2-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-2 [X]:1-1:2-4\n",
      "[X]:1-2:1-4 ||| [X]:2-2:1-4 [X]:1-2:4-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-1 [X]:1-1:1-4\n",
      "[X]:1-2:1-4 ||| [X]:1-2:1-2 [X]:2-2:2-4\n",
      "[X]:3-3:1-3 ||| [X]:3-3:1-2 [X]:3-3:2-3\n",
      "[X]:2-2:3-4 ||| 'of':2-2:3-4\n",
      "[X]:2-2:3-4 ||| 'a':2-2:3-4\n",
      "[X]:2-2:3-4 ||| 'some':2-2:3-4\n",
      "[X]:2-2:3-4 ||| '.':2-2:3-4\n",
      "[X]:2-2:3-4 ||| 'the':2-2:3-4\n",
      "[X]:0-2:1-1 ||| [X]:1-2:1-1 [X]:0-1:1-1\n",
      "[X]:0-2:1-1 ||| [X]:0-1:1-1 [X]:1-2:1-1\n",
      "[X]:0-1:3-4 ||| [X]:0-1:3-3 [X]:0-0:3-4\n",
      "[X]:0-1:3-4 ||| [X]:0-0:3-4 [X]:0-1:4-4\n",
      "[X]:0-1:3-4 ||| 'an':0-1:3-4\n",
      "[X]:0-1:3-4 ||| 'a':0-1:3-4\n",
      "[X]:0-1:3-4 ||| [X]:0-1:3-3 [X]:1-1:3-4\n",
      "[X]:0-1:3-4 ||| 'some':0-1:3-4\n",
      "[X]:0-1:3-4 ||| 'the':0-1:3-4\n",
      "[X]:0-1:3-4 ||| [X]:1-1:3-4 [X]:0-1:4-4\n",
      "[X]:0-0:0-1 ||| 'of':0-0:0-1\n",
      "[X]:0-0:0-1 ||| 'a':0-0:0-1\n",
      "[X]:0-0:0-1 ||| 'some':0-0:0-1\n",
      "[X]:0-0:0-1 ||| '.':0-0:0-1\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "[X]:1-1:2-4 ||| [X]:1-1:2-3 [X]:1-1:3-4\n",
      "[X]:1-2:1-1 ||| '-EPS-':1-2:1-1\n",
      "[X]:1-2:2-2 ||| '-EPS-':1-2:2-2\n",
      "[X]:1-2:0-1 ||| 'puppy':1-2:0-1\n",
      "[X]:1-2:0-1 ||| 'dog':1-2:0-1\n",
      "[X]:1-2:0-1 ||| [X]:1-1:0-1 [X]:1-2:1-1\n",
      "[X]:1-2:0-1 ||| [X]:1-2:0-0 [X]:2-2:0-1\n",
      "[X]:1-2:0-1 ||| [X]:1-2:0-0 [X]:1-1:0-1\n",
      "[X]:1-2:0-1 ||| 'canine':1-2:0-1\n",
      "[X]:1-2:0-1 ||| [X]:2-2:0-1 [X]:1-2:1-1\n",
      "[X]:1-2:0-1 ||| 'wolf':1-2:0-1\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-3 [X]:0-0:3-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-2 [X]:0-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-3 [X]:1-1:3-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-2 [X]:0-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-2 [X]:0-0:2-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-4 [X]:0-1:4-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-1 [X]:1-1:1-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-2 [X]:1-1:2-4\n",
      "[X]:0-1:1-4 ||| [X]:0-1:1-1 [X]:0-0:1-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-4 [X]:0-1:4-4\n",
      "[X]:0-1:1-4 ||| [X]:0-0:1-3 [X]:0-1:3-4\n",
      "[X]:0-1:1-4 ||| [X]:1-1:1-3 [X]:0-1:3-4\n",
      "[X]:2-3:3-4 ||| [X]:3-3:3-4 [X]:2-3:4-4\n",
      "[X]:2-3:3-4 ||| [X]:2-3:3-3 [X]:3-3:3-4\n",
      "[X]:2-3:3-4 ||| [X]:2-3:3-3 [X]:2-2:3-4\n",
      "[X]:2-3:3-4 ||| [X]:2-2:3-4 [X]:2-3:4-4\n",
      "[X]:2-3:3-4 ||| 'noir':2-3:3-4\n",
      "[X]:2-3:3-4 ||| 'void':2-3:3-4\n",
      "[X]:2-3:3-4 ||| 'black':2-3:3-4\n",
      "[X]:2-3:3-4 ||| 'dark':2-3:3-4\n",
      "[X]:3-3:3-4 ||| 'of':3-3:3-4\n",
      "[X]:3-3:3-4 ||| 'a':3-3:3-4\n",
      "[X]:3-3:3-4 ||| 'some':3-3:3-4\n",
      "[X]:3-3:3-4 ||| '.':3-3:3-4\n",
      "[X]:3-3:3-4 ||| 'the':3-3:3-4\n",
      "[X]:0-3:3-3 ||| [X]:1-3:3-3 [X]:0-1:3-3\n",
      "[X]:0-3:3-3 ||| [X]:0-1:3-3 [X]:1-3:3-3\n",
      "[X]:0-3:3-3 ||| [X]:2-3:3-3 [X]:0-2:3-3\n",
      "[X]:0-3:3-3 ||| [X]:0-2:3-3 [X]:2-3:3-3\n",
      "[X]:0-1:1-3 ||| [X]:0-0:1-3 [X]:0-1:3-3\n",
      "[X]:0-1:1-3 ||| [X]:1-1:1-3 [X]:0-1:3-3\n",
      "[X]:0-1:1-3 ||| [X]:1-1:1-2 [X]:0-1:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-0:1-2 [X]:0-1:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-2 [X]:0-0:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-1 [X]:1-1:1-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-2 [X]:1-1:2-3\n",
      "[X]:0-1:1-3 ||| [X]:0-1:1-1 [X]:0-0:1-3\n",
      "[X]:0-2:3-3 ||| [X]:1-2:3-3 [X]:0-1:3-3\n",
      "[X]:0-2:3-3 ||| [X]:0-1:3-3 [X]:1-2:3-3\n",
      "[X]:2-3:3-3 ||| '-EPS-':2-3:3-3\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-2 [X]:1-3:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "[X]:0-3:1-3 ||| [X]:0-3:1-2 [X]:3-3:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-0:1-2 [X]:0-3:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-3:1-2 [X]:0-0:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-3:1-1 [X]:3-3:1-3\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-1 [X]:0-2:1-3\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-2 [X]:0-1:2-3\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-1 [X]:0-1:1-3\n",
      "[X]:0-3:1-3 ||| [X]:0-3:1-1 [X]:0-0:1-3\n",
      "[X]:0-3:1-3 ||| [X]:3-3:1-3 [X]:0-3:3-3\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-3 [X]:0-2:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-2:1-3 [X]:2-3:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-0:1-3 [X]:0-3:3-3\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-3 [X]:1-3:3-3\n",
      "[X]:0-3:1-3 ||| [X]:3-3:1-2 [X]:0-3:2-3\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-2:1-2 [X]:2-3:2-3\n",
      "[X]:0-3:1-3 ||| [X]:0-2:1-1 [X]:2-3:1-3\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-2 [X]:0-0:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-4 [X]:1-3:4-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-3 [X]:3-3:3-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-1 [X]:0-1:1-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-2 [X]:1-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-4 [X]:2-3:4-4\n",
      "[X]:0-3:1-4 ||| [X]:0-0:1-2 [X]:0-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-1 [X]:3-3:1-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-4 [X]:0-1:4-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-3 [X]:0-2:3-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-4 [X]:0-2:4-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-2 [X]:0-1:2-4\n",
      "[X]:0-3:1-4 ||| [X]:3-3:1-3 [X]:0-3:3-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-1 [X]:0-2:1-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-3 [X]:2-3:3-4\n",
      "[X]:0-3:1-4 ||| [X]:0-0:1-3 [X]:0-3:3-4\n",
      "[X]:0-3:1-4 ||| [X]:2-3:1-2 [X]:0-2:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-1 [X]:2-3:1-4\n",
      "[X]:0-3:1-4 ||| [X]:1-3:1-3 [X]:0-1:3-4\n",
      "[X]:0-3:1-4 ||| [X]:3-3:1-4 [X]:0-3:4-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-1 [X]:0-0:1-4\n",
      "[X]:0-3:1-4 ||| [X]:0-0:1-4 [X]:0-3:4-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-3 [X]:1-3:3-4\n",
      "[X]:0-3:1-4 ||| [X]:3-3:1-2 [X]:0-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-2:1-2 [X]:2-3:2-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-3 [X]:0-0:3-4\n",
      "[X]:0-3:1-4 ||| [X]:0-1:1-1 [X]:1-3:1-4\n",
      "[X]:0-3:1-4 ||| [X]:0-3:1-2 [X]:3-3:2-4\n",
      "[X]:1-1:0-2 ||| [X]:1-1:0-1 [X]:1-1:1-2\n",
      "[X]:0-3:0-1 ||| [X]:1-3:0-1 [X]:0-1:1-1\n",
      "[X]:0-3:0-1 ||| [X]:0-1:0-0 [X]:1-3:0-1\n",
      "[X]:0-3:0-1 ||| [X]:2-3:0-1 [X]:0-2:1-1\n",
      "[X]:0-3:0-1 ||| [X]:0-0:0-1 [X]:0-3:1-1\n",
      "[X]:0-3:0-1 ||| [X]:0-3:0-0 [X]:0-0:0-1\n",
      "[X]:0-3:0-1 ||| [X]:0-1:0-1 [X]:1-3:1-1\n",
      "[X]:0-3:0-1 ||| [X]:2-3:0-0 [X]:0-2:0-1\n",
      "[X]:0-3:0-1 ||| [X]:0-2:0-0 [X]:2-3:0-1\n",
      "[X]:0-3:0-1 ||| [X]:1-3:0-0 [X]:0-1:0-1\n",
      "[X]:0-3:0-1 ||| [X]:3-3:0-1 [X]:0-3:1-1\n",
      "[X]:0-3:0-1 ||| [X]:0-2:0-1 [X]:2-3:1-1\n",
      "[X]:0-3:0-1 ||| [X]:0-3:0-0 [X]:3-3:0-1\n",
      "[X]:1-2:1-2 ||| [X]:1-1:1-2 [X]:1-2:2-2\n",
      "[X]:1-2:1-2 ||| 'wolf':1-2:1-2\n",
      "[X]:1-2:1-2 ||| [X]:1-2:1-1 [X]:1-1:1-2\n",
      "[X]:1-2:1-2 ||| 'canine':1-2:1-2\n",
      "[X]:1-2:1-2 ||| 'puppy':1-2:1-2\n",
      "[X]:1-2:1-2 ||| 'dog':1-2:1-2\n",
      "[X]:1-2:1-2 ||| [X]:2-2:1-2 [X]:1-2:2-2\n",
      "[X]:1-2:1-2 ||| [X]:1-2:1-1 [X]:2-2:1-2\n",
      "[X]:0-2:0-1 ||| [X]:1-2:0-0 [X]:0-1:0-1\n",
      "[X]:0-2:0-1 ||| [X]:0-0:0-1 [X]:0-2:1-1\n",
      "[X]:0-2:0-1 ||| [X]:1-2:0-1 [X]:0-1:1-1\n",
      "[X]:0-2:0-1 ||| [X]:0-2:0-0 [X]:2-2:0-1\n",
      "[X]:0-2:0-1 ||| [X]:0-2:0-0 [X]:0-0:0-1\n",
      "[X]:0-2:0-1 ||| [X]:0-1:0-0 [X]:1-2:0-1\n",
      "[X]:0-2:0-1 ||| [X]:2-2:0-1 [X]:0-2:1-1\n",
      "[X]:0-2:0-1 ||| [X]:0-1:0-1 [X]:1-2:1-1\n",
      "[X]:0-2:3-4 ||| [X]:0-0:3-4 [X]:0-2:4-4\n",
      "[X]:0-2:3-4 ||| [X]:0-1:3-3 [X]:1-2:3-4\n",
      "[X]:0-2:3-4 ||| [X]:2-2:3-4 [X]:0-2:4-4\n",
      "[X]:0-2:3-4 ||| [X]:0-1:3-4 [X]:1-2:4-4\n",
      "[X]:0-2:3-4 ||| [X]:1-2:3-4 [X]:0-1:4-4\n",
      "[X]:0-2:3-4 ||| [X]:1-2:3-3 [X]:0-1:3-4\n",
      "[X]:0-2:3-4 ||| [X]:0-2:3-3 [X]:2-2:3-4\n",
      "[X]:0-2:3-4 ||| [X]:0-2:3-3 [X]:0-0:3-4\n",
      "[X]:1-1:3-4 ||| 'of':1-1:3-4\n",
      "[X]:1-1:3-4 ||| 'a':1-1:3-4\n",
      "[X]:1-1:3-4 ||| 'some':1-1:3-4\n",
      "[X]:1-1:3-4 ||| '.':1-1:3-4\n",
      "[X]:1-1:3-4 ||| 'the':1-1:3-4\n",
      "[X]:0-0:2-3 ||| 'of':0-0:2-3\n",
      "[X]:0-0:2-3 ||| 'a':0-0:2-3\n",
      "[X]:0-0:2-3 ||| 'some':0-0:2-3\n",
      "[X]:0-0:2-3 ||| '.':0-0:2-3\n",
      "[X]:0-0:2-3 ||| 'the':0-0:2-3\n",
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-1 [X]:0-0:1-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-1 [X]:1-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:0-0:0-2 [X]:0-1:2-2\n",
      "[X]:0-1:0-2 ||| [X]:0-0:0-1 [X]:0-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:1-1:0-2 [X]:0-1:2-2\n",
      "[X]:0-1:0-2 ||| [X]:1-1:0-1 [X]:0-1:1-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-0 [X]:1-1:0-2\n",
      "[X]:0-1:0-2 ||| [X]:0-1:0-0 [X]:0-0:0-2\n",
      "[X]:1-1:0-1 ||| 'of':1-1:0-1\n",
      "[X]:1-1:0-1 ||| 'a':1-1:0-1\n",
      "[X]:1-1:0-1 ||| 'some':1-1:0-1\n",
      "[X]:1-1:0-1 ||| '.':1-1:0-1\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "[X]:3-3:1-4 ||| [X]:3-3:1-2 [X]:3-3:2-4\n",
      "[X]:3-3:1-4 ||| [X]:3-3:1-3 [X]:3-3:3-4\n",
      "[X]:2-2:2-3 ||| 'of':2-2:2-3\n",
      "[X]:2-2:2-3 ||| 'a':2-2:2-3\n",
      "[X]:2-2:2-3 ||| 'some':2-2:2-3\n",
      "[X]:2-2:2-3 ||| '.':2-2:2-3\n",
      "[X]:2-2:2-3 ||| 'the':2-2:2-3\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-2 [X]:1-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-0 [X]:3-3:0-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-1 [X]:1-1:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-0 [X]:2-3:0-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-3 [X]:1-2:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-2 [X]:2-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-2 [X]:1-2:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-3 [X]:1-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-2 [X]:1-1:2-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-1 [X]:1-2:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-2 [X]:1-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-2 [X]:3-3:2-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-3 [X]:1-1:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-3 [X]:3-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-4 [X]:1-3:4-4\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-1 [X]:1-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-4 [X]:1-2:4-4\n",
      "[X]:1-3:0-4 ||| [X]:2-3:0-0 [X]:1-2:0-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-0 [X]:1-1:0-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-4 [X]:1-3:4-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-3 [X]:2-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-1:0-1 [X]:1-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-3:0-1 [X]:3-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-1 [X]:2-3:1-4\n",
      "[X]:1-3:0-4 ||| [X]:3-3:0-3 [X]:1-3:3-4\n",
      "[X]:1-3:0-4 ||| [X]:1-2:0-4 [X]:2-3:4-4\n",
      "[X]:2-3:2-2 ||| '-EPS-':2-3:2-2\n",
      "[X]:1-1:1-4 ||| [X]:1-1:1-2 [X]:1-1:2-4\n",
      "[X]:1-1:1-4 ||| [X]:1-1:1-3 [X]:1-1:3-4\n",
      "[X]:0-3:0-2 ||| [X]:0-0:0-2 [X]:0-3:2-2\n",
      "[X]:0-3:0-2 ||| [X]:1-3:0-2 [X]:0-1:2-2\n",
      "[X]:0-3:0-2 ||| [X]:0-3:0-0 [X]:0-0:0-2\n",
      "[X]:0-3:0-2 ||| [X]:0-1:0-2 [X]:1-3:2-2\n",
      "[X]:0-3:0-2 ||| [X]:0-1:0-1 [X]:1-3:1-2\n",
      "[X]:0-3:0-2 ||| [X]:0-2:0-0 [X]:2-3:0-2\n",
      "[X]:0-3:0-2 ||| [X]:3-3:0-2 [X]:0-3:2-2\n",
      "[X]:0-3:0-2 ||| [X]:3-3:0-1 [X]:0-3:1-2\n",
      "[X]:0-3:0-2 ||| [X]:2-3:0-0 [X]:0-2:0-2\n",
      "[X]:0-3:0-2 ||| [X]:0-2:0-2 [X]:2-3:2-2\n",
      "[X]:0-3:0-2 ||| [X]:0-2:0-1 [X]:2-3:1-2\n",
      "[X]:0-3:0-2 ||| [X]:0-3:0-1 [X]:0-0:1-2\n",
      "[X]:0-3:0-2 ||| [X]:1-3:0-0 [X]:0-1:0-2\n",
      "[X]:0-3:0-2 ||| [X]:0-3:0-1 [X]:3-3:1-2\n",
      "[X]:0-3:0-2 ||| [X]:0-3:0-0 [X]:3-3:0-2\n",
      "[X]:0-3:0-2 ||| [X]:0-0:0-1 [X]:0-3:1-2\n",
      "[X]:0-3:0-2 ||| [X]:1-3:0-1 [X]:0-1:1-2\n",
      "[X]:0-3:0-2 ||| [X]:0-1:0-0 [X]:1-3:0-2\n",
      "[X]:0-3:0-2 ||| [X]:2-3:0-2 [X]:0-2:2-2\n",
      "[X]:0-3:0-2 ||| [X]:2-3:0-1 [X]:0-2:1-2\n",
      "[X]:0-3:4-4 ||| [X]:1-3:4-4 [X]:0-1:4-4\n",
      "[X]:0-3:4-4 ||| [X]:0-1:4-4 [X]:1-3:4-4\n",
      "[X]:0-3:4-4 ||| [X]:2-3:4-4 [X]:0-2:4-4\n",
      "[X]:0-3:4-4 ||| [X]:0-2:4-4 [X]:2-3:4-4\n",
      "[X]:0-1:2-3 ||| 'an':0-1:2-3\n",
      "[X]:0-1:2-3 ||| [X]:0-1:2-2 [X]:0-0:2-3\n",
      "[X]:0-1:2-3 ||| [X]:0-0:2-3 [X]:0-1:3-3\n",
      "[X]:0-1:2-3 ||| [X]:1-1:2-3 [X]:0-1:3-3\n",
      "[X]:0-1:2-3 ||| [X]:0-1:2-2 [X]:1-1:2-3\n",
      "[X]:0-1:2-3 ||| 'the':0-1:2-3\n",
      "[X]:0-1:2-3 ||| 'a':0-1:2-3\n",
      "[X]:0-1:2-3 ||| 'some':0-1:2-3\n",
      "[X]:1-3:3-3 ||| [X]:2-3:3-3 [X]:1-2:3-3\n",
      "[X]:1-3:3-3 ||| [X]:1-2:3-3 [X]:2-3:3-3\n",
      "[X]:0-3:3-4 ||| [X]:1-3:3-4 [X]:0-1:4-4\n",
      "[X]:0-3:3-4 ||| [X]:0-1:3-3 [X]:1-3:3-4\n",
      "[X]:0-3:3-4 ||| [X]:2-3:3-4 [X]:0-2:4-4\n",
      "[X]:0-3:3-4 ||| [X]:0-3:3-3 [X]:3-3:3-4\n",
      "[X]:0-3:3-4 ||| [X]:2-3:3-3 [X]:0-2:3-4\n",
      "[X]:0-3:3-4 ||| [X]:0-2:3-3 [X]:2-3:3-4\n",
      "[X]:0-3:3-4 ||| [X]:0-3:3-3 [X]:0-0:3-4\n",
      "[X]:0-3:3-4 ||| [X]:0-1:3-4 [X]:1-3:4-4\n",
      "[X]:0-3:3-4 ||| [X]:3-3:3-4 [X]:0-3:4-4\n",
      "[X]:0-3:3-4 ||| [X]:0-0:3-4 [X]:0-3:4-4\n",
      "[X]:0-3:3-4 ||| [X]:1-3:3-3 [X]:0-1:3-4\n",
      "[X]:0-3:3-4 ||| [X]:0-2:3-4 [X]:2-3:4-4\n",
      "[X]:1-3:2-2 ||| [X]:2-3:2-2 [X]:1-2:2-2\n",
      "[X]:1-3:2-2 ||| [X]:1-2:2-2 [X]:2-3:2-2\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-3 [X]:0-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-1 [X]:0-0:1-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-4 [X]:0-3:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-0 [X]:3-3:0-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-2 [X]:0-0:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-3 [X]:3-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-4 [X]:0-1:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-0 [X]:0-0:0-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-2 [X]:0-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-1 [X]:3-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-1 [X]:0-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-2 [X]:0-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-3 [X]:2-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-1 [X]:0-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-0 [X]:0-2:0-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-1 [X]:2-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-3 [X]:0-1:3-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-3 [X]:0-2:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-4 [X]:2-3:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-3 [X]:1-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-1 [X]:0-2:1-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-2 [X]:0-1:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-1 [X]:1-3:1-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-4 [X]:0-2:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-0 [X]:2-3:0-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-4 [X]:1-3:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-2:0-2 [X]:2-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-3 [X]:0-0:3-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-0 [X]:0-1:0-4\n",
      "[X]:0-3:0-4 ||| [X]:1-3:0-1 [X]:0-1:1-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-0 [X]:1-3:0-4\n",
      "[X]:0-3:0-4 ||| [X]:2-3:0-2 [X]:0-2:2-4\n",
      "[X]:0-3:0-4 ||| [X]:0-0:0-4 [X]:0-3:4-4\n",
      "[X]:0-3:0-4 ||| [X]:0-1:0-2 [X]:1-3:2-4\n",
      "[X]:0-3:0-4 ||| [X]:3-3:0-3 [X]:0-3:3-4\n",
      "[X]:0-3:0-4 ||| [X]:0-3:0-2 [X]:3-3:2-4\n",
      "[X]:1-3:1-2 ||| [X]:2-3:1-2 [X]:1-2:2-2\n",
      "[X]:1-3:1-2 ||| [X]:2-3:1-1 [X]:1-2:1-2\n",
      "[X]:1-3:1-2 ||| [X]:1-1:1-2 [X]:1-3:2-2\n",
      "[X]:1-3:1-2 ||| [X]:1-3:1-1 [X]:3-3:1-2\n",
      "[X]:1-3:1-2 ||| [X]:3-3:1-2 [X]:1-3:2-2\n",
      "[X]:1-3:1-2 ||| [X]:1-3:1-1 [X]:1-1:1-2\n",
      "[X]:1-3:1-2 ||| [X]:1-2:1-2 [X]:2-3:2-2\n",
      "[X]:1-3:1-2 ||| [X]:1-2:1-1 [X]:2-3:1-2\n",
      "[X]:0-2:0-0 ||| [X]:1-2:0-0 [X]:0-1:0-0\n",
      "[X]:0-2:0-0 ||| [X]:0-1:0-0 [X]:1-2:0-0\n",
      "[X]:0-2:1-3 ||| [X]:2-2:1-2 [X]:0-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:1-2:1-2 [X]:0-1:2-3\n",
      "[X]:0-2:1-3 ||| [X]:1-2:1-1 [X]:0-1:1-3\n",
      "[X]:0-2:1-3 ||| [X]:0-1:1-3 [X]:1-2:3-3\n",
      "[X]:0-2:1-3 ||| [X]:0-0:1-2 [X]:0-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-2 [X]:0-0:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-1:1-2 [X]:1-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-1:1-1 [X]:1-2:1-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-2 [X]:2-2:2-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-1 [X]:2-2:1-3\n",
      "[X]:0-2:1-3 ||| [X]:0-0:1-3 [X]:0-2:3-3\n",
      "[X]:0-2:1-3 ||| [X]:2-2:1-3 [X]:0-2:3-3\n",
      "[X]:0-2:1-3 ||| [X]:1-2:1-3 [X]:0-1:3-3\n",
      "[X]:0-2:1-3 ||| [X]:0-2:1-1 [X]:0-0:1-3\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-2 [X]:0-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-1 [X]:0-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-0 [X]:1-1:0-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-0 [X]:0-0:0-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-4 [X]:0-1:4-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-3 [X]:0-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-4 [X]:0-1:4-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-2 [X]:1-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-3 [X]:0-0:3-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-1 [X]:0-0:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-1 [X]:1-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-2 [X]:0-1:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-2 [X]:0-0:2-4\n",
      "[X]:0-1:0-4 ||| [X]:0-0:0-1 [X]:0-1:1-4\n",
      "[X]:0-1:0-4 ||| [X]:1-1:0-3 [X]:0-1:3-4\n",
      "[X]:0-1:0-4 ||| [X]:0-1:0-3 [X]:1-1:3-4\n",
      "[X]:0-3:1-1 ||| [X]:1-3:1-1 [X]:0-1:1-1\n",
      "[X]:0-3:1-1 ||| [X]:0-1:1-1 [X]:1-3:1-1\n",
      "[X]:0-3:1-1 ||| [X]:2-3:1-1 [X]:0-2:1-1\n",
      "[X]:0-3:1-1 ||| [X]:0-2:1-1 [X]:2-3:1-1\n",
      "[X]:1-1:2-3 ||| 'of':1-1:2-3\n",
      "[X]:1-1:2-3 ||| 'a':1-1:2-3\n",
      "[X]:1-1:2-3 ||| 'some':1-1:2-3\n",
      "[X]:1-1:2-3 ||| '.':1-1:2-3\n",
      "[X]:1-1:2-3 ||| 'the':1-1:2-3\n",
      "[X]:0-0:1-3 ||| [X]:0-0:1-2 [X]:0-0:2-3\n",
      "[X]:0-0:2-4 ||| [X]:0-0:2-3 [X]:0-0:3-4\n",
      "[X]:1-3:2-3 ||| [X]:2-3:2-3 [X]:1-2:3-3\n",
      "[X]:1-3:2-3 ||| [X]:2-3:2-2 [X]:1-2:2-3\n",
      "[X]:1-3:2-3 ||| [X]:1-2:2-2 [X]:2-3:2-3\n",
      "[X]:1-3:2-3 ||| [X]:1-3:2-2 [X]:1-1:2-3\n",
      "[X]:1-3:2-3 ||| [X]:3-3:2-3 [X]:1-3:3-3\n",
      "[X]:1-3:2-3 ||| [X]:1-1:2-3 [X]:1-3:3-3\n",
      "[X]:1-3:2-3 ||| [X]:1-3:2-2 [X]:3-3:2-3\n",
      "[X]:1-3:2-3 ||| [X]:1-2:2-3 [X]:2-3:3-3\n",
      "[X]:0-2:0-3 ||| [X]:2-2:0-2 [X]:0-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:2-2:0-1 [X]:0-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-2 [X]:1-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-1 [X]:1-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-0 [X]:1-2:0-3\n",
      "[X]:0-2:0-3 ||| [X]:0-1:0-3 [X]:1-2:3-3\n",
      "[X]:0-2:0-3 ||| [X]:0-0:0-3 [X]:0-2:3-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-2 [X]:2-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-1 [X]:0-0:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-1 [X]:2-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-3 [X]:0-1:3-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-0 [X]:0-1:0-3\n",
      "[X]:0-2:0-3 ||| [X]:0-0:0-2 [X]:0-2:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-2 [X]:0-0:2-3\n",
      "[X]:0-2:0-3 ||| [X]:0-0:0-1 [X]:0-2:1-3\n",
      "[X]:0-2:0-3 ||| [X]:2-2:0-3 [X]:0-2:3-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-2 [X]:0-1:2-3\n",
      "[X]:0-2:0-3 ||| [X]:1-2:0-1 [X]:0-1:1-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-0 [X]:2-2:0-3\n",
      "[X]:0-2:0-3 ||| [X]:0-2:0-0 [X]:0-0:0-3\n",
      "[X]:2-3:1-1 ||| '-EPS-':2-3:1-1\n",
      "[D(x)]:0-1 ||| [S]:0-3:0-1\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-1 [X]:1-1:1-2\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-1 [X]:2-2:1-2\n",
      "[X]:1-2:0-2 ||| [X]:1-1:0-2 [X]:1-2:2-2\n",
      "[X]:1-2:0-2 ||| [X]:1-1:0-1 [X]:1-2:1-2\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-0 [X]:2-2:0-2\n",
      "[X]:1-2:0-2 ||| [X]:1-2:0-0 [X]:1-1:0-2\n",
      "[X]:1-2:0-2 ||| [X]:2-2:0-2 [X]:1-2:2-2\n",
      "[X]:1-2:0-2 ||| [X]:2-2:0-1 [X]:1-2:1-2\n",
      "[D(x)]:0-0 ||| [S]:0-3:0-0\n",
      "[D_n(x)] ||| [D(x)]:0-0\n",
      "[D_n(x)] ||| [D(x)]:0-1\n",
      "[D_n(x)] ||| [D(x)]:0-2\n",
      "[D_n(x)] ||| [D(x)]:0-3\n",
      "[D_n(x)] ||| [D(x)]:0-4\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "[X]:0-1:1-2 ||| 'a':0-1:1-2\n",
      "[X]:0-1:1-2 ||| [X]:0-1:1-1 [X]:0-0:1-2\n",
      "[X]:0-1:1-2 ||| 'an':0-1:1-2\n",
      "[X]:0-1:1-2 ||| 'the':0-1:1-2\n",
      "[X]:0-1:1-2 ||| [X]:1-1:1-2 [X]:0-1:2-2\n",
      "[X]:0-1:1-2 ||| 'some':0-1:1-2\n",
      "[X]:0-1:1-2 ||| [X]:0-0:1-2 [X]:0-1:2-2\n",
      "[X]:0-1:1-2 ||| [X]:0-1:1-1 [X]:1-1:1-2\n",
      "[X]:1-2:4-4 ||| '-EPS-':1-2:4-4\n",
      "[X]:1-2:1-3 ||| [X]:1-2:1-2 [X]:2-2:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-1:1-3 [X]:1-2:3-3\n",
      "[X]:1-2:1-3 ||| [X]:2-2:1-3 [X]:1-2:3-3\n",
      "[X]:1-2:1-3 ||| [X]:2-2:1-2 [X]:1-2:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-2:1-1 [X]:2-2:1-3\n",
      "[X]:1-2:1-3 ||| [X]:1-1:1-2 [X]:1-2:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-2:1-2 [X]:1-1:2-3\n",
      "[X]:1-2:1-3 ||| [X]:1-2:1-1 [X]:1-1:1-3\n",
      "[X]:0-2:2-2 ||| [X]:1-2:2-2 [X]:0-1:2-2\n",
      "[X]:0-2:2-2 ||| [X]:0-1:2-2 [X]:1-2:2-2\n",
      "[X]:1-3:3-4 ||| [X]:1-1:3-4 [X]:1-3:4-4\n",
      "[X]:1-3:3-4 ||| [X]:3-3:3-4 [X]:1-3:4-4\n",
      "[X]:1-3:3-4 ||| [X]:1-2:3-4 [X]:2-3:4-4\n",
      "[X]:1-3:3-4 ||| [X]:2-3:3-4 [X]:1-2:4-4\n",
      "[X]:1-3:3-4 ||| [X]:2-3:3-3 [X]:1-2:3-4\n",
      "[X]:1-3:3-4 ||| [X]:1-3:3-3 [X]:3-3:3-4\n",
      "[X]:1-3:3-4 ||| [X]:1-3:3-3 [X]:1-1:3-4\n",
      "[X]:1-3:3-4 ||| [X]:1-2:3-3 [X]:2-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-3 [X]:2-2:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-1 [X]:2-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-3 [X]:2-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-3 [X]:3-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-0 [X]:3-3:0-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-0 [X]:2-2:0-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-2 [X]:2-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-1 [X]:2-3:1-4\n",
      "[X]:2-3:0-4 ||| [X]:3-3:0-4 [X]:2-3:4-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-2 [X]:3-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-4 [X]:2-3:4-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-3 [X]:2-3:3-4\n",
      "[X]:2-3:0-4 ||| [X]:2-2:0-2 [X]:2-3:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-2 [X]:2-2:2-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-1 [X]:2-2:1-4\n",
      "[X]:2-3:0-4 ||| [X]:2-3:0-1 [X]:3-3:1-4\n",
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "[S]:0-3:0-4 ||| [X]:0-3:0-4\n",
      "[X]:2-3:0-1 ||| [X]:3-3:0-1 [X]:2-3:1-1\n",
      "[X]:2-3:0-1 ||| 'black':2-3:0-1\n",
      "[X]:2-3:0-1 ||| 'void':2-3:0-1\n",
      "[X]:2-3:0-1 ||| 'dark':2-3:0-1\n",
      "[X]:2-3:0-1 ||| 'noir':2-3:0-1\n",
      "[X]:2-3:0-1 ||| [X]:2-2:0-1 [X]:2-3:1-1\n",
      "[X]:2-3:0-1 ||| [X]:2-3:0-0 [X]:3-3:0-1\n",
      "[X]:2-3:0-1 ||| [X]:2-3:0-0 [X]:2-2:0-1\n",
      "[X]:1-3:2-4 ||| [X]:1-1:2-3 [X]:1-3:3-4\n",
      "[X]:1-3:2-4 ||| [X]:2-3:2-4 [X]:1-2:4-4\n",
      "[X]:1-3:2-4 ||| [X]:2-3:2-2 [X]:1-2:2-4\n",
      "[X]:1-3:2-4 ||| [X]:1-3:2-2 [X]:1-1:2-4\n",
      "[X]:1-3:2-4 ||| [X]:1-2:2-4 [X]:2-3:4-4\n",
      "[X]:1-3:2-4 ||| [X]:1-3:2-3 [X]:3-3:3-4\n",
      "[X]:1-3:2-4 ||| [X]:3-3:2-4 [X]:1-3:4-4\n",
      "[X]:1-3:2-4 ||| [X]:1-3:2-2 [X]:3-3:2-4\n",
      "[X]:1-3:2-4 ||| [X]:1-2:2-3 [X]:2-3:3-4\n",
      "[X]:1-3:2-4 ||| [X]:1-1:2-4 [X]:1-3:4-4\n",
      "[X]:1-3:2-4 ||| [X]:1-3:2-3 [X]:1-1:3-4\n",
      "[X]:1-3:2-4 ||| [X]:3-3:2-3 [X]:1-3:3-4\n",
      "[X]:1-3:2-4 ||| [X]:2-3:2-3 [X]:1-2:3-4\n",
      "[X]:1-3:2-4 ||| [X]:1-2:2-2 [X]:2-3:2-4\n",
      "[D(x)]:0-2 ||| [S]:0-3:0-2\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-1 [X]:3-3:1-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-3 [X]:3-3:3-4\n",
      "[X]:3-3:0-4 ||| [X]:3-3:0-2 [X]:3-3:2-4\n",
      "[X]:2-2:0-1 ||| 'of':2-2:0-1\n",
      "[X]:2-2:0-1 ||| 'a':2-2:0-1\n",
      "[X]:2-2:0-1 ||| 'some':2-2:0-1\n",
      "[X]:2-2:0-1 ||| '.':2-2:0-1\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "[X]:1-1:1-2 ||| 'of':1-1:1-2\n",
      "[X]:1-1:1-2 ||| 'a':1-1:1-2\n",
      "[X]:1-1:1-2 ||| 'some':1-1:1-2\n",
      "[X]:1-1:1-2 ||| '.':1-1:1-2\n",
      "[X]:1-1:1-2 ||| 'the':1-1:1-2\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "[X]:0-3:2-4 ||| [X]:0-2:2-3 [X]:2-3:3-4\n",
      "[X]:0-3:2-4 ||| [X]:0-2:2-2 [X]:2-3:2-4\n",
      "[X]:0-3:2-4 ||| [X]:0-0:2-4 [X]:0-3:4-4\n",
      "[X]:0-3:2-4 ||| [X]:0-0:2-3 [X]:0-3:3-4\n",
      "[X]:0-3:2-4 ||| [X]:1-3:2-3 [X]:0-1:3-4\n",
      "[X]:0-3:2-4 ||| [X]:0-3:2-2 [X]:0-0:2-4\n",
      "[X]:0-3:2-4 ||| [X]:0-1:2-3 [X]:1-3:3-4\n",
      "[X]:0-3:2-4 ||| [X]:3-3:2-3 [X]:0-3:3-4\n",
      "[X]:0-3:2-4 ||| [X]:0-3:2-3 [X]:3-3:3-4\n",
      "[X]:0-3:2-4 ||| [X]:2-3:2-3 [X]:0-2:3-4\n",
      "[X]:0-3:2-4 ||| [X]:1-3:2-4 [X]:0-1:4-4\n",
      "[X]:0-3:2-4 ||| [X]:0-1:2-2 [X]:1-3:2-4\n",
      "[X]:0-3:2-4 ||| [X]:2-3:2-2 [X]:0-2:2-4\n",
      "[X]:0-3:2-4 ||| [X]:0-2:2-4 [X]:2-3:4-4\n",
      "[X]:0-3:2-4 ||| [X]:0-3:2-3 [X]:0-0:3-4\n",
      "[X]:0-3:2-4 ||| [X]:1-3:2-2 [X]:0-1:2-4\n",
      "[X]:0-3:2-4 ||| [X]:0-1:2-4 [X]:1-3:4-4\n",
      "[X]:0-3:2-4 ||| [X]:3-3:2-4 [X]:0-3:4-4\n",
      "[X]:0-3:2-4 ||| [X]:0-3:2-2 [X]:3-3:2-4\n",
      "[X]:0-3:2-4 ||| [X]:2-3:2-4 [X]:0-2:4-4\n",
      "[S]:0-3:0-0 ||| [X]:0-3:0-0\n",
      "[D(x)]:0-4 ||| [S]:0-3:0-4\n",
      "[X]:2-2:0-2 ||| [X]:2-2:0-1 [X]:2-2:1-2\n",
      "[X]:0-0:1-4 ||| [X]:0-0:1-2 [X]:0-0:2-4\n",
      "[X]:0-0:1-4 ||| [X]:0-0:1-3 [X]:0-0:3-4\n",
      "[X]:3-3:1-2 ||| 'of':3-3:1-2\n",
      "[X]:3-3:1-2 ||| 'a':3-3:1-2\n",
      "[X]:3-3:1-2 ||| 'some':3-3:1-2\n",
      "[X]:3-3:1-2 ||| '.':3-3:1-2\n",
      "[X]:3-3:1-2 ||| 'the':3-3:1-2\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-3 [X]:1-3:3-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-1 [X]:1-2:1-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-0 [X]:1-2:0-3\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-2 [X]:1-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-0 [X]:3-3:0-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-0 [X]:1-1:0-3\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-3 [X]:1-3:3-3\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-2 [X]:1-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-2 [X]:1-1:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-1 [X]:1-1:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-2 [X]:3-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-3 [X]:2-3:3-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-0 [X]:2-3:0-3\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:1-3:0-1 [X]:3-3:1-3\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-3 [X]:1-2:3-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-2 [X]:2-3:2-3\n",
      "[X]:1-3:0-3 ||| [X]:1-2:0-1 [X]:2-3:1-3\n",
      "[X]:0-2:2-4 ||| [X]:2-2:2-4 [X]:0-2:4-4\n",
      "[X]:0-2:2-4 ||| [X]:1-2:2-3 [X]:0-1:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-2 [X]:2-2:2-4\n",
      "[X]:0-2:2-4 ||| [X]:0-1:2-3 [X]:1-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-0:2-4 [X]:0-2:4-4\n",
      "[X]:0-2:2-4 ||| [X]:0-0:2-3 [X]:0-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:2-2:2-3 [X]:0-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:1-2:2-2 [X]:0-1:2-4\n",
      "[X]:0-2:2-4 ||| [X]:0-1:2-2 [X]:1-2:2-4\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-3 [X]:0-0:3-4\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-3 [X]:2-2:3-4\n",
      "[X]:0-2:2-4 ||| [X]:1-2:2-4 [X]:0-1:4-4\n",
      "[X]:0-2:2-4 ||| [X]:0-2:2-2 [X]:0-0:2-4\n",
      "[X]:0-2:2-4 ||| [X]:0-1:2-4 [X]:1-2:4-4\n",
      "[X]:1-2:0-3 ||| [X]:1-1:0-2 [X]:1-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-2 [X]:1-1:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-1:0-1 [X]:1-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:2-2:0-3 [X]:1-2:3-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-0 [X]:2-2:0-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-0 [X]:1-1:0-3\n",
      "[X]:1-2:0-3 ||| [X]:2-2:0-2 [X]:1-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:2-2:0-1 [X]:1-2:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-1:0-3 [X]:1-2:3-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-2 [X]:2-2:2-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-1 [X]:1-1:1-3\n",
      "[X]:1-2:0-3 ||| [X]:1-2:0-1 [X]:2-2:1-3\n",
      "[X]:3-3:0-3 ||| [X]:3-3:0-1 [X]:3-3:1-3\n",
      "[X]:3-3:0-3 ||| [X]:3-3:0-2 [X]:3-3:2-3\n",
      "[X]:1-3:1-1 ||| [X]:2-3:1-1 [X]:1-2:1-1\n",
      "[X]:1-3:1-1 ||| [X]:1-2:1-1 [X]:2-3:1-1\n",
      "[X]:0-2:4-4 ||| [X]:1-2:4-4 [X]:0-1:4-4\n",
      "[X]:0-2:4-4 ||| [X]:0-1:4-4 [X]:1-2:4-4\n",
      "[X]:0-2:1-4 ||| [X]:0-0:1-2 [X]:0-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-2 [X]:0-0:2-4\n",
      "[X]:0-2:1-4 ||| [X]:2-2:1-4 [X]:0-2:4-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-4 [X]:0-1:4-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-2 [X]:1-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-1 [X]:1-2:1-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-2 [X]:2-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-1 [X]:2-2:1-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-4 [X]:1-2:4-4\n",
      "[X]:0-2:1-4 ||| [X]:0-0:1-4 [X]:0-2:4-4\n",
      "[X]:0-2:1-4 ||| [X]:0-0:1-3 [X]:0-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:2-2:1-3 [X]:0-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-3 [X]:0-1:3-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-1 [X]:0-0:1-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-3 [X]:0-0:3-4\n",
      "[X]:0-2:1-4 ||| [X]:2-2:1-2 [X]:0-2:2-4\n",
      "[X]:0-2:1-4 ||| [X]:0-2:1-3 [X]:2-2:3-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-2 [X]:0-1:2-4\n",
      "[X]:0-2:1-4 ||| [X]:1-2:1-1 [X]:0-1:1-4\n",
      "[X]:0-2:1-4 ||| [X]:0-1:1-3 [X]:1-2:3-4\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "[X]:3-3:2-4 ||| [X]:3-3:2-3 [X]:3-3:3-4\n",
      "[X]:1-2:3-4 ||| [X]:1-1:3-4 [X]:1-2:4-4\n",
      "[X]:1-2:3-4 ||| 'puppy':1-2:3-4\n",
      "[X]:1-2:3-4 ||| [X]:1-2:3-3 [X]:2-2:3-4\n",
      "[X]:1-2:3-4 ||| [X]:1-2:3-3 [X]:1-1:3-4\n",
      "[X]:1-2:3-4 ||| [X]:2-2:3-4 [X]:1-2:4-4\n",
      "[X]:1-2:3-4 ||| 'dog':1-2:3-4\n",
      "[X]:1-2:3-4 ||| 'wolf':1-2:3-4\n",
      "[X]:1-2:3-4 ||| 'canine':1-2:3-4\n",
      "[X]:0-1:0-1 ||| 'a':0-1:0-1\n",
      "[X]:0-1:0-1 ||| 'some':0-1:0-1\n",
      "[X]:0-1:0-1 ||| 'an':0-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "[X]:2-2:1-3 ||| [X]:2-2:1-2 [X]:2-2:2-3\n",
      "[X]:1-2:2-3 ||| [X]:1-1:2-3 [X]:1-2:3-3\n",
      "[X]:1-2:2-3 ||| 'puppy':1-2:2-3\n",
      "[X]:1-2:2-3 ||| [X]:1-2:2-2 [X]:1-1:2-3\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "[X]:1-2:2-3 ||| 'canine':1-2:2-3\n",
      "[X]:1-2:2-3 ||| [X]:2-2:2-3 [X]:1-2:3-3\n",
      "[X]:1-2:2-3 ||| [X]:1-2:2-2 [X]:2-2:2-3\n",
      "[X]:1-2:2-3 ||| 'wolf':1-2:2-3\n",
      "[X]:3-3:2-3 ||| 'of':3-3:2-3\n",
      "[X]:3-3:2-3 ||| 'a':3-3:2-3\n",
      "[X]:3-3:2-3 ||| 'some':3-3:2-3\n",
      "[X]:3-3:2-3 ||| '.':3-3:2-3\n",
      "[X]:3-3:2-3 ||| 'the':3-3:2-3\n",
      "[X]:0-0:0-2 ||| [X]:0-0:0-1 [X]:0-0:1-2\n",
      "[X]:2-2:1-4 ||| [X]:2-2:1-2 [X]:2-2:2-4\n",
      "[X]:2-2:1-4 ||| [X]:2-2:1-3 [X]:2-2:3-4\n",
      "[X]:2-2:1-2 ||| 'of':2-2:1-2\n",
      "[X]:2-2:1-2 ||| 'a':2-2:1-2\n",
      "[X]:2-2:1-2 ||| 'some':2-2:1-2\n",
      "[X]:2-2:1-2 ||| '.':2-2:1-2\n",
      "[X]:2-2:1-2 ||| 'the':2-2:1-2\n",
      "[X]:2-3:2-3 ||| [X]:3-3:2-3 [X]:2-3:3-3\n",
      "[X]:2-3:2-3 ||| 'dark':2-3:2-3\n",
      "[X]:2-3:2-3 ||| 'void':2-3:2-3\n",
      "[X]:2-3:2-3 ||| 'black':2-3:2-3\n",
      "[X]:2-3:2-3 ||| [X]:2-3:2-2 [X]:2-2:2-3\n",
      "[X]:2-3:2-3 ||| [X]:2-3:2-2 [X]:3-3:2-3\n",
      "[X]:2-3:2-3 ||| [X]:2-2:2-3 [X]:2-3:3-3\n",
      "[X]:2-3:2-3 ||| 'noir':2-3:2-3\n",
      "[X]:1-3:0-1 ||| [X]:1-2:0-0 [X]:2-3:0-1\n",
      "[X]:1-3:0-1 ||| [X]:1-1:0-1 [X]:1-3:1-1\n",
      "[X]:1-3:0-1 ||| [X]:1-2:0-1 [X]:2-3:1-1\n",
      "[X]:1-3:0-1 ||| [X]:2-3:0-1 [X]:1-2:1-1\n",
      "[X]:1-3:0-1 ||| [X]:2-3:0-0 [X]:1-2:0-1\n",
      "[X]:1-3:0-1 ||| [X]:3-3:0-1 [X]:1-3:1-1\n",
      "[X]:1-3:0-1 ||| [X]:1-3:0-0 [X]:3-3:0-1\n",
      "[X]:1-3:0-1 ||| [X]:1-3:0-0 [X]:1-1:0-1\n",
      "[X]:0-2:1-2 ||| [X]:0-2:1-1 [X]:0-0:1-2\n",
      "[X]:0-2:1-2 ||| [X]:2-2:1-2 [X]:0-2:2-2\n",
      "[X]:0-2:1-2 ||| [X]:1-2:1-2 [X]:0-1:2-2\n",
      "[X]:0-2:1-2 ||| [X]:1-2:1-1 [X]:0-1:1-2\n",
      "[X]:0-2:1-2 ||| [X]:0-0:1-2 [X]:0-2:2-2\n",
      "[X]:0-2:1-2 ||| [X]:0-1:1-2 [X]:1-2:2-2\n",
      "[X]:0-2:1-2 ||| [X]:0-1:1-1 [X]:1-2:1-2\n",
      "[X]:0-2:1-2 ||| [X]:0-2:1-1 [X]:2-2:1-2\n",
      "[X]:2-2:0-3 ||| [X]:2-2:0-1 [X]:2-2:1-3\n",
      "[X]:2-2:0-3 ||| [X]:2-2:0-2 [X]:2-2:2-3\n",
      "[X]:0-3:2-3 ||| [X]:1-3:2-3 [X]:0-1:3-3\n",
      "[X]:0-3:2-3 ||| [X]:0-3:2-2 [X]:3-3:2-3\n",
      "[X]:0-3:2-3 ||| [X]:2-3:2-2 [X]:0-2:2-3\n",
      "[X]:0-3:2-3 ||| [X]:0-2:2-3 [X]:2-3:3-3\n",
      "[X]:0-3:2-3 ||| [X]:0-3:2-2 [X]:0-0:2-3\n",
      "[X]:0-3:2-3 ||| [X]:0-1:2-3 [X]:1-3:3-3\n",
      "[X]:0-3:2-3 ||| [X]:3-3:2-3 [X]:0-3:3-3\n",
      "[X]:0-3:2-3 ||| [X]:2-3:2-3 [X]:0-2:3-3\n",
      "[X]:0-3:2-3 ||| [X]:0-2:2-2 [X]:2-3:2-3\n",
      "[X]:0-3:2-3 ||| [X]:0-0:2-3 [X]:0-3:3-3\n",
      "[X]:0-3:2-3 ||| [X]:1-3:2-2 [X]:0-1:2-3\n",
      "[X]:0-3:2-3 ||| [X]:0-1:2-2 [X]:1-3:2-3\n",
      "[X]:0-1:2-4 ||| [X]:0-0:2-4 [X]:0-1:4-4\n",
      "[X]:0-1:2-4 ||| [X]:0-0:2-3 [X]:0-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:1-1:2-3 [X]:0-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-3 [X]:0-0:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-3 [X]:1-1:3-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-2 [X]:1-1:2-4\n",
      "[X]:0-1:2-4 ||| [X]:1-1:2-4 [X]:0-1:4-4\n",
      "[X]:0-1:2-4 ||| [X]:0-1:2-2 [X]:0-0:2-4\n",
      "[X]:0-3:2-2 ||| [X]:1-3:2-2 [X]:0-1:2-2\n",
      "[X]:0-3:2-2 ||| [X]:0-1:2-2 [X]:1-3:2-2\n",
      "[X]:0-3:2-2 ||| [X]:2-3:2-2 [X]:0-2:2-2\n",
      "[X]:0-3:2-2 ||| [X]:0-2:2-2 [X]:2-3:2-2\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "[X]:2-3:1-2 ||| [X]:2-3:1-1 [X]:2-2:1-2\n",
      "[X]:2-3:1-2 ||| 'void':2-3:1-2\n",
      "[X]:2-3:1-2 ||| 'noir':2-3:1-2\n",
      "[X]:2-3:1-2 ||| [X]:2-2:1-2 [X]:2-3:2-2\n",
      "[X]:2-3:1-2 ||| [X]:3-3:1-2 [X]:2-3:2-2\n",
      "[X]:2-3:1-2 ||| [X]:2-3:1-1 [X]:3-3:1-2\n",
      "[X]:2-3:1-2 ||| 'dark':2-3:1-2\n",
      "[S]:0-3:0-2 ||| [X]:0-3:0-2\n",
      "[X]:2-3:0-0 ||| '-EPS-':2-3:0-0\n",
      "[X]:0-2:0-2 ||| [X]:0-0:0-2 [X]:0-2:2-2\n",
      "[X]:0-2:0-2 ||| [X]:0-0:0-1 [X]:0-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:1-2:0-2 [X]:0-1:2-2\n",
      "[X]:0-2:0-2 ||| [X]:1-2:0-1 [X]:0-1:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-0 [X]:2-2:0-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-0 [X]:0-0:0-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-0 [X]:1-2:0-2\n",
      "[X]:0-2:0-2 ||| [X]:2-2:0-2 [X]:0-2:2-2\n",
      "[X]:0-2:0-2 ||| [X]:2-2:0-1 [X]:0-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-2 [X]:1-2:2-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-1 [X]:1-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-1 [X]:0-0:1-2\n",
      "[X]:0-2:0-2 ||| [X]:0-2:0-1 [X]:2-2:1-2\n",
      "[X]:0-2:0-2 ||| [X]:1-2:0-0 [X]:0-1:0-2\n",
      "[X]:0-1:4-4 ||| '-EPS-':0-1:4-4\n",
      "[X]:1-2:3-3 ||| '-EPS-':1-2:3-3\n",
      "[X]:0-2:2-3 ||| [X]:0-2:2-2 [X]:0-0:2-3\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "[X]:0-2:2-3 ||| [X]:0-2:2-2 [X]:2-2:2-3\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-3 [X]:1-2:3-3\n",
      "[X]:0-2:2-3 ||| [X]:0-0:2-3 [X]:0-2:3-3\n",
      "[X]:0-2:2-3 ||| [X]:2-2:2-3 [X]:0-2:3-3\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-2 [X]:0-1:2-3\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-1 [X]:1-1:1-4\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-3 [X]:1-1:3-4\n",
      "[X]:1-1:0-4 ||| [X]:1-1:0-2 [X]:1-1:2-4\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-2 [X]:0-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-2 [X]:0-0:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-1 [X]:0-1:1-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-3 [X]:0-1:3-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-2 [X]:0-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:1-1:0-1 [X]:0-1:1-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-0 [X]:1-1:0-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-0 [X]:0-0:0-3\n",
      "[X]:0-1:0-3 ||| [X]:0-0:0-3 [X]:0-1:3-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-2 [X]:1-1:2-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-1 [X]:0-0:1-3\n",
      "[X]:0-1:0-3 ||| [X]:0-1:0-1 [X]:1-1:1-3\n",
      "[X]:1-3:1-3 ||| [X]:1-3:1-2 [X]:3-3:2-3\n",
      "[X]:1-3:1-3 ||| [X]:1-1:1-3 [X]:1-3:3-3\n",
      "[X]:1-3:1-3 ||| [X]:1-1:1-2 [X]:1-3:2-3\n",
      "[X]:1-3:1-3 ||| [X]:1-3:1-2 [X]:1-1:2-3\n",
      "[X]:1-3:1-3 ||| [X]:3-3:1-3 [X]:1-3:3-3\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-3 [X]:1-2:3-3\n",
      "[X]:1-3:1-3 ||| [X]:1-3:1-1 [X]:3-3:1-3\n",
      "[X]:1-3:1-3 ||| [X]:1-2:1-3 [X]:2-3:3-3\n",
      "[X]:1-3:1-3 ||| [X]:3-3:1-2 [X]:1-3:2-3\n",
      "[X]:1-3:1-3 ||| [X]:1-3:1-1 [X]:1-1:1-3\n",
      "[X]:1-3:1-3 ||| [X]:1-2:1-2 [X]:2-3:2-3\n",
      "[X]:1-3:1-3 ||| [X]:1-2:1-1 [X]:2-3:1-3\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-1 [X]:1-2:1-3\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-2 [X]:3-3:2-3\n",
      "[X]:2-3:1-3 ||| [X]:3-3:1-3 [X]:2-3:3-3\n",
      "[X]:2-3:1-3 ||| [X]:2-2:1-3 [X]:2-3:3-3\n",
      "[X]:2-3:1-3 ||| [X]:2-2:1-2 [X]:2-3:2-3\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-2 [X]:2-2:2-3\n",
      "[X]:2-3:1-3 ||| [X]:3-3:1-2 [X]:2-3:2-3\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-1 [X]:3-3:1-3\n",
      "[X]:2-3:1-3 ||| [X]:2-3:1-1 [X]:2-2:1-3\n",
      "[X]:0-0:3-4 ||| 'of':0-0:3-4\n",
      "[X]:0-0:3-4 ||| 'a':0-0:3-4\n",
      "[X]:0-0:3-4 ||| 'some':0-0:3-4\n",
      "[X]:0-0:3-4 ||| '.':0-0:3-4\n",
      "[X]:0-0:3-4 ||| 'the':0-0:3-4\n",
      "[X]:0-3:1-2 ||| [X]:1-3:1-1 [X]:0-1:1-2\n",
      "[X]:0-3:1-2 ||| [X]:3-3:1-2 [X]:0-3:2-2\n",
      "[X]:0-3:1-2 ||| [X]:2-3:1-2 [X]:0-2:2-2\n",
      "[X]:0-3:1-2 ||| [X]:0-2:1-2 [X]:2-3:2-2\n",
      "[X]:0-3:1-2 ||| [X]:0-2:1-1 [X]:2-3:1-2\n",
      "[X]:0-3:1-2 ||| [X]:0-1:1-2 [X]:1-3:2-2\n",
      "[X]:0-3:1-2 ||| [X]:0-1:1-1 [X]:1-3:1-2\n",
      "[X]:0-3:1-2 ||| [X]:0-0:1-2 [X]:0-3:2-2\n",
      "[X]:0-3:1-2 ||| [X]:0-3:1-1 [X]:3-3:1-2\n",
      "[X]:0-3:1-2 ||| [X]:1-3:1-2 [X]:0-1:2-2\n",
      "[X]:0-3:1-2 ||| [X]:0-3:1-1 [X]:0-0:1-2\n",
      "[X]:0-3:1-2 ||| [X]:2-3:1-1 [X]:0-2:1-2\n",
      "[X]:1-3:0-2 ||| [X]:1-1:0-1 [X]:1-3:1-2\n",
      "[X]:1-3:0-2 ||| [X]:1-3:0-1 [X]:3-3:1-2\n",
      "[X]:1-3:0-2 ||| [X]:1-2:0-2 [X]:2-3:2-2\n",
      "[X]:1-3:0-2 ||| [X]:1-2:0-1 [X]:2-3:1-2\n",
      "[X]:1-3:0-2 ||| [X]:2-3:0-1 [X]:1-2:1-2\n",
      "[X]:1-3:0-2 ||| [X]:3-3:0-2 [X]:1-3:2-2\n",
      "[X]:1-3:0-2 ||| [X]:3-3:0-1 [X]:1-3:1-2\n",
      "[X]:1-3:0-2 ||| [X]:2-3:0-2 [X]:1-2:2-2\n",
      "[X]:1-3:0-2 ||| [X]:2-3:0-0 [X]:1-2:0-2\n",
      "[X]:1-3:0-2 ||| [X]:1-3:0-0 [X]:3-3:0-2\n",
      "[X]:1-3:0-2 ||| [X]:1-3:0-0 [X]:1-1:0-2\n",
      "[X]:1-3:0-2 ||| [X]:1-1:0-2 [X]:1-3:2-2\n",
      "[X]:1-3:0-2 ||| [X]:1-3:0-1 [X]:1-1:1-2\n",
      "[X]:1-3:0-2 ||| [X]:1-2:0-0 [X]:2-3:0-2\n",
      "[X]:1-3:4-4 ||| [X]:2-3:4-4 [X]:1-2:4-4\n",
      "[X]:1-3:4-4 ||| [X]:1-2:4-4 [X]:2-3:4-4\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-2 [X]:2-3:2-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-1 [X]:2-2:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-1 [X]:3-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-0 [X]:3-3:0-2\n",
      "[X]:2-3:0-2 ||| [X]:2-3:0-0 [X]:2-2:0-2\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-2 [X]:2-3:2-2\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-1 [X]:2-2:1-4\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-3 [X]:2-2:3-4\n",
      "[X]:2-2:0-4 ||| [X]:2-2:0-2 [X]:2-2:2-4\n",
      "[X]:3-3:0-2 ||| [X]:3-3:0-1 [X]:3-3:1-2\n",
      "[X]:0-0:1-2 ||| 'of':0-0:1-2\n",
      "[X]:0-0:1-2 ||| 'a':0-0:1-2\n",
      "[X]:0-0:1-2 ||| 'some':0-0:1-2\n",
      "[X]:0-0:1-2 ||| '.':0-0:1-2\n",
      "[X]:0-0:1-2 ||| 'the':0-0:1-2\n",
      "[S]:0-3:0-1 ||| [X]:0-3:0-1\n",
      "[X]:1-1:0-3 ||| [X]:1-1:0-1 [X]:1-1:1-3\n",
      "[X]:1-1:0-3 ||| [X]:1-1:0-2 [X]:1-1:2-3\n",
      "[X]:3-3:0-1 ||| 'of':3-3:0-1\n",
      "[X]:3-3:0-1 ||| 'a':3-3:0-1\n",
      "[X]:3-3:0-1 ||| 'some':3-3:0-1\n",
      "[X]:3-3:0-1 ||| '.':3-3:0-1\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "[X]:2-3:1-4 ||| [X]:3-3:1-3 [X]:2-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-2:1-4 [X]:2-3:4-4\n",
      "[X]:2-3:1-4 ||| [X]:2-2:1-3 [X]:2-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-2:1-2 [X]:2-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-2 [X]:2-2:2-4\n",
      "[X]:2-3:1-4 ||| [X]:3-3:1-2 [X]:2-3:2-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-3 [X]:3-3:3-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-1 [X]:3-3:1-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-3 [X]:2-2:3-4\n",
      "[X]:2-3:1-4 ||| [X]:3-3:1-4 [X]:2-3:4-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-1 [X]:2-2:1-4\n",
      "[X]:2-3:1-4 ||| [X]:2-3:1-2 [X]:3-3:2-4\n",
      "[X]:0-0:0-3 ||| [X]:0-0:0-1 [X]:0-0:1-3\n",
      "[X]:0-0:0-3 ||| [X]:0-0:0-2 [X]:0-0:2-3\n",
      "[X]:1-3:0-0 ||| [X]:2-3:0-0 [X]:1-2:0-0\n",
      "[X]:1-3:0-0 ||| [X]:1-2:0-0 [X]:2-3:0-0\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-1 [X]:0-0:1-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-3 [X]:0-0:3-4\n",
      "[X]:0-0:0-4 ||| [X]:0-0:0-2 [X]:0-0:2-4\n",
      "[X]:1-2:0-0 ||| '-EPS-':1-2:0-0\n",
      "[X]:0-3:0-0 ||| [X]:1-3:0-0 [X]:0-1:0-0\n",
      "[X]:0-3:0-0 ||| [X]:0-1:0-0 [X]:1-3:0-0\n",
      "[X]:0-3:0-0 ||| [X]:2-3:0-0 [X]:0-2:0-0\n",
      "[X]:0-3:0-0 ||| [X]:0-2:0-0 [X]:2-3:0-0\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-1 [X]:2-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-3 [X]:0-2:3-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-3 [X]:1-3:3-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-1 [X]:0-2:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-3 [X]:0-3:3-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-2 [X]:0-1:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-0 [X]:0-2:0-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-0 [X]:2-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-2 [X]:2-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-1 [X]:0-1:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-2 [X]:1-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-3 [X]:0-3:3-3\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-0 [X]:0-1:0-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-2 [X]:3-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-1 [X]:0-0:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-0 [X]:3-3:0-3\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-2 [X]:0-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-2 [X]:0-0:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-0 [X]:0-0:0-3\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-2 [X]:0-3:2-3\n",
      "[X]:0-3:0-3 ||| [X]:0-3:0-1 [X]:3-3:1-3\n",
      "[X]:0-3:0-3 ||| [X]:0-2:0-3 [X]:2-3:3-3\n",
      "[X]:1-2:2-4 ||| [X]:2-2:2-3 [X]:1-2:3-4\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-2 [X]:2-2:2-4\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-3 [X]:1-1:3-4\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-3 [X]:2-2:3-4\n",
      "[X]:1-2:2-4 ||| [X]:1-1:2-4 [X]:1-2:4-4\n",
      "[X]:1-2:2-4 ||| [X]:1-1:2-3 [X]:1-2:3-4\n",
      "[X]:1-2:2-4 ||| [X]:2-2:2-4 [X]:1-2:4-4\n",
      "[X]:1-2:2-4 ||| [X]:1-2:2-2 [X]:1-1:2-4\n",
      "[X]:1-1:1-3 ||| [X]:1-1:1-2 [X]:1-1:2-3\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-2 [X]:1-2:2-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-1 [X]:1-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-4 [X]:1-2:4-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-3 [X]:1-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-4 [X]:1-2:4-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-2 [X]:2-2:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-3 [X]:1-1:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-1 [X]:1-1:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-1 [X]:2-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-2 [X]:1-2:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-2 [X]:1-1:2-4\n",
      "[X]:1-2:0-4 ||| [X]:1-1:0-1 [X]:1-2:1-4\n",
      "[X]:1-2:0-4 ||| [X]:2-2:0-3 [X]:1-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-3 [X]:2-2:3-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-0 [X]:2-2:0-4\n",
      "[X]:1-2:0-4 ||| [X]:1-2:0-0 [X]:1-1:0-4\n",
      "[X]:2-3:0-3 ||| [X]:2-2:0-3 [X]:2-3:3-3\n",
      "[X]:2-3:0-3 ||| [X]:2-2:0-2 [X]:2-3:2-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-2 [X]:2-2:2-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-1 [X]:2-2:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-1 [X]:3-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-2:0-1 [X]:2-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:3-3:0-3 [X]:2-3:3-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-0 [X]:3-3:0-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-0 [X]:2-2:0-3\n",
      "[X]:2-3:0-3 ||| [X]:3-3:0-2 [X]:2-3:2-3\n",
      "[X]:2-3:0-3 ||| [X]:3-3:0-1 [X]:2-3:1-3\n",
      "[X]:2-3:0-3 ||| [X]:2-3:0-2 [X]:3-3:2-3\n",
      "[X]:2-3:2-4 ||| [X]:2-2:2-3 [X]:2-3:3-4\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-2 [X]:3-3:2-4\n",
      "[X]:2-3:2-4 ||| [X]:3-3:2-3 [X]:2-3:3-4\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-3 [X]:3-3:3-4\n",
      "[X]:2-3:2-4 ||| [X]:2-2:2-4 [X]:2-3:4-4\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-2 [X]:2-2:2-4\n",
      "[X]:2-3:2-4 ||| [X]:2-3:2-3 [X]:2-2:3-4\n",
      "[X]:2-3:2-4 ||| [X]:3-3:2-4 [X]:2-3:4-4\n",
      "[X]:2-3:4-4 ||| '-EPS-':2-3:4-4\n",
      "[X]:2-2:2-4 ||| [X]:2-2:2-3 [X]:2-2:3-4\n",
      "958\n"
     ]
    }
   ],
   "source": [
    "Dnx = libitg.earley(Dx, length_fsa,\n",
    "                    start_symbol=Nonterminal(\"D(x)\"), \n",
    "                    sprime_symbol=Nonterminal(\"D_n(x)\"))\n",
    "print(Dnx)\n",
    "print(len(Dnx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the new version of Earley can get rid of useless edges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 rules in clean forest and 958 rules in dirty forest\n"
     ]
    }
   ],
   "source": [
    "# This is a case where cleaning the forest makes a difference\n",
    "size_of_dirty_Dx = len(libitg.earley(Dx, length_fsa,\n",
    "                                     start_symbol=Nonterminal(\"D(x)\"),\n",
    "                                     sprime_symbol=Nonterminal(\"D_n(x)\"), \n",
    "                                     clean=False))\n",
    "print('%d rules in clean forest and %d rules in dirty forest' % (len(Dx), size_of_dirty_Dx))\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is too slow\n",
    "# this produces a super large FSA which enumerates the strings in the forest\n",
    "# Dnx_as_fsa = libitg.forest_to_fsa(Dnx, Nonterminal('D_n(x)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we get the strings in a normal python set\n",
    "#candidates = libitg.language_of_fsa(Dnx_as_fsa)\n",
    "#for candidate in sorted(candidates):\n",
    "#    print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's check how many string we got\n",
    "#print('Strings in D_n(x): %d' % len(candidates))\n",
    "# and check whether the gold-standard reference is still in the set of candidates (it should be!)\n",
    "#print('Does D_n(x) contain the reference (it should!)? %s' % ('yes' if 'the black dog' in candidates else 'no'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insertion constraint\n",
    "\n",
    "Constraining \\\\(D(x)\\\\) by length results in a very large forest, if your lexicon maps -EPS- to very few English types, you might be able to live with it. If you want to have smaller forests, you can use a different constraint, one that bounds the number of insertions in a derivation.\n",
    "\n",
    "We can implement this constraint easily by designing an FSA whose states keep track of the number of -EPS- symbols in a derivations. This FSA is pretty trivial as you will see.\n",
    "\n",
    "The only detail is that from the perspective of the parser, now we want to pretend -EPS- is a normal terminal, for that we simply disable Earley's special treatment for -EPS-.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=0 1 2 3\n",
      "arcs=7\n",
      "origin=0 destination=0 label=-WILDCARD-\n",
      "origin=0 destination=1 label=-EPS-\n",
      "origin=1 destination=1 label=-WILDCARD-\n",
      "origin=1 destination=2 label=-EPS-\n",
      "origin=2 destination=2 label=-WILDCARD-\n",
      "origin=2 destination=3 label=-EPS-\n",
      "origin=3 destination=3 label=-WILDCARD-\n"
     ]
    }
   ],
   "source": [
    "eps_count_fsa = libitg.InsertionConstraint(3)\n",
    "print(eps_count_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the FSA uses -WILDCARD- to match every symbol which is different from -EPS-.\n",
    "\n",
    "We are now going to use this FSA to restrict \\\\(D(x)\\\\) in a different way, this resulting set which I'll name \\\\(D^i(x)\\\\) will be our substitute to \\\\(D_n(x))\\\\). I will discuss caveats in a bit, but first let's see why this is a useful thing to have.\n",
    "\n",
    "We want to count -EPS- in the \\\\(D(x)\\\\) set before projection (when strings are still defined over the source vocabulary). So we call Earley on that set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 rules before projection\n",
      "564 rules after projection\n"
     ]
    }
   ],
   "source": [
    "# here we use _Dx (before projection onto target vocabulary)\n",
    "# and we produce a constrained forest that needs to be projected\n",
    "_Dix = libitg.earley(_Dx, \n",
    "              eps_count_fsa, \n",
    "              start_symbol=Nonterminal('D(x)'), \n",
    "              sprime_symbol=Nonterminal('D_n(x)'), \n",
    "              eps_symbol=None)  # Note I've disabled special treatment of -EPS-\n",
    "# we project it just like before\n",
    "Dix = libitg.make_target_side_itg(_Dix, lexicon)\n",
    "print('%d rules before projection' % len(_Dix))\n",
    "print('%d rules after projection' % len(Dix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are constraining on the number of insertions, you lose the guarantee that the observation will be in  \\\\(D^i(x)\\\\). Before when we constrained by length, we would make sure that \\\\(n\\\\) was sufficiently large to include our target observation in \\\\(D_n(x)\\\\), but now that may be gone.\n",
    "\n",
    "Thus, we have to find a set of derivations of the observation out of this constrained set.\n",
    "That is to say that, while before you could get \\\\(D(x,y)\\\\) directly from \\\\(D(x)\\\\) by using it to parse \\\\(y\\\\), now you have to get \\\\(D^i(x,y)\\\\) from our alternative \\\\(D^i(x)\\\\).\n",
    "\n",
    "This boils down to a simple call to Earley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 rules in D_i(x,y)\n"
     ]
    }
   ],
   "source": [
    "Dixy = libitg.earley(Dix, tgt_fsa, start_symbol=Nonterminal(\"D_n(x)\"), sprime_symbol=Nonterminal('D(x,y)'))\n",
    "print('%d rules in D_i(x,y)' % len(Dixy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap up about constraints and large (or empty) forests\n",
    "\n",
    "* if you constrain by *length* you do so on the project \\\\(D(x)\\\\), but then you will end up with larger forests\n",
    "* if you constrain by *insertions* you do so before projecting \\\\(D(x)\\\\), you will get a smaller forest which then you will use to also parse the target observation\n",
    "\n",
    "In any case, if either forest is empty you need to discard the training instance.\n",
    "\n",
    "Also, do not worry about discarding data, the important thing is to complete the project and learn about the ML techniques involved. Code optimisation is beyond the scope. Thus do not be afraid to\n",
    "\n",
    "* use small lexicons\n",
    "* allow very few insertions\n",
    "* ignore sentences longer than 15 words, for example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Earley parsing helped us instatiated the sets which will be support to our probability distribution, in particular,\n",
    "\n",
    "* \\\\(\\mathcal D_n(x)\\\\) will be support to our joint distribution \\\\(P(Y, D|X=x, N=n)\\\\)\n",
    "* \\\\(\\mathcal D(x, y)\\\\) will be support to our posterior distribution \\\\(P(D|X=x, Y=y, N=n)\\\\)\n",
    "\n",
    "and our joint distribution corresponds to \n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y, D=d|X=x, N=n) &= \\frac{\\exp(w^\\top \\Phi(y, d, x, n))}{\\sum_{y' \\in \\mathcal Y_n(x)} \\sum_{d' \\in \\mathcal D(x, y)} \\exp(w^\\top \\Phi(y', d', x, n))} \\\\\n",
    " &= \\frac{\\exp(w^\\top \\Phi(y, d, x, n))}{\\sum_{d' \\in \\mathcal D_n(x)} \\exp(w^\\top \\Phi(y', d', x, n))}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "* \\\\(\\Phi\\\\) is a feature function that maps any tuple \\\\(y, d, x, n\\\\) to \\\\(\\mathbb R^d\\\\)\n",
    "* \\\\(\\theta\\\\) is a weight vector in \\\\(\\mathbb R^d\\\\)\n",
    "* \\\\(\\mathcal Y_n(x)\\\\) is the set of translations of \\\\(x\\\\) that are at most \\\\(n\\\\)-words long\n",
    "* and the last equality is due to the fact that there is a deterministic mapping between a derivation and the string it projects on either language, that is, \\\\(y' = \\text{yield}_\\Delta(d')\\\\).\n",
    "\n",
    "In order to instantiate the joint distribution for the entire space \\\\(\\mathcal D_n(x)\\\\) we must use features that can be locally assigned to the steps of a derivation. This corresponds to the idea of having *local features* or *edge potentials*:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y, D=d|X=x, N=n) \n",
    " &= \\frac{\\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right)}{\\sum_{r_{s, t} \\in d' \\in \\mathcal D_n(x)} \\exp\\left( \\sum_c w^\\top \\phi(r, s, t, x, n) \\right)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* a derivation is seen as a sequence of rules decorated with spans\n",
    "* \\\\(r_{s,t}\\\\) is a rule decorated with a source and a target span (in the code sometimes we call this an *edge*)\n",
    "* \\\\(\\phi\\\\) is a (local) feature function that maps any tuple \\\\((r, s, t, x, n)\\\\) to \\\\(\\mathbb R^d\\\\)\n",
    "* note that \\\\( \\Phi(y, d, x, n) = \\sum_{r_{s, t} \\in d} \\phi(r, s, t, x, n) \\\\)\n",
    "\n",
    "The formulation in terms of local potentials make it clear that in designing feature representations for derivations we have unrestricted access to \\\\(x\\\\), \\\\(n\\\\), the rule identiy \\\\(r\\\\), the source span \\\\(s\\\\) and the target span \\\\(t\\\\), but note that because we do not have unrestricted access to \\\\(y\\\\), the information that \\\\(t\\\\) provides is rather limited.\n",
    "\n",
    "* For example, knowing the rule `X_{1-3,1-3} -> X_{2-3,1-2} X_{1-2,2-3}` tells us that the LHS spans from 1 to 3 on the source. Suppose, we are translating \\\\(x\\\\) = `le chien noir`, then we know the the LHS projects onto `chien noir`, that it is prefixed by `BOS le` and followed by `EOS` (end-of-sentence). On the other hand, if we focus on the target span, we only know that we will produce up 2 words (because `3-1=2`) but we dont't know which words those are;\n",
    "* Another rule, `X_{2-3,1-2} -> noir/black` tells us a different story, it tells us that the lexical entry `noir` has been aligned to the lexical entry `black`, it also tell us about the positions where these words are, so we could for example compute a distortion feature similar to IBM2's jump value;\n",
    "* A rule, `X_{0-1,0-0} -> le/-EPS-` tells us that a deletion has happened;\n",
    "* Insertions can for example appear like this `X_{1-2,1-3} -> X_{1-1,1-2} X_{1-2,2-3}`, note that the first RHS symbol has an empty source span `1-1`, but the target span is longer than 0, this means an insertion has happened;\n",
    "* The rule `X_{1-3,1-3} -> X_{2-3,1-2} X_{1-2,2-3}` also tells us that we are making an inversion, note that the source spans are not adjacent, but rather flipped, e.g. 2-3 for the first RHS symbol and 1-2 for the second RHS symbol, we could have a feature capture that;\n",
    "\n",
    "After undestanding how to use the parser to get the relevant sets, you should focus on implement a feature function that can convert the information available on an edge into a sparse feature vector.\n",
    "\n",
    "We suggest you start with a very simple feature function that is mostly *dense*. This will make development a lot easier in the beginning. Here we show a minimal version of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some auxiliary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_source_word(fsa: FSA, origin: int, destination: int, eps_str='-EPS-') -> str:\n",
    "    \"\"\"Returns the python string representing a source word from origin to destination (assuming there's a single one)\"\"\"\n",
    "    labels = list(fsa.labels(origin, destination))\n",
    "    assert len(labels) <= 1, 'Use this function only when you know the path is unambiguous, found %d labels %s for (%d, %d)' % (len(labels), labels, origin, destination)\n",
    "    return eps_str if not labels else labels[0]\n",
    "\n",
    "def get_target_word(symbol: Symbol):\n",
    "    \"\"\"Returns the python string underlying a certain terminal (thus unwrapping all span annotations)\"\"\"\n",
    "    if not symbol.is_terminal():\n",
    "        raise ValueError('I need a terminal, got %s of type %s' % (symbol, type(symbol)))\n",
    "    return symbol.root().obj()\n",
    "\n",
    "def get_bispans(symbol: Span):\n",
    "    \"\"\"\n",
    "    Returns the bispans associated with a symbol. \n",
    "    \n",
    "    The first span returned corresponds to paths in the source FSA (typically a span in the source sentence),\n",
    "     the second span returned corresponds to either\n",
    "        a) paths in the target FSA (typically a span in the target sentence)\n",
    "        or b) paths in the length FSA\n",
    "    depending on the forest where this symbol comes from.\n",
    "    \"\"\"\n",
    "    if not isinstance(symbol, Span):\n",
    "        raise ValueError('I need a span, got %s of type %s' % (symbol, type(symbol)))\n",
    "    s, start2, end2 = symbol.obj()  # this unwraps the target or length annotation\n",
    "    _, start1, end1 = s.obj()  # this unwraps the source annotation\n",
    "    return (start1, end1), (start2, end2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and define our prototype feature function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features(edge: Rule, src_fsa: FSA, eps=Terminal('-EPS-'), \n",
    "                    sparse_del=False, sparse_ins=False, sparse_trans=False) -> dict:\n",
    "    \"\"\"\n",
    "    Featurises an edge given\n",
    "        * rule and spans\n",
    "        * src sentence as an FSA\n",
    "        * TODO: target sentence length n\n",
    "        * TODO: extract IBM1 dense features\n",
    "    crucially, note that the target sentence y is not available!    \n",
    "    \"\"\"\n",
    "    fmap = defaultdict(float)\n",
    "    if len(edge.rhs) == 2:  # binary rule\n",
    "        fmap['type:binary'] += 1.0\n",
    "        # here we could have sparse features of the source string as a function of spans being concatenated\n",
    "        (ls1, ls2), (lt1, lt2) = get_bispans(edge.rhs[0])  # left of RHS\n",
    "        (rs1, rs2), (rt1, rt2) = get_bispans(edge.rhs[1])  # right of RHS        \n",
    "        # TODO: double check these, assign features, add some more\n",
    "        if ls1 == ls2:  # deletion of source left child\n",
    "            pass\n",
    "        if rs1 == rs2:  # deletion of source right child\n",
    "            pass\n",
    "        if ls2 == rs1:  # monotone\n",
    "            pass\n",
    "        if ls1 == rs2:  # inverted\n",
    "            pass        \n",
    "    else:  # unary\n",
    "        symbol = edge.rhs[0]\n",
    "        if symbol.is_terminal():  # terminal rule\n",
    "            fmap['type:terminal'] += 1.0\n",
    "            # we could have IBM1 log probs for the traslation pair or ins/del\n",
    "            (s1, s2), (t1, t2) = get_bispans(symbol)            \n",
    "            if symbol.root() == eps:  # symbol.root() gives us a Terminal free of annotation\n",
    "                # for sure there is a source word\n",
    "                src_word = get_source_word(src_fsa, s1, s2)                \n",
    "                fmap['type:deletion'] += 1.0\n",
    "                # dense versions (for initial development phase)\n",
    "                # TODO: use IBM1 prob\n",
    "                #ff['ibm1:del:logprob'] += \n",
    "                # sparse version\n",
    "                if sparse_del:\n",
    "                    fmap['del:%s' % src_word] += 1.0\n",
    "            else:                  \n",
    "                # for sure there's a target word\n",
    "                tgt_word = get_target_word(symbol)\n",
    "                if s1 == s2:  # has not consumed any source word, must be an eps rule                    \n",
    "                    fmap['type:insertion'] += 1.0\n",
    "                    # dense version\n",
    "                    # TODO: use IBM1 prob\n",
    "                    #ff['ibm1:ins:logprob'] += \n",
    "                    # sparse version\n",
    "                    if sparse_ins:\n",
    "                        fmap['ins:%s' % tgt_word] += 1.0\n",
    "                else:\n",
    "                    # for sure there's a source word\n",
    "                    src_word = get_source_word(src_fsa, s1, s2)\n",
    "                    fmap['type:translation'] += 1.0\n",
    "                    # dense version\n",
    "                    # TODO: use IBM1 prob\n",
    "                    #ff['ibm1:x2y:logprob'] += \n",
    "                    #ff['ibm1:y2x:logprob'] += \n",
    "                    # sparse version                    \n",
    "                    if sparse_trans:\n",
    "                        fmap['trans:%s/%s' % (src_word, tgt_word)] += 1.0\n",
    "        else:  # S -> X\n",
    "            fmap['top'] += 1.0\n",
    "    return fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply featurize edges, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_edges(forest, src_fsa, \n",
    "                    sparse_del=False, sparse_ins=False, sparse_trans=False,\n",
    "                    eps=Terminal('-EPS-')) -> dict:\n",
    "    edge2fmap = dict()\n",
    "    for edge in forest:\n",
    "        edge2fmap[edge] = simple_features(edge, src_fsa, eps, sparse_del, sparse_ins, sparse_trans)\n",
    "    return edge2fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what we get if we use some sparse features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:0-3:0-3 ||| [X]:0-1:0-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:1-1:0-1 [X]:0-1:1-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-0:0-1 ||| 'the':0-0:0-1\n",
      "defaultdict(<class 'float'>, {'ins:the': 1.0, 'type:terminal': 1.0, 'type:insertion': 1.0})\n",
      "\n",
      "[X]:0-1:1-1 ||| '-EPS-':0-1:1-1\n",
      "defaultdict(<class 'float'>, {'del:le': 1.0, 'type:deletion': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[D(x)]:0-3 ||| [S]:0-3:0-3\n",
      "defaultdict(<class 'float'>, {'top': 1.0})\n",
      "\n",
      "[X]:1-3:1-3 ||| [X]:2-3:1-2 [X]:1-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:1-3 ||| [X]:1-3:1-3 [X]:0-1:3-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[S]:0-3:0-3 ||| [X]:0-3:0-3\n",
      "defaultdict(<class 'float'>, {'top': 1.0})\n",
      "\n",
      "[X]:1-3:0-3 ||| [X]:1-1:0-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:2-2:0-1 ||| 'the':2-2:0-1\n",
      "defaultdict(<class 'float'>, {'ins:the': 1.0, 'type:terminal': 1.0, 'type:insertion': 1.0})\n",
      "\n",
      "[X]:1-2:2-3 ||| 'dog':1-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:translation': 1.0, 'type:terminal': 1.0, 'trans:chien/dog': 1.0})\n",
      "\n",
      "[X]:0-2:2-3 ||| [X]:0-1:2-2 [X]:1-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:0-0:0-1 [X]:0-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:2-3:0-2 ||| [X]:3-3:0-1 [X]:2-3:1-2\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| 'the':0-1:0-1\n",
      "defaultdict(<class 'float'>, {'type:translation': 1.0, 'type:terminal': 1.0, 'trans:le/the': 1.0})\n",
      "\n",
      "[D(x,y)] ||| [D(x)]:0-3\n",
      "defaultdict(<class 'float'>, {'top': 1.0})\n",
      "\n",
      "[X]:0-1:0-0 ||| '-EPS-':0-1:0-0\n",
      "defaultdict(<class 'float'>, {'del:le': 1.0, 'type:deletion': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:2-3:0-2 ||| [X]:2-2:0-1 [X]:2-3:1-2\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:0-0:0-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:1-3:0-3 ||| [X]:3-3:0-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:2-3:1-2 ||| 'black':2-3:1-2\n",
      "defaultdict(<class 'float'>, {'trans:noir/black': 1.0, 'type:translation': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:3-3:0-1 [X]:0-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-2:2-3 ||| [X]:1-2:2-3 [X]:0-1:3-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:3-3:0-1 ||| 'the':3-3:0-1\n",
      "defaultdict(<class 'float'>, {'ins:the': 1.0, 'type:terminal': 1.0, 'type:insertion': 1.0})\n",
      "\n",
      "[X]:0-1:2-2 ||| '-EPS-':0-1:2-2\n",
      "defaultdict(<class 'float'>, {'del:le': 1.0, 'type:deletion': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:0-1:0-0 [X]:1-3:0-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:0-0:0-1 [X]:0-1:1-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:1-3 ||| [X]:2-3:1-2 [X]:0-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:1-3:0-3 ||| [X]:2-3:0-2 [X]:1-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:1-3 ||| [X]:0-1:1-1 [X]:1-3:1-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:0-1 ||| [X]:0-1:0-0 [X]:1-1:0-1\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:2-3:0-2 [X]:0-2:2-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:0-1:3-3 ||| '-EPS-':0-1:3-3\n",
      "defaultdict(<class 'float'>, {'del:le': 1.0, 'type:deletion': 1.0, 'type:terminal': 1.0})\n",
      "\n",
      "[X]:0-3:0-3 ||| [X]:1-3:0-3 [X]:0-1:3-3\n",
      "defaultdict(<class 'float'>, {'type:binary': 1.0})\n",
      "\n",
      "[X]:1-1:0-1 ||| 'the':1-1:0-1\n",
      "defaultdict(<class 'float'>, {'ins:the': 1.0, 'type:terminal': 1.0, 'type:insertion': 1.0})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for edge, fmap in featurize_edges(Dxy, src_fsa, True, True, True).items():\n",
    "    print(edge)\n",
    "    print(fmap)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recall the definition of the joint distribution\n",
    "\n",
    "\\begin{align}\n",
    "P(y,d|x, n) &= \\frac{\\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right)}{Z_n(x)}\n",
    "\\end{align}\n",
    "\n",
    "for convenience, let's work with log probabilities, \n",
    "\n",
    "\\begin{align}\n",
    "\\log P(y,d|x,n) &= \\log \\exp \\left( \\sum_{r_{s,t} \\in d} w^\\top \\phi(r, s, t, x, n) \\right) - \\log Z_n(x) \\\\\n",
    "  &= \\underbrace{\\sum_{r_{s,t} \\in d} \\underbrace{w^\\top \\phi(r, s, t, x, n)}_{\\text{local log-potential}}}_{\\text{log of unnormalised distribution}} - \\log Z_n(x)\n",
    "\\end{align}\n",
    "\n",
    "Now, in the log-domain, we can compute the logarithm of unnormalised probabilities by summing along the edges in a derivation.\n",
    "\n",
    "This is great news! We can express the log of the unnormalised distribution by decorating edges with local log-potentials, that is, \n",
    "\n",
    "\\begin{equation}\n",
    "\\omega(r_{s,t}) = w^\\top \\phi(r, s, t, x, n)\n",
    "\\end{equation}\n",
    "\n",
    "where \\\\(\\omega\\\\) takes an edge in the hypergraph and returns its local log-potential (dot product of feature weights and feature vector).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to define a weight function now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_function(edge, fmap, wmap) -> float:\n",
    "    pass  # dot product of fmap and wmap  (working in log-domain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "Working with hypergraphs can be tedious, they can be quite large, a few tips:\n",
    "\n",
    "* do not parse very long sentences: discard training instances where either string is longer than say 10 words in development phase, for the final report you can try to stretch all the way to 30 or so;\n",
    "* parse once: with a fixed lexicon and grammar, the parse forests will never change, thus produce them one at a time and pickle them to disk, so that whenever you need the forest during SGD it will be pre-computed\n",
    "* featurise once: same applies to feature vectors, if you do not change your feature function, feature vectors will remain the same, thus, extract them once and pickle them to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE\n",
    "\n",
    "Now it's time you optimise your likelihood\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L(w|x, y) &= \\log P(y|x, n) \\\\\n",
    " &= \\log \\sum_{d \\in \\mathcal D(x, y)} P(y, d|x, n) \\\\\n",
    " &= \\log \\sum_{d \\in \\mathcal D(x, y)} \\frac{\\exp(w^\\top \\Phi(y, d|x, n))}{Z_n(x)} \\\\\n",
    " &= \\log \\frac{\\sum_{d \\in \\mathcal D(x, y)} \\exp(w^\\top \\Phi(y, d|x, n))}{Z_n(x)} \\\\\n",
    " &= \\log \\frac{Z(x, y)}{Z_n(x)} \\\\\n",
    " &= \\log Z(x, y) - \\log Z_n(x)\n",
    "\\end{align}\n",
    "\n",
    "First of all, we need to efficiently compute\n",
    "* \\\\(\\log Z(x,y)\\\\) the sum of all unnormalised probabilities for derivations that explain the translation pair \\\\(x, y\\\\)\n",
    "* and \\\\(\\log Z_n(x)\\\\) the sum of all unnormalised probabilities for derivations that explain the incomplete observation \\\\(x, n\\\\)\n",
    "now note that we have acyclic hypergraphs whose edges represent log-potentials for a model which is log-linear. This means that we can efficiently compute the global log-normalisers with a recursive formula that visits each node in the forest once. This recursion is called the `Inside algorithm`.\n",
    "\n",
    "The *inside weight* of a node corresponds to the sum of the weights of all paths under this node in the forest.\n",
    "\n",
    "\\begin{equation}\n",
    "    I(v) = \n",
    "    \\begin{cases}\n",
    "        \\bar{1} \\quad \\text{if } v \\text{ is terminal }\\\\\n",
    "        \\bigoplus_{e \\in BS(v)} \\omega(e) \\otimes \\bigotimes_{u \\in tail(e)} I(u) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* \\\\(u\\\\) and \\\\(v\\\\) are nodes (these are annotated symbols in the forest)\n",
    "* \\\\(e\\\\) is an edge (this is an annotated rule in the forest)\n",
    "* \\\\(\\text{tail}(e)\\\\) is a sequence of children nodes (this is the RHS of an annotated rule in the forest)\n",
    "* \\\\(BS(v)\\\\) corresponds to the set of edges that are incoming to v (this is the set of annotated rules for which v is LHS in the forest)\n",
    "* \\\\(\\otimes\\\\) corresponds to product of probabilities or equivalently sum of log-probabilities\n",
    "* \\\\(\\oplus\\\\) corresponds to sum of probabilities or equivalently log-of-sum-of-exponentiated-log-probabilities, i.e. \\\\(\\log (\\exp(a) + \\exp(b))\\\\) where \\\\(a\\\\) and \\\\(b\\\\) are log-probabilities\n",
    "* \\\\(\\bar{1}\\\\) corresponds to the multiplicative identity (which is 0 in the log-domain)\n",
    "* if \\\\(\\otimes\\\\) and \\\\(\\oplus\\\\) confused you, then you need to learn a little more about [semirings](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf)\n",
    "\n",
    "The inside recursion can be implemented iteratively by visiting the nodes in [top-sorted order](https://en.wikipedia.org/wiki/Topological_sorting).\n",
    "              \n",
    "Check the lecture notes and [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a pseudo-code of the Inside algorithm.\n",
    "\n",
    "The inside at the root of the forest represents the log-normaliser of the forest.\n",
    "\n",
    "Now you should be ready to implement topsort and the inside algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_sort(forest: CFG) -> list:\n",
    "    \"\"\"Returns ordered list of nodes according to topsort order in an acyclic forest\"\"\"\n",
    "    pass\n",
    "\n",
    "def inside_algorithm(forest: CFG, tsort: list, edge_weights: dict) -> dict:\n",
    "    \"\"\"Returns the inside weight of each node\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we will approach the optimisation of the log-likelihood via SGD, that is, \n",
    "we will do so by taking steps towards the steepest ascent at each training instance (or mini-batch).\n",
    "\n",
    "Taking derivatives with respect to \\\\(w\\\\) we get\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w \\mathcal L(w|x, y, n) &= \\mathbb E_{P(D|Y=y, X=x, N=n)}[\\Phi(Y, D, X, N)] - \\mathbb E_{P(Y, D|X=x, N=n)}[\\Phi(Y, D, X, N)]\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* the first expectation uses the forest \\\\(\\mathcal D(x, y)\\\\)\n",
    "* the second expectation uses the forest \\\\(\\mathcal D_n(x)\\\\)\n",
    "\n",
    "again because our CRF is edge-factored, we can compute this rather efficiently combining inside weights with *outside* weights, for which we need the *outside algorithm*. If you heard of forward-backward, then inside-outside is just a generalisation for hypergraphs (instead of graphs). You can learn more about expectations from forest from [Li and Eisner](http://www.aclweb.org/anthology/D09-1005).\n",
    "\n",
    "Whereas the inside was defined easily on its own, the outside will be defined differently so that\n",
    "\\begin{align}\n",
    "    I(u) \\otimes O(u)\n",
    "\\end{align}\n",
    "is proportional to the marginal probability of node \\\\(u\\\\), where the missing proportionality constant corresponds to the log-normaliser of the distribution (which is precisely the Inside weight of the root of the forest).\n",
    "\n",
    "\\begin{equation}\n",
    "    O(v) = \n",
    "    \\begin{cases}\n",
    "        \\bar{1} \\quad \\text{if } FS(v) =\\emptyset \\\\\n",
    "        \\bigoplus_{e \\in FS(v)} \\omega(e) \\otimes O(\\text{head}(e)) \\otimes \\bigotimes_{s \\in tail(e)\\setminus\\{v\\}} I(s) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* \\\\(s\\\\) and \\\\(v\\\\) are nodes (these are annotated symbols in the forest)\n",
    "* \\\\(e\\\\) is an edge (this is an annotated rule in the forest)\n",
    "* \\\\(\\text{head}(e)\\\\) is a the edge's head node (this is the annotated LHS of an annotated rule in the forest)\n",
    "* \\\\(FS(v)\\\\) corresponds to the set of edges that are outgoing from v (this is the set of annotated rules for which v appears in the RHS)\n",
    "* \\\\(I(s)\\\\) is the inside weight of node \\\\(s\\\\)\n",
    "* \\\\(\\otimes\\\\) corresponds to product of probabilities or equivalently sum of log-probabilities\n",
    "* \\\\(\\oplus\\\\) corresponds to sum of probabilities or equivalently log-of-sum-of-exponentiated-log-probabilities, i.e. \\\\(\\log (\\exp(a) + \\exp(b))\\\\) where \\\\(a\\\\) and \\\\(b\\\\) are log-probabilities\n",
    "* \\\\(\\bar{1}\\\\) corresponds to the multiplicative identity (which is 0 in the log-domain)\n",
    "* if \\\\(\\otimes\\\\) and \\\\(\\oplus\\\\) confused you, then you need to learn a little more about [semirings](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf)\n",
    "\n",
    "\n",
    "Again, check the [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) for a pseu-code for the outside algorithm.\n",
    "\n",
    "Now you should be ready to implement the *outside algorithm*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outside_algorithm(forest: CFG, tsort:list, edge_weights: dict, inside: dict) -> dict:\n",
    "    \"\"\"Returns the outside weight of each node\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected feature vector\n",
    "\n",
    "It's now time to combine inside and outside and efficiently compute the expected feature vector under each distribution.\n",
    "\n",
    "Expectations live in the *Probability (or Real)* semiring, that is, the normal algebra of real numbers/vectors you are familiar with. Thus you need to be careful here: either think of Inside/Outside in the *Probability* semiring or convert Inside/Outside weights back and forth where appropriate.\n",
    "\n",
    "After reading [(Li and Eisner, 2009)](http://www.aclweb.org/anthology/D09-1005) we see that to get the expected feature vector we need to sum scaled contributions from each individual edge. The features of each edge should be scaled by a quantity that is proportional to the probability of all derivations that cross an edge where the proportionality accounts for the edges own weight. Li and Eisner called this quantity the *exclusive weight* of the edge.\n",
    "\n",
    "Here I will be assuming that \\\\(I(u)\\\\) is a function mapping from nodes to unnormalised inside weights in the *LogProb* semiring. Similarly, \\\\(O(u)\\\\) is a function mapping from nodes to unnormalised outside weights in the *LogProb* semiring. \n",
    "\n",
    "The exclusive weight of each edge in the *LogProb* semiring is the function \\\\(k_{\\log}(e)\\\\) below\n",
    "\\begin{equation}\n",
    "k_{\\log}(e) = O(\\text{head}(e)) \\otimes \\bigotimes_{u \\in \\text{tail}(e)} I(u)\n",
    "\\end{equation}\n",
    "where \\\\(\\text{head}(e)\\\\) returns the head of an edge, \\\\(\\text{tail}(e)\\\\) returns the sequence of nodes in the edges tail, and \\\\(\\otimes\\\\) is a generalised product (which for *LogProb* semiring corresponds to \\\\(+\\\\)).\n",
    "\n",
    "Now we can compute the exclusive weight \\\\(k(e)\\\\) in the *Probability* semiring. For that, all we need is to normalise and exponentiate\n",
    "\\begin{equation}\n",
    "k(e) = exp(k_{\\log}(e) - I(\\text{root}))  \n",
    "\\end{equation}\n",
    "where \\\\(\\text{root}\\\\) refers to the root of the forest, and therefore, \\\\(I(\\text{root})\\\\) is the log-normaliser of the distribution.\n",
    "\n",
    "Li and Eisner has shown that the expected feature vector is then\n",
    "\\begin{equation}\n",
    "\\sum_{e \\in \\text{forest}} k(e) \\phi(e)\n",
    "\\end{equation}\n",
    "where \\\\(k(e) \\phi(e)\\\\) involves a normal multiplication by scalar (because expectations live in the *Probability* semiring).\n",
    "\n",
    "\n",
    "You should be ready to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expected_feature_vector(forest: CFG, inside: dict, outside: dict, edge_features: dict) -> dict:\n",
    "    \"\"\"Returns an expected feature vector (here a sparse python dictionary)\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Now it's time to take gradient steps,\n",
    "\n",
    "\\begin{align}\n",
    "    w = w + \\delta \\nabla_w \\mathcal L(w|x, y, n)\n",
    "\\end{align}\n",
    "\n",
    "where for a certain observation \\\\(x, n, y\\\\) we compute the expected feature vectors, subtract them, scale by a learning rate and get a parameter update.\n",
    "If we have a mini-batch, we accumulate the gradients.\n",
    "\n",
    "In the beginning of the project you will be working with the simple (mostly dense) feature function, thus this should work reasonably well. At some point you will model sparse features and then you will most likely need an [ L2 regulariser](http://www.aclweb.org/anthology/P08-1024).\n",
    "\n",
    "To diagonose convergence we typically track log-likelihood of training data, with online learning or mini-batching is typically too expensive to go over the entire trainin set computing log-likelihood, thus you can do that on each batch individually and track both the batch log-likelihood and a running average.\n",
    "\n",
    "Lastly, you might want to experiment with averaged parameters where in parallel to the parameters \\\\(w\\\\) that you actually use, you can keep a running average of parameters \\\\(w_{\\text{avg}}\\\\), at then end of learning, this running average typically shows better performance at prediction time.\n",
    "\n",
    "For *learning rate schedule* and *averaged parameters* check Leon Bottou's [bag of SGD tricks](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf), in particular, sections 5.2 and 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Finally, at prediction time we have a big problem, because\n",
    "\\begin{align}\n",
    "y^\\star =    \\arg\\max_y P(y|x, n)\n",
    "\\end{align}\n",
    "is intractable as proven by [Sima'an (1996)](http://www.aclweb.org/anthology/C/C96/C96-2215.pdf).\n",
    "Thus we typically approximate that criterion by\n",
    "\\begin{align}\n",
    "y^\\star =  \\text{yield}_\\Delta \\left(  \\arg\\max_d P(y, d|x, n) \\right)\n",
    "\\end{align}\n",
    "which is often termed *Viterbi decoding*.\n",
    "\n",
    "All this takes is a forest respresenting all translation candidates, i.e. \\\\(\\mathcal D_n(x)\\\\) and a the Viterbi algorithm. Because in this project you need to implement *inside*, there's little point in programming Viterbi, you can simply use Inside for that. It will require though a change of **semiring**.\n",
    "If you run *inside* where you replace \\\\(\\oplus\\\\) by \\\\(\\max\\\\), then you will see that finding the argmax of the distribution can be done with a very simple top-down algorithm. Check our [slides](https://uva-slpl.github.io/nlp2/resources/slides/crf.pdf) again.\n",
    "\n",
    "Recap on semirings:\n",
    "\n",
    "* Boolean: weights are binary, \\\\(\\oplus\\\\) is `or`, \\\\(\\otimes\\\\) is `and`, \\\\(\\bar{0}\\\\) is `False` and \\\\(\\bar{1}\\\\) is `True`;\n",
    "* Real: weights are non-negative numbers, \\\\(\\oplus\\\\) is normal sum, \\\\(\\otimes\\\\) is normal product, \\\\(\\bar{0}\\\\) is 0 and \\\\(\\bar{1}\\\\) is 1;\n",
    "* Log: weights are logartihms of non-negative numbers, \\\\(\\oplus\\\\) is \\\\(\\log(\\exp(a)+\\exp(b))\\\\), \\\\(\\otimes\\\\) is sum of logs, \\\\(\\bar{0}\\\\) is minus infinity (i.e. \\\\(\\log 0\\\\) ), and \\\\(\\bar{1}\\\\) is 0 (i.e. \\\\(\\log 1\\\\) )\n",
    "* Viterbi: just like Log, but \\\\(\\oplus\\\\) becomes \\\\(\\max\\\\)\n",
    "\n",
    "In doubt, check [Goodman's paper](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
