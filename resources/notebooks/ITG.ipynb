{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should help you with project 2, in particular, it implements a basic ITG parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbols\n",
    "\n",
    "Let's start by defining the symbols that can be used in our grammars.\n",
    "\n",
    "We are going to design symbols as immutable objects. \n",
    "\n",
    "* a symbol is going to be a container\n",
    "* Terminal and Nonterminal are basic symbols, they simply store a python string\n",
    "* Span is a composed symbol, it contains a Symbol and a range represented as two integers\n",
    "* Internally a Span is a python tuple of the kind (symbol: Symbol, start: int, end: int)\n",
    "* We define two *3* special methods to interact with basic and composed symbols\n",
    "    * root: goes all the way up to the root symbol (for example, returns the Symbol in a Span)\n",
    "    * obj: returns the underlying python object (for example, a str for Terminal, or tuple for Span)    \n",
    "    * translate: creates a symbol identical in structure, but translates the underlying python object of the root symbol (for example, translates the Terminal of a Span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Symbol:\n",
    "    pass\n",
    "\n",
    "class Terminal(Symbol):\n",
    "    pass\n",
    "\n",
    "class Nonterminal(Symbol):\n",
    "    pass\n",
    "\n",
    "class Symbol:\n",
    "    \"\"\"\n",
    "    A symbol in a grammar. In this class we basically wrap a certain type of object and treat it as a symbol.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def is_terminal(self) -> bool:\n",
    "        \"\"\"Whether or not this is a terminal symbol\"\"\"\n",
    "        pass\n",
    "\n",
    "    def root(self) -> Symbol:\n",
    "        \"\"\"Some symbols are represented as a hierarchy of symbols, this method returns the root of that hierarchy.\"\"\"\n",
    "        pass    \n",
    "    \n",
    "    def obj(self) -> object:\n",
    "        \"\"\"Returns the underlying python object.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def translate(self, target) -> Symbol:\n",
    "        \"\"\"Translate the underlying python object of the root symbol and return a new Symbol\"\"\"\n",
    "        pass\n",
    "    \n",
    "class Terminal(Symbol):\n",
    "    \"\"\"\n",
    "    Terminal symbols are words in a vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, symbol: str):\n",
    "        assert type(symbol) is str, 'A Terminal takes a python string, got %s' % type(symbol)\n",
    "        self._symbol = symbol\n",
    "        \n",
    "    def is_terminal(self):\n",
    "        return True\n",
    "        \n",
    "    def root(self) -> Terminal:\n",
    "        # Terminals are not hierarchical symbols\n",
    "        return self\n",
    "    \n",
    "    def obj(self) -> str:\n",
    "        \"\"\"The underlying python string\"\"\"\n",
    "        return self._symbol\n",
    "    \n",
    "    def translate(self, target) -> Terminal:\n",
    "        return Terminal(target)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"'%s'\" % self._symbol\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Terminal(%r)' % self._symbol\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self._symbol)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return type(self) == type(other) and self._symbol == other._symbol\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return not (self == other)\n",
    "    \n",
    "class Nonterminal(Symbol):\n",
    "    \"\"\"\n",
    "    Nonterminal symbols are variables in a grammar.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, symbol: str):\n",
    "        assert type(symbol) is str, 'A Nonterminal takes a python string, got %s' % type(symbol)\n",
    "        self._symbol = symbol\n",
    "        \n",
    "    def is_terminal(self):\n",
    "        return False\n",
    "        \n",
    "    def root(self) -> Nonterminal:\n",
    "        # Nonterminals are not hierarchical symbols\n",
    "        return self\n",
    "    \n",
    "    def obj(self) -> str:\n",
    "        \"\"\"The underlying python string\"\"\"\n",
    "        return self._symbol\n",
    "    \n",
    "    def translate(self, target) -> Nonterminal:\n",
    "        return Nonterminal(target)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"[%s]\" % self._symbol\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Nonterminal(%r)' % self._symbol\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self._symbol)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return type(self) == type(other) and self._symbol == other._symbol\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return not (self == other)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of *span* will come in handy when designing parsers, thus let's define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span(Symbol):\n",
    "    pass\n",
    "\n",
    "class Span(Symbol):\n",
    "    \"\"\"\n",
    "    A span can be a terminal, a nonterminal, or a span wrapped around two integers.\n",
    "    Internally, we represent spans with tuples of the kind (symbol, start, end).\n",
    "    \n",
    "    Example:\n",
    "        Span(Terminal('the'), 0, 1)\n",
    "        Span(Nonterminal('[X]'), 0, 1)\n",
    "        Span(Span(Terminal('the'), 0, 1), 1, 2)\n",
    "        Span(Span(Nonterminal('[X]'), 0, 1), 1, 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, symbol: Symbol, start: int, end: int):\n",
    "        assert isinstance(symbol, Symbol), 'A span takes an instance of Symbol, got %s' % type(symbol)\n",
    "        self._symbol = symbol\n",
    "        self._start = start\n",
    "        self._end = end\n",
    "        \n",
    "    def is_terminal(self):\n",
    "        # a span delegates this to an underlying symbol\n",
    "        return self._symbol.is_terminal()\n",
    "        \n",
    "    def root(self) -> Symbol:\n",
    "        # Spans are hierarchical symbols, thus we delegate \n",
    "        return self._symbol.root()\n",
    "    \n",
    "    def obj(self) -> (Symbol, int, int):\n",
    "        \"\"\"The underlying python tuple (Symbol, start, end)\"\"\"\n",
    "        return (self._symbol, self._start, self._end)\n",
    "    \n",
    "    def translate(self, target) -> Span:\n",
    "        return Span(self._symbol.translate(target), self._start, self._end)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"%s:%s-%s\" % (self._symbol, self._start, self._end)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Span(%r, %r, %r)' % (self._symbol, self._start, self._end)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self._symbol, self._start, self._end))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return type(self) == type(other) and self._symbol == other._symbol and self._start == other._start and self._end == other._end\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return not (self == other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules \n",
    "\n",
    "\n",
    "A context-free rule rewrites a nonterminal LHS symbol into a sequence of terminal and nonterminal symbols.\n",
    "We expect sequences to be non-empty, and we reserve a special terminal symbol to act as an epsilon string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Rule(object):\n",
    "    \"\"\"\n",
    "    A rule is a container for a LHS symbol and a sequence of RHS symbols.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lhs: Symbol, rhs: list):\n",
    "        \"\"\"\n",
    "        A rule takes a LHS symbol and a list/tuple of RHS symbols\n",
    "        \"\"\"\n",
    "        assert isinstance(lhs, Symbol), 'LHS must be an instance of Symbol'\n",
    "        assert len(rhs) > 0, 'If you want an empty RHS, use an epsilon Terminal'\n",
    "        assert all(isinstance(s, Symbol) for s in rhs), 'RHS must be a sequence of Symbol objects'\n",
    "        self._lhs = lhs\n",
    "        self._rhs = tuple(rhs)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return type(self) == type(other) and self._lhs == other._lhs and self._rhs == other._rhs\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not (self == other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self._lhs, self._rhs))\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s ||| %s' % (self._lhs, ' '.join(str(s) for s in self._rhs))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Rule(%r, %r)' % (self._lhs, self._rhs)\n",
    "\n",
    "    @property\n",
    "    def lhs(self):\n",
    "        return self._lhs\n",
    "\n",
    "    @property\n",
    "    def rhs(self):\n",
    "        return self._rhs\n",
    "    \n",
    "    @property\n",
    "    def arity(self):\n",
    "        return len(self._rhs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what we have built so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols\n",
      "[S]\n",
      "[X]\n",
      "'a'\n",
      "Rules\n",
      "[S] ||| [X]\n",
      "[X] ||| [X] [X]\n",
      "[X] ||| 'a'\n"
     ]
    }
   ],
   "source": [
    "S = Nonterminal('S')\n",
    "X = Nonterminal('X')\n",
    "a = Terminal('a')\n",
    "r1 = Rule(S, [X])\n",
    "r2 = Rule(X, [X, X])\n",
    "r3 = Rule(X, [a])\n",
    "print('Symbols')\n",
    "for sym in [S, X, a]:\n",
    "    print(sym)\n",
    "print('Rules')\n",
    "for r in [r1, r2, r3]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG\n",
    "\n",
    "Now let us write a CFG class, which will organise rules for us in a convenient manner.\n",
    "We will design CFGs to be immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \"\"\"\n",
    "    A CFG is nothing but a container for rules.\n",
    "    We group rules by LHS symbol and keep a set of terminals and nonterminals.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rules=[]):\n",
    "        self._rules = []\n",
    "        self._rules_by_lhs = defaultdict(list)\n",
    "        self._terminals = set()\n",
    "        self._nonterminals = set()\n",
    "        # organises rules\n",
    "        for rule in rules:\n",
    "            self._rules.append(rule)\n",
    "            self._rules_by_lhs[rule.lhs].append(rule)\n",
    "            self._nonterminals.add(rule.lhs)\n",
    "            for s in rule.rhs:\n",
    "                if s.is_terminal():\n",
    "                    self._terminals.add(s)\n",
    "                else:\n",
    "                    self._nonterminals.add(s)\n",
    "\n",
    "    @property\n",
    "    def nonterminals(self):\n",
    "        return self._nonterminals\n",
    "\n",
    "    @property\n",
    "    def terminals(self):\n",
    "        return self._terminals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._rules)\n",
    "\n",
    "    def __getitem__(self, lhs):\n",
    "        return self._rules_by_lhs.get(lhs, frozenset())\n",
    "\n",
    "    def get(self, lhs, default=frozenset()):\n",
    "        \"\"\"rules whose LHS is the given symbol\"\"\"\n",
    "        return self._rules_by_lhs.get(lhs, default)\n",
    "\n",
    "    def can_rewrite(self, lhs):\n",
    "        \"\"\"Whether a given nonterminal can be rewritten.\n",
    "\n",
    "        This may differ from ``self.is_nonterminal(symbol)`` which returns whether a symbol belongs\n",
    "        to the set of nonterminals of the grammar.\n",
    "        \"\"\"\n",
    "        return len(self[lhs]) > 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"iterator over rules (in arbitrary order)\"\"\"\n",
    "        return iter(self._rules)\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"iterator over pairs of the kind (LHS, rules rewriting LHS)\"\"\"\n",
    "        return self._rules_by_lhs.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        lines = []\n",
    "        for lhs, rules in self.items():\n",
    "            for rule in rules:\n",
    "                lines.append(str(rule))\n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITG\n",
    "\n",
    "We do not really need a special class for ITGs, they are just a generalisation of CFGs for multiple streams.\n",
    "What we can do is to treat the source side and the target side of the ITG as CFGs.\n",
    "\n",
    "We will represent a lexicon\n",
    "\n",
    "* a collection of translation pairs \\\\((x, y) \\in \\Sigma \\times \\Delta\\\\) where \\\\(\\Sigma\\\\) is the source vocabulary and \\\\(\\Delta\\\\) is the target vocabulary\n",
    "* these vocabularies are extended with an empty string, i.e., \\\\(\\epsilon\\\\)\n",
    "* we will assume the lexicon expliclty states which words can be inserted/deleted \n",
    "\n",
    "We build the source side by inspecting a lexicon\n",
    "\n",
    "* terminal rules: \\\\(X \\rightarrow x\\\\) where \\\\(x \\in \\Sigma\\\\)\n",
    "* binary rules: \\\\(X \\rightarrow X ~ X\\\\)\n",
    "* start rule: \\\\(S \\rightarrow X\\\\)\n",
    "\n",
    "Then, when the time comes, we will project this source grammar using the lexicon\n",
    "\n",
    "* terminal rules of the form \\\\(X_{i,j} \\rightarrow x\\\\) will become \\\\(X_{i,j} \\rightarrow y\\\\) for every possible translation pair \\\\((x, y)\\\\) in the lexicon\n",
    "* binary rules of the form \\\\(X_{i,k} \\rightarrow X_{i,j} ~ X_{j,k}\\\\) will be copied and also inverted as in \\\\(X_{i,k} \\rightarrow X_{j,k} ~ X_{i,j}\\\\)\n",
    "* the start rule will be copied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_lexicon(path):\n",
    "    \"\"\"\n",
    "    Read translation dictionary from a file (one word pair per line) and return a dictionary\n",
    "    mapping x \\in \\Sigma to a set of y \\in \\Delta\n",
    "    \"\"\"\n",
    "    lexicon = defaultdict(set)\n",
    "    with open(path) as istream:        \n",
    "        for n, line in enumerate(istream):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            words = line.split()\n",
    "            if len(words) != 2:\n",
    "                raise ValueError('I expected a word pair in line %d, got %s' % (n, line))\n",
    "            x, y = words\n",
    "            lexicon[x].add(y)\n",
    "    return lexicon\n",
    "            \n",
    "def make_source_side_itg(lexicon, s_str='S', x_str='X') -> CFG:\n",
    "    \"\"\"Constructs the source side of an ITG from a dictionary\"\"\"\n",
    "    S = Nonterminal(s_str)\n",
    "    X = Nonterminal(x_str)\n",
    "    def iter_rules():\n",
    "        yield Rule(S, [X])  # Start: S -> X\n",
    "        yield Rule(X, [X, X])  # Segment: X -> X X\n",
    "        for x in lexicon.keys():\n",
    "            yield Rule(X, [Terminal(x)])  # X - > x  \n",
    "    return CFG(iter_rules())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a lexicon and a source CFG for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = defaultdict(set)\n",
    "lexicon['le'].update(['the', '-EPS-'])  # we will assume that `le` can be deleted\n",
    "lexicon['-EPS-'].update(['a', 'the'])  # we will assume that `the` and `a` can be inserted\n",
    "lexicon['e'].add('and')\n",
    "lexicon['chien'].add('dog')\n",
    "lexicon['noir'].update(['black', 'noir'])  \n",
    "lexicon['blanc'].add('white')\n",
    "lexicon['petit'].update(['small', 'little'])\n",
    "lexicon['petite'].update(['small', 'little'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_cfg = make_source_side_itg(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S] ||| [X]\n",
      "[X] ||| [X] [X]\n",
      "[X] ||| 'le'\n",
      "[X] ||| '-EPS-'\n",
      "[X] ||| 'petite'\n",
      "[X] ||| 'chien'\n",
      "[X] ||| 'blanc'\n",
      "[X] ||| 'petit'\n",
      "[X] ||| 'noir'\n",
      "[X] ||| 'e'\n"
     ]
    }
   ],
   "source": [
    "print(src_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically represent sentences using finite-state automata, this allows for a more general view of parsing.\n",
    "Let's define an FSA class and a function to instantiate the FSA that corresponds to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FSA:\n",
    "    \"\"\"\n",
    "    A container for arcs. This implements a deterministic unweighted FSA.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # each state is represented as a collection of outgoing arcs\n",
    "        # which are organised in a dictionary mapping a label to a destination state\n",
    "        self._states = []\n",
    "        self._initial = set()\n",
    "        self._final = set()\n",
    "        \n",
    "    def nb_states(self):\n",
    "        \"\"\"Number of states\"\"\"\n",
    "        return len(self._states)\n",
    "    \n",
    "    def nb_arcs(self):\n",
    "        \"\"\"Number of arcs\"\"\"\n",
    "        return sum(len(outgoing) for outgoing in self._states)\n",
    "    \n",
    "    def add_state(self, initial=False, final=False) -> int:\n",
    "        \"\"\"Add a state marking it as initial and/or final and return its 0-based id\"\"\"\n",
    "        sid = len(self._states)\n",
    "        self._states.append(defaultdict(int))\n",
    "        if initial:\n",
    "            self.make_initial(sid)\n",
    "        if final:\n",
    "            self.make_final(sid)\n",
    "        return sid\n",
    "    \n",
    "    def add_arc(self, origin, destination, label: str):\n",
    "        \"\"\"Add an arc between `origin` and `destination` with a certain label (states should be added before calling this method)\"\"\"\n",
    "        outgoing = self._states[origin]\n",
    "        outgoing[label] = destination\n",
    "    \n",
    "    def destination(self, origin: int, label: str) -> int:\n",
    "        \"\"\"Return the destination from a certain `origin` state with a certain `label` (-1 means no destination available)\"\"\"\n",
    "        if origin >= len(self._states):\n",
    "            return -1\n",
    "        outgoing = self._states[origin] \n",
    "        if not outgoing:\n",
    "            return -1\n",
    "        return outgoing.get(label, -1)\n",
    "    \n",
    "    def make_initial(self, state: int):\n",
    "        \"\"\"Mark a state as initial\"\"\"\n",
    "        self._initial.add(state)\n",
    "        \n",
    "    def is_initial(self, state: int) -> bool:\n",
    "        \"\"\"Test whether a state is initial\"\"\"\n",
    "        return state in self._initial\n",
    "        \n",
    "    def make_final(self, state: int):\n",
    "        \"\"\"Mark a state as final/accepting\"\"\"\n",
    "        self._final.add(state)\n",
    "        \n",
    "    def is_final(self, state: int) -> bool:\n",
    "        \"\"\"Test whether a state is final/accepting\"\"\"\n",
    "        return state in self._final\n",
    "        \n",
    "    def iterinitial(self):\n",
    "        \"\"\"Iterates over initial states\"\"\"\n",
    "        return iter(self._initial)\n",
    "    \n",
    "    def iterfinal(self):\n",
    "        \"\"\"Iterates over final states\"\"\"\n",
    "        return iter(self._final)\n",
    "    \n",
    "    def iterarcs(self, origin: int):\n",
    "        return self._states[origin].items() if origin < len(self._states) else []\n",
    "    \n",
    "    def __str__(self):\n",
    "        lines = ['states=%d' % self.nb_states(), \n",
    "                 'initial=%s' % ' '.join(str(s) for s in self._initial),\n",
    "                 'final=%s' % ' '.join(str(s) for s in self._final),\n",
    "                 'arcs=%d' % self.nb_arcs()]        \n",
    "        for origin, arcs in enumerate(self._states):\n",
    "            for label, destination in sorted(arcs.items(), key=lambda pair: pair[1]):            \n",
    "                lines.append('origin=%d destination=%d label=%s' % (origin, destination, label))\n",
    "        return '\\n'.join(lines)\n",
    "        \n",
    "def make_fsa(string: str) -> FSA:\n",
    "    \"\"\"Converts a sentence (string) to an FSA (labels are python str objects)\"\"\"\n",
    "    fsa = FSA()\n",
    "    fsa.add_state(initial=True)\n",
    "    for i, word in enumerate(string.split()):\n",
    "        fsa.add_state()  # create a destination state \n",
    "        fsa.add_arc(i, i + 1, word)  # label the arc with the current word\n",
    "    fsa.make_final(fsa.nb_states() - 1)\n",
    "    return fsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use our FSA class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=4\n",
      "initial=0\n",
      "final=3\n",
      "arcs=3\n",
      "origin=0 destination=1 label=le\n",
      "origin=1 destination=2 label=chien\n",
      "origin=2 destination=3 label=noir\n"
     ]
    }
   ],
   "source": [
    "print(make_fsa('le chien noir'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deductive system\n",
    "\n",
    "We implement our parser using a deductive system.\n",
    "\n",
    "## Items\n",
    "\n",
    "First we represent the items of our deductive system (again immutable objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An item in a CKY/Earley program.\n",
    "\"\"\"\n",
    "\n",
    "class Item:\n",
    "    pass\n",
    "\n",
    "class Item:\n",
    "    \"\"\"A dotted rule used in CKY/Earley where dots store the intersected FSA states.\"\"\"\n",
    "\n",
    "    def __init__(self, rule: Rule, dots: list):\n",
    "        assert len(dots) > 0, 'I do not accept an empty list of dots'\n",
    "        self._rule = rule\n",
    "        self._dots = tuple(dots)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return type(self) == type(other) and self._rule == other._rule and self._dots == other._dots\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not(self == other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self._rule, self._dots))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{0} ||| {1}'.format(self._rule, self._dots)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '{0} ||| {1}'.format(self._rule, self._dots)\n",
    "\n",
    "    @property\n",
    "    def lhs(self) -> Symbol:\n",
    "        return self._rule.lhs\n",
    "\n",
    "    @property\n",
    "    def rule(self) -> Rule:\n",
    "        return self._rule\n",
    "\n",
    "    @property\n",
    "    def dot(self) -> int:\n",
    "        return self._dots[-1]\n",
    "\n",
    "    @property\n",
    "    def start(self) -> int:\n",
    "        return self._dots[0]\n",
    "\n",
    "    @property\n",
    "    def next(self) -> Symbol:\n",
    "        \"\"\"return the symbol to the right of the dot (or None, if the item is complete)\"\"\"\n",
    "        if self.is_complete():\n",
    "            return None\n",
    "        return self._rule.rhs[len(self._dots) - 1]\n",
    "\n",
    "    def state(self, i) -> int:\n",
    "        \"\"\"The state associated with the ith dot\"\"\"\n",
    "        return self._dots[i]\n",
    "\n",
    "    def advance(self, dot) -> Item:\n",
    "        \"\"\"return a new item with an extended sequence of dots\"\"\"\n",
    "        return Item(self._rule, self._dots + (dot,))\n",
    "\n",
    "    def is_complete(self) -> bool:\n",
    "        \"\"\"complete items are those whose dot reached the end of the RHS sequence\"\"\"\n",
    "        return len(self._rule.rhs) + 1 == len(self._dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Next we define an agenda of active/passive items. \n",
    "Agendas are much like queues, but with some added functionality (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agenda of active/passive items in CKY/Ealery program.\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class Agenda:\n",
    "\n",
    "    def __init__(self):\n",
    "        # we are organising active items in a stack (last in first out)\n",
    "        self._active = deque([])\n",
    "        # an item should never queue twice, thus we will manage a set of items which we have already seen\n",
    "        self._seen = set()\n",
    "        # we organise incomplete items by the symbols they wait for at a certain position\n",
    "        # that is, if the key is a pair (Y, i)\n",
    "        # the value is a set of items of the form\n",
    "        # [X -> alpha * Y beta, [...i]]\n",
    "        self._incomplete = defaultdict(set)\n",
    "        # we organise complete items by their LHS symbol spanning from a certain position\n",
    "        # if the key is a pair (X, i)\n",
    "        # then the value is a set of items of the form\n",
    "        # [X -> gamma *, [i ... j]]\n",
    "        self._complete = defaultdict(set)\n",
    "        # here we store the destinations already discovered\n",
    "        self._destinations = defaultdict(set)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"return the number of active items\"\"\"\n",
    "        return len(self._active)\n",
    "\n",
    "    def push(self, item: Item):\n",
    "        \"\"\"push an item into the queue of active items\"\"\"\n",
    "        if item not in self._seen:  # if an item has been seen before, we simply ignore it\n",
    "            self._active.append(item)\n",
    "            self._seen.add(item)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def pop(self) -> Item:\n",
    "        \"\"\"pop an active item\"\"\"\n",
    "        assert len(self._active) > 0, 'I have no items left.'\n",
    "        return self._active.pop()\n",
    "\n",
    "    def make_passive(self, item: Item):\n",
    "        \"\"\"Store an item as passive: complete items are part of the chart, incomplete items are waiting for completion.\"\"\"\n",
    "        if item.is_complete():  # complete items offer a way to rewrite a certain LHS from a certain position\n",
    "            self._complete[(item.lhs, item.start)].add(item)\n",
    "            self._destinations[(item.lhs, item.start)].add(item.dot)\n",
    "        else:  # incomplete items are waiting for the completion of the symbol to the right of the dot\n",
    "            self._incomplete[(item.next, item.dot)].add(item)\n",
    "\n",
    "    def waiting(self, symbol: Symbol, dot: int) -> set:\n",
    "        \"\"\"return items waiting for `symbol` spanning from `dot`\"\"\"\n",
    "        return self._incomplete.get((symbol, dot), set())\n",
    "\n",
    "    def complete(self, lhs: Symbol, start: int) -> set:\n",
    "        \"\"\"return complete items whose LHS symbol is `lhs` spanning from `start`\"\"\"\n",
    "        return self._complete.get((lhs, start), set())\n",
    "    \n",
    "    def destinations(self, lhs: Symbol, start: int) -> set:\n",
    "        \"\"\"return destinations (in the FSA) for `lhs` spanning from `start`\"\"\"\n",
    "        return self._destinations.get((lhs, start), set())\n",
    "\n",
    "    def itercomplete(self):\n",
    "        \"\"\"an iterator over complete items in arbitrary order\"\"\"\n",
    "        for items in self._complete.itervalues():\n",
    "            for item in items:\n",
    "                yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference rules\n",
    "\n",
    "Now, let's implement an Earley parser. It is based on a set of *axioms* and 3 inference rules (i.e. *predict*, *scan*, and *complete*).\n",
    "\n",
    "The strategy we adopt here is to design a function for each inference rule which\n",
    "* may consult the agenda, but not alter it\n",
    "* infers and returns a list of potential consequents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axioms(cfg: CFG, fsa: FSA, s: Symbol) -> list:\n",
    "    \"\"\"\n",
    "    Axioms for Earley.\n",
    "\n",
    "    Inference rule:\n",
    "        -------------------- (S -> alpha) \\in R and q0 \\in I\n",
    "        [S -> * alpha, [q0]] \n",
    "        \n",
    "    R is the rule set of the grammar.\n",
    "    I is the set of initial states of the automaton.\n",
    "\n",
    "    :param cfg: a CFG\n",
    "    :param fsa: an FSA\n",
    "    :param s: the CFG's start symbol (S)\n",
    "    :returns: a list of items that are Earley axioms  \n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for q0 in fsa.iterinitial():\n",
    "        for rule in cfg.get(s):\n",
    "            items.append(Item(rule, [q0]))\n",
    "    return items\n",
    "\n",
    "def predict(cfg: CFG, item: Item) -> list:\n",
    "    \"\"\"\n",
    "    Prediction for Earley.\n",
    "\n",
    "    Inference rule:\n",
    "        [X -> alpha * Y beta, [r, ..., s]]\n",
    "        --------------------   (Y -> gamma) \\in R\n",
    "        [Y -> * gamma, [s]] \n",
    "        \n",
    "    R is the ruleset of the grammar.\n",
    "\n",
    "    :param item: an active Item\n",
    "    :returns: a list of predicted Items or None  \n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for rule in cfg.get(item.next):\n",
    "        items.append(Item(rule, [item.dot]))\n",
    "    return items\n",
    "\n",
    "def scan(fsa: FSA, item: Item, eps_symbol: Terminal=Terminal('-EPS-')) -> list:\n",
    "    \"\"\"\n",
    "    Scan a terminal (compatible with CKY and Earley).\n",
    "\n",
    "    Inference rule:\n",
    "\n",
    "        [X -> alpha * x beta, [q, ..., r]]\n",
    "        ------------------------------------    where (r, x, s) \\in FSA and x != \\epsilon\n",
    "        [X -> alpha x * beta, [q, ..., r, s]]\n",
    "        \n",
    "        \n",
    "    If x == \\epsilon, we have a different rule\n",
    "    \n",
    "        [X -> alpha * \\epsilon beta, [q, ..., r]]\n",
    "        ---------------------------------------------   \n",
    "        [X -> alpha \\epsilon * beta, [q, ..., r, r]]\n",
    "    \n",
    "    that is, the dot moves over the empty string and we loop into the same FSA state (r)\n",
    "\n",
    "    :param item: an active Item\n",
    "    :param eps_symbol: a list/tuple of terminals (set to None to disable epsilon rules)\n",
    "    :returns: scanned items\n",
    "    \"\"\"\n",
    "    assert item.next.is_terminal(), 'Only terminal symbols can be scanned, got %s' % item.next\n",
    "    if eps_symbol and item.next.root() == eps_symbol:\n",
    "        return [item.advance(item.dot)]\n",
    "    else:\n",
    "        destination = fsa.destination(origin=item.dot, label=item.next.root().obj())  # we call .obj() because labels are strings, not Terminals\n",
    "        if destination < 0:  # cannot scan the symbol from this state\n",
    "            return []\n",
    "        return [item.advance(destination)]\n",
    "        \n",
    "def complete(agenda: Agenda, item: Item):\n",
    "    \"\"\"\n",
    "    Move dot over nonterminals (compatible with CKY and Earley).\n",
    "\n",
    "    Inference rule:\n",
    "\n",
    "        [X -> alpha * Y beta, [i ... k]] [Y -> gamma *, [k ... j]]\n",
    "        ----------------------------------------------------------\n",
    "                 [X -> alpha Y * beta, [i ... j]]\n",
    "\n",
    "    :param item: an active Item.\n",
    "        if `item` is complete, we advance the dot of incomplete passive items to `item.dot`\n",
    "        otherwise, we check whether we know a set of positions J = {j1, j2, ..., jN} such that we can\n",
    "        advance this item's dot to.\n",
    "    :param agenda: an instance of Agenda\n",
    "    :returns: a list of items\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    if item.is_complete():\n",
    "        # advance the dot for incomplete items waiting for item.lhs spanning from item.start\n",
    "        for incomplete in agenda.waiting(item.lhs, item.start):\n",
    "            items.append(incomplete.advance(item.dot))\n",
    "    else:\n",
    "        # look for completions of item.next spanning from item.dot\n",
    "        for destination in agenda.destinations(item.next, item.dot):                \n",
    "            items.append(item.advance(destination))\n",
    "    return items\n",
    "    \n",
    "def earley(cfg: CFG, fsa: FSA, start_symbol: Symbol, sprime_symbol=None, eps_symbol=Terminal('-EPS-')):\n",
    "    \"\"\"\n",
    "    Earley intersection between a CFG and an FSA.\n",
    "    \n",
    "    :param cfg: a grammar or forest\n",
    "    :param fsa: an acyclic FSA\n",
    "    :param start_symbol: the grammar/forest start symbol\n",
    "    :param sprime_symbol: if specified, the resulting forest will have sprime_symbol as its starting symbol\n",
    "    :param eps_symbol: if not None, the parser will support epsilon rules\n",
    "    :returns: a CFG object representing the intersection between the cfg and the fsa \n",
    "    \"\"\"\n",
    "    \n",
    "    # start an agenda of items\n",
    "    A = Agenda()\n",
    "    \n",
    "    # this is used to avoid a bit of spurious computation\n",
    "    have_predicted = set()\n",
    "\n",
    "    # populate the agenda with axioms\n",
    "    for item in axioms(cfg, fsa, start_symbol):\n",
    "        A.push(item)\n",
    "        \n",
    "    # call inference rules for as long as we have active items in the agenda\n",
    "    while len(A) > 0:  \n",
    "        antecedent = A.pop()\n",
    "        consequents = []\n",
    "        if antecedent.is_complete():  # dot at the end of rule                    \n",
    "            # try to complete other items            \n",
    "            consequents = complete(A, antecedent)\n",
    "        else:\n",
    "            if antecedent.next.is_terminal():  # dot before a terminal \n",
    "                consequents = scan(fsa, antecedent, eps_symbol=eps_symbol)\n",
    "            else:  # dot before a nonterminal\n",
    "                if (antecedent.next, antecedent.dot) not in have_predicted:  # test for spurious computation\n",
    "                    consequents = predict(cfg, antecedent)  # attempt prediction\n",
    "                    have_predicted.add((antecedent.next, antecedent.dot))\n",
    "                else:  # we have already predicted in this context, let's attempt completion\n",
    "                    consequents = complete(A, antecedent)\n",
    "        for item in consequents:            \n",
    "            A.push(item)\n",
    "        # mark this antecedent as processed\n",
    "        A.make_passive(antecedent)\n",
    "\n",
    "    def iter_intersected_rules():\n",
    "        \"\"\"\n",
    "        Here we convert complete items into CFG rules.\n",
    "        This is a top-down process where we visit complete items at most once.\n",
    "        \"\"\"\n",
    "        \n",
    "        # in the agenda, items are organised by \"context\" where a context is a tuple (LHS, start state)\n",
    "        to_do = deque()  # contexts to be processed\n",
    "        discovered_set = set()  # contexts discovered\n",
    "        top_symbols = []  # here we store tuples of the kind (start_symbol, initial state, final state)\n",
    "        \n",
    "        # we start with items that rewrite the start_symbol from an initial FSA state\n",
    "        for q0 in fsa.iterinitial():\n",
    "            to_do.append((start_symbol, q0))  # let's mark these as discovered\n",
    "            discovered_set.add((start_symbol, q0))\n",
    "                        \n",
    "        # for as long as there are rules to be discovered\n",
    "        while to_do:\n",
    "            nonterminal, start = to_do.popleft()                             \n",
    "            # give every complete item matching the context above a chance to yield a rule\n",
    "            for item in A.complete(nonterminal, start):\n",
    "                # create a new LHS symbol based on intersected states\n",
    "                lhs = Span(item.lhs, item.start, item.dot)\n",
    "                # if LHS is the start_symbol, then we must respect FSA initial/final states\n",
    "                # also, we must remember to add a goal rule for this\n",
    "                if item.lhs == start_symbol:\n",
    "                    if not (fsa.is_initial(start) and fsa.is_final(item.dot)):\n",
    "                        continue  # we discard this item because S can only span from initial to final in FSA                        \n",
    "                    else:\n",
    "                        top_symbols.append(lhs)\n",
    "                # create new RHS symbols based on intersected states\n",
    "                #  and update discovered set\n",
    "                rhs = []\n",
    "                for i, sym in enumerate(item.rule.rhs):\n",
    "                    context = (sym, item.state(i))\n",
    "                    if not sym.is_terminal() and context not in discovered_set:\n",
    "                        to_do.append(context)  # book this nonterminal context\n",
    "                        discovered_set.add(context)  # mark as discovered\n",
    "                    # create a new RHS symbol based on intersected states\n",
    "                    rhs.append(Span(sym, item.state(i), item.state(i + 1)))\n",
    "                yield Rule(lhs, rhs)\n",
    "        if sprime_symbol:\n",
    "            for lhs in top_symbols:\n",
    "                yield Rule(sprime_symbol, [lhs])\n",
    "    # return the intersected CFG :)\n",
    "    return CFG(iter_intersected_rules())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=3\n",
      "initial=0\n",
      "final=2\n",
      "arcs=2\n",
      "origin=0 destination=1 label=petit\n",
      "origin=1 destination=2 label=chien\n",
      "states=3\n",
      "initial=0\n",
      "final=2\n",
      "arcs=2\n",
      "origin=0 destination=1 label=petit\n",
      "origin=1 destination=2 label=chien\n",
      "[S] ||| [X]\n",
      "[X] ||| [X] [X]\n",
      "[X] ||| 'le'\n",
      "[X] ||| '-EPS-'\n",
      "[X] ||| 'petite'\n",
      "[X] ||| 'chien'\n",
      "[X] ||| 'blanc'\n",
      "[X] ||| 'petit'\n",
      "[X] ||| 'noir'\n",
      "[X] ||| 'e'\n"
     ]
    }
   ],
   "source": [
    "src_str = 'petit chien'\n",
    "src_fsa = make_fsa(src_str)\n",
    "print(src_fsa)\n",
    "print(src_fsa)\n",
    "print(src_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I am going to use [S'] as the new start symbol\n",
    "forest = earley(src_cfg, src_fsa, start_symbol=Nonterminal('S'), sprime_symbol=Nonterminal(\"D(x)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:0-0 ||| '-EPS-':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[X]:2-2 ||| '-EPS-':2-2\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n",
      "[S]:0-2 ||| [X]:0-2\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| 'petit':0-1\n",
      "[D(x)] ||| [S]:0-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-2 ||| 'chien':1-2\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[X]:1-1 ||| '-EPS-':1-1\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n"
     ]
    }
   ],
   "source": [
    "print(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target side of the ITG\n",
    "\n",
    "Now we can project the forest onto the target vocabulary by using ITG rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_target_side_itg(source_forest: CFG, lexicon: dict) -> CFG:\n",
    "    \"\"\"Constructs the target side of an ITG from a source forest and a dictionary\"\"\"    \n",
    "    def iter_rules():\n",
    "        for lhs, rules in source_forest.items():            \n",
    "            for r in rules:\n",
    "                if r.arity == 1:  # unary rules\n",
    "                    if r.rhs[0].is_terminal():  # terminal rules\n",
    "                        x_str = r.rhs[0].root().obj()  # this is the underlying string of a Terminal\n",
    "                        targets = lexicon.get(x_str, set())\n",
    "                        if not targets:\n",
    "                            pass  # TODO: do something with unknown words?\n",
    "                        else:\n",
    "                            for y_str in targets:\n",
    "                                yield Rule(r.lhs, [r.rhs[0].translate(y_str)])  # translation\n",
    "                    else:\n",
    "                        yield r  # nonterminal rules\n",
    "                elif r.arity == 2:\n",
    "                    yield r  # monotone\n",
    "                    if r.rhs[0] != r.rhs[1]:  # avoiding some spurious derivations by blocking invertion of identical spans\n",
    "                        yield Rule(r.lhs, [r.rhs[1], r.rhs[0]])  # inverted\n",
    "                else:\n",
    "                    raise ValueError('ITG rules are unary or binary, got %r' % r)        \n",
    "    return CFG(iter_rules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is D(x)\n",
    "projected_forest = make_target_side_itg(forest, lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:0-0 ||| 'a':0-0\n",
      "[X]:0-0 ||| 'the':0-0\n",
      "[X]:0-0 ||| [X]:0-0 [X]:0-0\n",
      "[X]:2-2 ||| 'a':2-2\n",
      "[X]:2-2 ||| 'the':2-2\n",
      "[X]:2-2 ||| [X]:2-2 [X]:2-2\n",
      "[S]:0-2 ||| [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-2 [X]:2-2\n",
      "[X]:0-2 ||| [X]:2-2 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-0 [X]:0-2\n",
      "[X]:0-2 ||| [X]:0-2 [X]:0-0\n",
      "[X]:0-2 ||| [X]:0-1 [X]:1-2\n",
      "[X]:0-2 ||| [X]:1-2 [X]:0-1\n",
      "[X]:0-1 ||| [X]:0-0 [X]:0-1\n",
      "[X]:0-1 ||| [X]:0-1 [X]:0-0\n",
      "[X]:0-1 ||| [X]:0-1 [X]:1-1\n",
      "[X]:0-1 ||| [X]:1-1 [X]:0-1\n",
      "[X]:0-1 ||| 'small':0-1\n",
      "[X]:0-1 ||| 'little':0-1\n",
      "[D(x)] ||| [S]:0-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:2-2\n",
      "[X]:1-2 ||| [X]:2-2 [X]:1-2\n",
      "[X]:1-2 ||| 'dog':1-2\n",
      "[X]:1-2 ||| [X]:1-1 [X]:1-2\n",
      "[X]:1-2 ||| [X]:1-2 [X]:1-1\n",
      "[X]:1-1 ||| [X]:1-1 [X]:1-1\n",
      "[X]:1-1 ||| 'a':1-1\n",
      "[X]:1-1 ||| 'the':1-1\n"
     ]
    }
   ],
   "source": [
    "print(projected_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(projected_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=3\n",
      "initial=0\n",
      "final=2\n",
      "arcs=2\n",
      "origin=0 destination=1 label=little\n",
      "origin=1 destination=2 label=dog\n"
     ]
    }
   ],
   "source": [
    "tgt_str = 'little dog'\n",
    "tgt_fsa = make_fsa(tgt_str)\n",
    "print(tgt_fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is D(x, y)\n",
    "ref_forest = earley(projected_forest, tgt_fsa, start_symbol=Nonterminal(\"D(x)\"), sprime_symbol=Nonterminal('D(x,y)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]:1-2:1-2 ||| 'dog':1-2:1-2\n",
      "[S]:0-2:0-2 ||| [X]:0-2:0-2\n",
      "[D(x,y)] ||| [D(x)]:0-2\n",
      "[X]:0-2:0-2 ||| [X]:0-1:0-1 [X]:1-2:1-2\n",
      "[D(x)]:0-2 ||| [S]:0-2:0-2\n",
      "[X]:0-1:0-1 ||| 'little':0-1:0-1\n"
     ]
    }
   ],
   "source": [
    "print(ref_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legth constraint\n",
    "\n",
    "To constrain the space of derivations by length we can parse a special FSA using the forest that represents \\\\(D(x)\\\\), i.e. `tgt_forest` in the code above.\n",
    "\n",
    "For maximum lenght \\\\(n\\\\), this special FSA must accept the language \\\\(\\Sigma^0 \\cup \\Sigma^1 \\cup \\cdots \\cup \\Sigma^n\\\\). You can implement this FSA by designing a specialisation of the FSA class which never rejects a terminal (for example by defining a *wildcard* symbol). No changes to the parser are necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological sort\n",
    "\n",
    "An acyclic hypergraph defines a partial order of its nodes. *Topsort* is an algorithm that can compute this partial order in linear time (on the number of nodes). Algorithms such as *Inside* and *Outside* are typically easier if one first top-sorts the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside weights\n",
    "\n",
    "The inside recursion accumulates the weight of all subtrees under a certain node.\n",
    "\n",
    "\\begin{equation}\n",
    "    I(v) = \n",
    "    \\begin{cases}\n",
    "        1 \\quad \\text{if } v \\text{ is terminal }\\\\\n",
    "        0 \\quad \\text{if } v \\text{ is nonterminal and } BS(v) = \\emptyset \\\\\n",
    "        \\sum_{e \\in BS(v)} w(e) \\prod_{u \\in tail(e)} I(u) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "                                        \n",
    "Here we are going to compute inside weights for acyclic forests, for a more general treatment see Goodman's \"Semiring Parsing\" paper (1999).\n",
    "\n",
    "Inside weights can be used, for instance, to answer the question:\n",
    "\n",
    "* what is the probability of sentence x?\n",
    "\n",
    "It can also be used to find the best derivation and to sample derivations, as we will show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting derivations\n",
    "\n",
    "Another interesting question is\n",
    "\n",
    "* how many analyses of a given sentence do we have?\n",
    "\n",
    "This question is very simple to answer for **acyclic hypergraphs** and it turns out to be a special case of the inside recursion.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    I(v) = \n",
    "    \\begin{cases}\n",
    "        1 \\quad \\text{if } v \\text{ is terminal }\\\\\n",
    "        0 \\quad \\text{if } v \\text{ is nonterminal and } BS(v) = \\emptyset \\\\\n",
    "        \\sum_{e \\in BS(v)} \\prod_{u \\in tail(e)} N(u) \\quad \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "                             \n",
    "\n",
    "Compare the definition above with the inside recursion presented earlier.\n",
    "Also compare the program below with the inside computation and comment on the differences.\n",
    "\n",
    "Can you explain this recursion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi (best derivation)\n",
    "\n",
    "We might want to know which analysis score highest. Once we have computed inside weights, this is extremely simple to solve.\n",
    "However, we can also define a recursion which is specific for the computation of the Viterbi derivation. Do you think you can come up with its formula? Can you implement it?\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
