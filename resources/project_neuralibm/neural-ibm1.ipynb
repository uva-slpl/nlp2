{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1\n",
    "\n",
    "NLP2 2016/2017 Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'], ['36', 'e', 'Législature', ',', '2', 'ième', 'Session'])\n",
      "(['edited', 'HANSARD', '*', 'NUMBER', '1'], ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1'])\n",
      "(['contents'], ['table', 'DES', 'MATIÈRES'])\n",
      "(['Tuesday', ',', 'October', '12', ',', '1999'], ['le', 'mardi', '12', 'octobre', '1999'])\n"
     ]
    }
   ],
   "source": [
    "# check utils.py if you want to see how smart_reader and bitext_reader work in detail\n",
    "from utils import smart_reader, bitext_reader\n",
    "\n",
    "    \n",
    "def bitext_reader_demo(src_path, trg_path):\n",
    "  \"\"\"Demo of the bitext reader.\"\"\"\n",
    " \n",
    "  # create a reader\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "  # to see that it really works, try this:\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))  \n",
    "\n",
    "\n",
    "bitext_reader_demo(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 178928 sentences with max_length = 30\n"
     ]
    }
   ],
   "source": [
    "# To see how many sentences are left if you filter by length, you can do this:\n",
    "\n",
    "def demo_number_filtered_sentence_pairs(src_path, trg_path):\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  max_length = 30\n",
    "  bitext = bitext_reader(src_reader, trg_reader, max_length=max_length)   \n",
    "  num_sentences = sum([1 for _ in bitext])\n",
    "  print(\"There are {} sentences with max_length = {}\".format(num_sentences, max_length))\n",
    "  \n",
    "  \n",
    "demo_number_filtered_sentence_pairs(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create a vocabulary!\n",
    "\n",
    "We first define a class `Vocabulary` that helps us convert tokens (words) into numbers. This is useful later, because then we can e.g. index a word embedding table using the ID of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check vocabulary.py to see how the Vocabulary class is defined\n",
    "from vocabulary import OrderedCounter, Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our Vocabulary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 36640\n",
      "Trimmed vocabulary size: 1005\n",
      "The index of \"<PAD>\" is: 0\n",
      "The index of \"<UNK>\" is: 1\n",
      "The index of \"the\" is: 5\n",
      "The token with index 0 is: <PAD>\n",
      "The token with index 1 is: <UNK>\n",
      "The token with index 2 is: <S>\n",
      "The token with index 3 is: </S>\n",
      "The token with index 4 is: <NULL>\n",
      "The token with index 5 is: the\n",
      "The token with index 6 is: .\n",
      "The token with index 7 is: ,\n",
      "The token with index 8 is: of\n",
      "The token with index 9 is: to\n",
      "The index of \"!@!_not_in_vocab_!@!\" is: 1\n"
     ]
    }
   ],
   "source": [
    "def vocabulary_demo():\n",
    "\n",
    "  # We used up a few lines in the previous example, so we set up\n",
    "  # our data generator again.\n",
    "  corpus = smart_reader(train_e_path)    \n",
    "\n",
    "  # Let's create a vocabulary given our (tokenized) corpus\n",
    "  vocabulary = Vocabulary(corpus=corpus)\n",
    "  print(\"Original vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we only keep the highest-frequency words\n",
    "  vocabulary_size=1000\n",
    "  vocabulary.trim(vocabulary_size)\n",
    "  print(\"Trimmed vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we can get word indexes using v.get_word_id():\n",
    "  for t in [\"<PAD>\", \"<UNK>\", \"the\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "\n",
    "  # And the inverse too, using v.i2t:\n",
    "  for i in range(10):\n",
    "    print(\"The token with index {} is: {}\".format(i, vocabulary.get_token(i)))\n",
    "\n",
    "  # Now let's try to get a word ID for a word not in the vocabulary\n",
    "  # we should get 1 (so, <UNK>)\n",
    "  for t in [\"!@!_not_in_vocab_!@!\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "    \n",
    "    \n",
    "vocabulary_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the vocabularies that we use further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 1005\n",
      "French vocabulary size: 1005\n",
      "\n",
      "A few English words:\n",
      "property\n",
      "reasons\n",
      "Standing\n",
      "all\n",
      "party\n",
      "\n",
      "A few French words:\n",
      "discuter\n",
      "nationale\n",
      "hier\n",
      "déclare\n",
      "internationale\n"
     ]
    }
   ],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "print(\"English vocabulary size: {}\".format(len(vocabulary_e)))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))\n",
    "print(\"French vocabulary size: {}\".format(len(vocabulary_f)))\n",
    "print()\n",
    "\n",
    "\n",
    "def sample_words(vocabulary, n=5):\n",
    "  \"\"\"Print a few words from the vocabulary.\"\"\"\n",
    "  for _ in range(n):\n",
    "    token_id = np.random.randint(0, len(vocabulary) - 1)\n",
    "    print(vocabulary.get_token(token_id))\n",
    "\n",
    "\n",
    "print(\"A few English words:\")\n",
    "sample_words(vocabulary_e, n=5)\n",
    "print()\n",
    "\n",
    "print(\"A few French words:\")\n",
    "sample_words(vocabulary_f, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mini-batching\n",
    "\n",
    "With our vocabulary, we still need a method that converts a whole sentence to a sequence of IDs.\n",
    "And, to speed up training, we would like to get a so-called mini-batch at a time: multiple of such sequences together. So our function takes a corpus iterator and a vocabulary, and returns a mini-batch of shape [Batch, Time], where the first dimension indexes the sentences in the batch, and the second the time steps in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'],\n",
      "  ['36', 'e', 'Législature', ',', '2', 'ième', 'Session']),\n",
      " (['edited', 'HANSARD', '*', 'NUMBER', '1'],\n",
      "  ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1']),\n",
      " (['contents'], ['table', 'DES', 'MATIÈRES']),\n",
      " (['Tuesday', ',', 'October', '12', ',', '1999'],\n",
      "  ['le', 'mardi', '12', 'octobre', '1999'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 745 325   7 262   1   1]\n",
      " [  4   1   1  67   1 238   0   0]\n",
      " [  4   1   0   0   0   0   0   0]\n",
      " [  4   1   7 813 882   7 297   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1   1   1   7 254   1   1]\n",
      " [  1   1  62   1 250   0   0]\n",
      " [  1 463   1   0   0   0   0]\n",
      " [  6   1   1 840 295   0   0]]\n",
      "\n",
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['opening',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   'SECOND',\n",
      "   'SESSION',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   '36',\n",
      "   'TH',\n",
      "   'PARLIAMENT'],\n",
      "  ['ouverture',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   'DEUXIÈME',\n",
      "   'SESSION',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   '36E',\n",
      "   'LÉGISLATURE']),\n",
      " (['oaths', 'OF', 'OFFICE'], ['les', 'SERMENTS', 'De', 'OFFICE']),\n",
      " (['bill', 'C', '-', '1', '.'], ['projet', 'de', 'loi', 'C', '-', '1', '.']),\n",
      " (['introduction', 'and', 'first', 'reading'],\n",
      "  ['présentation', 'et', 'première', 'lecture'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 488 800   1   1 488 800   1   1   1]\n",
      " [  4   1 488   1   0   0   0   0   0   0   0]\n",
      " [  4  63  99  20 238   6   0   0   0   0   0]\n",
      " [  4 931  10 125 385   0   0   0   0   0   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1 239 542   1   1 239 542   1   1]\n",
      " [  9   1   1   1   0   0   0   0   0]\n",
      " [ 40   5  34  94  14 250   8   0   0]\n",
      " [832  13 227 336   0   0   0   0   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_reader = smart_reader(train_e_path)\n",
    "trg_reader = smart_reader(train_f_path)\n",
    "bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "\n",
    "for batch_id, batch in enumerate(iterate_minibatches(bitext, batch_size=4)):\n",
    "\n",
    "  print(\"This is the batch of data that we will train on, as tokens:\")\n",
    "  pprint(batch)\n",
    "  print()\n",
    "\n",
    "  x, y = prepare_data(batch, vocabulary_e, vocabulary_f)\n",
    "\n",
    "  print(\"These are our inputs (i.e. words replaced by IDs):\")\n",
    "  print(x)\n",
    "  print()\n",
    "  \n",
    "  print(\"These are the outputs (the foreign sentences):\")\n",
    "  print(y)\n",
    "  print()\n",
    "\n",
    "  if batch_id > 0:\n",
    "    break  # stop after the first batch, this is just a demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice the following:\n",
    "\n",
    "1. Every English sequence starts with a 4, the ID for < NULL \\>.\n",
    "2. The longest sequence in the batch contains no padding symbols. Any sequences shorter, however, will have padding zeros.\n",
    "\n",
    "With our input pipeline in place, now let's create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check neuralibm1.py for the Model code\n",
    "from neuralibm1 import NeuralIBM1Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Now that we have a model, we need to train it. To do so we define a Trainer class that takes our model as an argument and trains it, keeping track of some important information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check neuralibm1trainer.py for the Trainer code\n",
    "from neuralibm1trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=128 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 54.212376 accuracy 0.20 lr 0.001000\n",
      "Iter   200 loss 55.561035 accuracy 0.20 lr 0.001000\n",
      "Iter   300 loss 56.071678 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 55.794945 accuracy 0.19 lr 0.001000\n",
      "Iter   500 loss 54.081459 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 49.831757 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss 48.379910 accuracy 0.21 lr 0.001000\n",
      "Iter   800 loss 49.563995 accuracy 0.19 lr 0.001000\n",
      "Iter   900 loss 47.163254 accuracy 0.19 lr 0.001000\n",
      "Iter  1000 loss 49.483170 accuracy 0.19 lr 0.001000\n",
      "Iter  1100 loss 43.530159 accuracy 0.20 lr 0.001000\n",
      "Iter  1200 loss 49.092842 accuracy 0.19 lr 0.001000\n",
      "Iter  1300 loss 45.030621 accuracy 0.21 lr 0.001000\n",
      "Epoch 1 loss 51.812266 accuracy 0.19 val_aer 0.45 val_acc 0.18\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 50.025169 accuracy 0.19 lr 0.001000\n",
      "Iter   200 loss 42.010185 accuracy 0.22 lr 0.001000\n",
      "Iter   300 loss 44.619980 accuracy 0.20 lr 0.001000\n",
      "Iter   400 loss 45.875542 accuracy 0.20 lr 0.001000\n",
      "Iter   500 loss 53.600739 accuracy 0.18 lr 0.001000\n",
      "Iter   600 loss 48.202789 accuracy 0.19 lr 0.001000\n",
      "Iter   700 loss 44.158955 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 45.145424 accuracy 0.19 lr 0.001000\n",
      "Iter   900 loss 45.568626 accuracy 0.19 lr 0.001000\n",
      "Iter  1000 loss 44.768608 accuracy 0.20 lr 0.001000\n",
      "Iter  1100 loss 48.583778 accuracy 0.20 lr 0.001000\n",
      "Iter  1200 loss 44.277405 accuracy 0.21 lr 0.001000\n",
      "Iter  1300 loss 46.623184 accuracy 0.20 lr 0.001000\n",
      "Epoch 2 loss 45.228529 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 45.857399 accuracy 0.19 lr 0.001000\n",
      "Iter   200 loss 46.269180 accuracy 0.21 lr 0.001000\n",
      "Iter   300 loss 46.236465 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 46.437416 accuracy 0.20 lr 0.001000\n",
      "Iter   500 loss 41.562038 accuracy 0.18 lr 0.001000\n",
      "Iter   600 loss 45.770592 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss 44.901611 accuracy 0.21 lr 0.001000\n",
      "Iter   800 loss 49.778854 accuracy 0.19 lr 0.001000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=128  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "  \n",
    "  # our model\n",
    "  model = NeuralIBM1Model(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "  \n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer(\n",
    "    model, train_e_path, train_f_path, \n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=10, batch_size=batch_size, \n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
