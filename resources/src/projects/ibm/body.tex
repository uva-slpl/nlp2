
This project will help you familiarise yourself with word-based
models.  Word-based models remain at the core of today's SMT systems
in the form of alignment models.
You will implement the simplest (though still widely used) word-based
models, namely, IBM model 1, a lexical translation model, and IBM model 2, which models an impoverished form of word alignments.

In summary, your task is to

\begin{itemize}
	\item Implement IBM model 1;
	\item Implement IBM model 2 using a jump distribution as in \cite{Vogel+1996:HMMWA};
	\item Experiment with maximum likelihood estimation;
	\item Write a technical report where you present the models and an empirical comparison. Your report should also present learning curves
    where applicable along with a discussion explaining aspects such
    as non-convexity, stability and convergence.
\end{itemize}

\section{IBM model 1}

\begin{enumerate}
	\item 
	\begin{enumerate}[a)]
		\item Implement EM training \citep{Brown+1993:smt} for IBM model 1;
		\item All of the tasks below should be performed for both models.
	\end{enumerate}
	\item Plot the evolution of {\bf training} log likelihood as a function of the iteration. 
	%How to compute the Dirichlet par of the ELBO is described \href{https://github.com/philschulz/PublicWriting/blob/master/DirichletElbo/DirichletELBO.pdf}{here};
	\item Plot the evolution of alignment error rate (AER) on {\bf validation} data as a function of the iteration;
	\item Experiment with two criteria for model selection (i.e. deciding on number of training iterations): 1) convergence in terms of {\bf training log likelihood}; 2) best {\bf AER on validation} data;
	\item For the selected models, obtain Viterbi alignments for every sentence pair in a test corpus and compute AER using a gold-standard provided by the assistant;
\end{enumerate}

%In your report, you should also consider the limitations of Model 1
%as described by \cite{Moore:2004:IBM1}, and find examples in your
%output to illustrate these limitations.

\section{IBM model 2}

\begin{enumerate}
  \item Extend your previous model by implementing a full IBM model 2 \citep{Brown+1993:smt}, however using the cheaper parameterisation in terms of jumps;
  \item IBM 2 is non-convex, thus you will see that optimising the log-likelihood function is 
        not as trivial as in the case of IBM model 1, particularly, convergence will depend 
        on how you initialise the model parameters, you will try
  \begin{itemize}
    \item uniform initialisation
    \item random initialisation (try 3 different starting points)
    \item initialise the lexical parameters using the output of a complete run of model 1
  \end{itemize}
  \item Plot {\bf training} log-likelihood as a function of the iteration for all these cases
  \item Plot {\bf validation} AER as a function of the iteration for all these cases
  \item Select two models: 1) one in terms of {\bf training log likelihood}, 2) another in terms of {\bf validation AER};
  \item Compare the selected models to IBM model 1 in terms of AER in the test set.
\end{enumerate}

%{\bf Extra (optional)} variational Bayes for IBM model 2 (at least for the lexical model).

\section{Data}

All relevant data (including details about file formats) are available from \url{https://uva-slpl.github.io/nlp2/projects.html}.

In this project, you will work with a parallel corpus taken from the Canadian Hansards (parliament proceedings). 
The data consists
of preprocessed sentence pairs (please do not further pre-process the data). There are two files, one for the
English and one for the French sentences. Sentences with the same line number are translations of each other.

We are making available \emph{training} data (which you can use to perform parameter estimation), \emph{validation} data (which you can use to debug your implementation as well as to perform model selection), and finally in due time \emph{test} data (which you will use to conduct your final empirical comparison).

You can use the results in Table \ref{tab:validation} to sanity check your own implementation.

\begin{table}\centering
\begin{tabular}{l | c | p{10cm}}
\bf Model & \bf AER & \bf Training regime \\ \hline
IBM 1 & 0.3378 & 10 iterations \\
IBM 2 & 0.2428 & 10 iterations (lexical component), then 5 additional iterations (lexical and jump components) \\ \hline
\end{tabular}
\caption{\label{tab:validation}Validation results for IBM model 1 and 2 trained for maximum likelihood via EM.}
\end{table}

\section{Report}

You should use latex for your report, and you should use the ACL template available from \url{http://acl2017.org/downloads/acl17-latex.zip} (unlike the template suggests, your submission should not be anonymous). 

We expect short reports (5 pages plus references) written in English. The typical submission is organised as follows: 
\begin{itemize}
	\item abstract: conveys scope and contributions;
	\item introduction: present the problem and relevant background;
	\item model: technical description of models;
	\item experiments: details about the data, experimental setup and findings;
	\item conclusion: a critical take on contributions and limitations.
\end{itemize}


\section{Submission}

You should submit a tgz file containing a folder (folder name {\tt lastname1.lastname2}) with the following content: 
\begin{itemize}
	\item Test predictions (in naacl format) using your best run for each of the following models
	\begin{itemize}
		\item IBM1 MLE (filename: {\tt ibm1.mle.naacl})
		%\item IBM1 VB (filename: {\tt ibm1.vb.naacl})
		\item IBM2 MLE (filename: {\tt ibm2.mle.naacl})
		%\item If you opted for the extra exercise, then also include IBM2 VB (filename: {\tt ibm2.vb.naacl})
	\end{itemize}
	\item Report as a single pdf file (filename: {\tt report.pdf})
\end{itemize}

Your report may contain a link to an open-source repository (such as github), but please do not attach code or additional data to your tgz submission.

You can complete your project submission on Canvas.

%You should email your tgz file to \url{J.C.P.Bastings@uva.nl} no later than {\bf 23:59 GMT-8 on April 26}. If you do not get a confirmation email within 24h after submission, please make sure to contact one of us.

\section{Assessment}

Your report will be assessed according to the following evaluation criteria:
\begin{enumerate}
	\item Scope (max 2 points): Is the problem well presented? Do students understand the challenges/contributions?
	\item Theoretical description (max 3 points): Are the models presented clearly and correctly?
	\item Empirical evaluation (max 3 points): Is the experimental setup sound/convincing? Are experimental findings presented in an organised and effective manner? 
	\item Writing style (max 2 points): use of latex, structure of report, use of tables/figures/plots, command of English.	
	\item Extra (max 1 point).
\end{enumerate}



