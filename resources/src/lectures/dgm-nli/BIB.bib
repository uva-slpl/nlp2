@inproceedings{Blundell:2015:WUN:3045118.3045290,
 author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
 title = {Weight Uncertainty in Neural Networks},
 booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
 series = {ICML'15},
 year = {2015},
 location = {Lille, France},
 pages = {1613--1622},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3045118.3045290},
 acmid = {3045290},
 publisher = {JMLR.org},
} 

@inproceedings{gal2016theoretically,
  title={A theoretically grounded application of dropout in recurrent neural networks},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={1019--1027},
  year={2016}
}

@article{merity2017regularizing,
  title={Regularizing and optimizing LSTM language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}

@inproceedings{chen2017enhanced,
	title={Enhanced lstm for natural language inference},
	author={Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Wei, Si and Jiang, Hui and Inkpen, Diana},
	booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	volume={1},
	pages={1657--1668},
	year={2017}
}

@inproceedings{glockner-etal-2018-breaking,
    title = "Breaking {NLI} Systems with Sentences that Require Simple Lexical Inferences",
    author = "Glockner, Max  and
      Shwartz, Vered  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2103",
    pages = "650--655",
    abstract = "We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.",
}


@article{MacKay:1992:PBF:148147.148165,
 author = {MacKay, David J. C.},
 title = {A Practical Bayesian Framework for Backpropagation Networks},
 journal = {Neural Comput.},
 issue_date = {May 1992},
 volume = {4},
 number = {3},
 month = may,
 year = {1992},
 issn = {0899-7667},
 pages = {448--472},
 numpages = {25},
 url = {http://dx.doi.org/10.1162/neco.1992.4.3.448},
 doi = {10.1162/neco.1992.4.3.448},
 acmid = {148165},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@phdthesis{Neal:1995:BLN:922680,
 author = {Neal, Radford M.},
 advisor = {Hinton, Geoffrey},
 title = {Bayesian Learning for Neural Networks},
 year = {1995},
 isbn = {0-612-02676-0},
 note = {AAINN02676},
 publisher = {University of Toronto},
 address = {Toronto, Ont., Canada, Canada},
} 

@MISC{MacKay95bayesianmethods,
    author = {David J.C. MacKay},
    title = {Bayesian Methods for Neural Networks: Theory and Applications},
    year = {1995}
}


@InProceedings{doprout_gal16,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Yarin Gal and Zoubin Ghahramani},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}