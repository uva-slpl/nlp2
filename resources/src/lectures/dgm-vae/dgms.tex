\section{Deep generative models}


\begin{frame}{Problems}

{\bf Supervised} problems
\begin{center}\emph{``learn a distribution over \textcolor{blue}{observed} data''}\end{center}

\begin{itemize}
	\item \textcolor{black}{sentences in natural language}
	\item images, \ldots
\end{itemize}

~ \pause

{\bf Unsupervised} problems
\begin{center}\emph{``learn a distribution over \textcolor{blue}{observed} and \textcolor{red}{unobserved} data''}\end{center}
\begin{itemize}
	\item \textcolor{black}{sentences in natural language + parse trees}
	\item images + bounding boxes, \ldots
\end{itemize}
\end{frame}


\begin{frame}{Supervised problems}

\small

We have data $x^{(1)}, \ldots, x^{(N)}$ e.g.  \\
\begin{itemize}
	\item sentences, images, ...
\end{itemize}
generated by some {\bf unknown} procedure

\pause

which we assume can be captured by a probabilistic model

\pause

\begin{itemize}
	\item with {\bf known} probability (mass/density) function e.g.
	\begin{align*}
    \underbrace{X \sim \Cat(\alert{\pi_1}, \alert{\ldots}, \alert{\pi_K})}_{\text{e.g. nationality}} & & \text{or} & & \underbrace{X \sim \mathcal N(\alert\mu, \alert\sigma^2)}_{\text{e.g. height}}
    \end{align*}    
\end{itemize}
\pause
\alert{estimate parameters} that assign maximum likelihood to observations

%\pause
%e.g. $\pdv{\pi_j} \sum_{i=1}^N \log \Cat(X=x^{(i)}|\pi_1^K) $\\
%e.g. $\pdv{\mu} \sum_{i=1}^N \log  \mathcal N(X=x^{(i)}|\mu, \sigma^2)$

\end{frame}

\begin{frame}{Multiple problems, same language}



\begin{small}

\begin{columns}
\begin{column}{0.3\textwidth}
\scalebox{0.8}{
\begin{tikzpicture}
\node[obs] (x) {$ x $};
\node[left=of x] (phi) {$ \phi $};

\edge{phi}{x};

\plate {data} {(x)(phi)} {$ N $};
\end{tikzpicture}
}
\end{column}
\begin{column}{0.6\textwidth}
\alert{(Conditional) Density estimation}
\end{column}

\end{columns}

\begin{tabular}{p{2cm} p{4cm} p{4cm}}
 & Side information ($\phi$) & Observation ($x$) \\ \cline{2-3}
Parsing &   \textcolor{black}{a sentence} & \textcolor{blue}{parse tree} \\
&&\\
Translation &  \textcolor{black}{a sentence in French} & \textcolor{blue}{translation in English} \\
&&\\
Captioning &  \textcolor{black}{an image} & \textcolor{blue}{caption in English} \\
&&\\
Entailment  & \textcolor{black}{a text and hypothesis} & \textcolor{blue}{entailment relation}
\end{tabular}
\end{small}

\end{frame}

\begin{frame}{Where does deep learning kick in?}

Let $\phi$ be all side information available\\
~ e.g. deterministic \emph{inputs/features}

~ \pause

Have neural networks predict parameters of our probabilistic model
	\begin{align*}
    X|\phi \sim \Cat(\pi_{\alert \theta}(\phi)) & & \text{or} & & X|\phi \sim \mathcal N(\mu_{\alert \theta}(\phi), \sigma_{\alert \theta}(\phi)^2)
    \end{align*} \pause
~ and proceed to \alert{estimate parameters} $\theta$ of the NNs %via MLE % that assign maximum likelihood to observations

 





\end{frame}


\begin{comment}
\begin{frame}{Task-driven feature extraction}

Often our side information $\phi$ is itself some high dimensional data
\begin{itemize}
	\item $\phi$ is a sentence and $x$ a tree
	\item $\phi$ is the source sentence and $x$ is the target
	\item $\phi$ is an image and $x$ is a caption
\end{itemize}
and part of the job of the NNs that parametrise our models is to also \alert{deterministically} encode that input in a low-dimensional space

\end{frame}
\end{comment}


\begin{frame}{NN as efficient parametrisation}

From the statistical point of view NNs do not generate data\\
\begin{itemize}
	\item \alert{they parametrise distributions} that \\
	\emph{by assumption} govern data
	\item compact and efficient way to \alert{map from complex side information to parameter space}
\end{itemize}

\vspace{10pt}

\pause
Prediction is done by a decision rule outside the statistical model
\begin{itemize}
	\item e.g. beam search
\end{itemize}

\end{frame}

\begin{frame}{Maximum likelihood estimation}

Let $p(x|\theta)$ be the probability of an observation $x$\\
~and $\theta$ refer to all of its parameters \\
~e.g. parameters of NNs involved

~ \pause

Given a dataset $x^{(1)}, \ldots, x^{(N)}$ of i.i.d. observations, the likelihood function 
\begin{equation*}
\begin{aligned}
\mathcal L(\theta|x^{(1:N)}) &= \log \prod_{s=1}^N p(x^{(s)}|\theta) \\ \pause
 &= \sum_{s=1}^N \log p(x^{(s)}|\theta)
\end{aligned}
\end{equation*} \pause
quantifies the fitness of our model to data

\end{frame}

\begin{frame}{MLE via gradient-based optimisation}

If assessing the log-likelihood is {\bf differentiable} and assessing it is {\bf tractable}, then backpropagation can give us the gradient
\begin{equation*}
\begin{aligned}
\grad_\theta \mathcal L(\theta|x^{(1:N)}) &= \grad_\theta \sum_{s=1}^N \log p(x^{(s)}|\theta) \\ \pause
 &=  \sum_{s=1}^N \grad_\theta \log p(x^{(s)}|\theta)
\end{aligned}
\end{equation*}  \pause

and we can update $\theta$ in the direction
\begin{equation*}
\gamma \grad_\theta \mathcal L(\theta|x^{(1:N)})
\end{equation*}
to attain a local optimum of the likelihood function

\end{frame}

\begin{frame}[plain]{Stochastic optimisation}

We can also use a gradient estimate 
\begin{equation*}
\begin{aligned}
\grad_\theta \mathcal L(\theta|x^{(1:N)}) &= \grad_\theta \underbrace{\mathbb E_{S\sim \mathcal U(1..N)}\left[ N \log p(x^{(S)}|\theta)\right]}_{\mathcal L(\theta|x^{(1:N)})} \\ \pause
 &=  \underbrace{\mathbb E_{S\sim \mathcal U(1..N)}\left[ N \grad_\theta  \log p(x^{(S)}|\theta)\right]}_{\text{expected gradient :)}} \\ \pause
 &\overset{\text{MC}}{\approx} \frac{1}{M} \sum_{m=1}^M N  \grad_\theta \log p(x^{(s_i)}|\theta) \\
 &S_i \sim \mathcal U(1..N)
\end{aligned}
\end{equation*}  \pause
and take steps in the direction
\begin{equation*}
\gamma \frac{N}{M} \grad_\theta \mathcal L(\theta|x^{(s_1:s_M)})
\end{equation*}
where $x^{(s_1)}, \ldots, x^{(s_M)}$ is a random mini-batch of size $M$


\end{frame}



\begin{frame}{DL in NLP recipe}


%Vast majority of papers published at ACL

%\begin{small}
%\begin{figure}
%\scalebox{0.8}{
%\begin{tikzpicture}
%\node[obs] (x) {$ x $};
%\node[left=of x] (phi) {$ \phi $};
%\factor[left=of x] {f} {below:$f_w$} {phi} {x} ; 
%%\edge{phi}{x} ;
%\plate {data} {(x)(phi)} {$ N $};
%\end{tikzpicture}
%}
%\end{figure}
%\end{small}
	Maximum likelihood estimation
	\begin{itemize}
		\item  tells you which \alert{loss} to optimise \\
		(i.e. negative log-likelihood)
	\end{itemize}
	
	\pause
	Automatic differentiation (\emph{backprop})
	\begin{itemize}
		\item chain rule of derivatives: ``give me a tractable forward pass and I will give you \alert{gradients}''
	\end{itemize}
	
	\pause
	Stochastic optimisation powered by backprop
	\begin{itemize}
		\item general purpose gradient-based optimisers
	\end{itemize}

\end{frame}


\begin{frame}{Tractability is central}

Likelihood gives us a differentiable objective to optimise for
\begin{itemize}
	\item but we need to stick with \alert{tractable} likelihood functions
\end{itemize}



%, any intractable likelihood will leave us in bad territory because
%\begin{itemize}
%	\item stochastic optimisation requires gradient estimates
%	\item which must be unbiased (forget greedy techniques)
%	\item and some estimation techniques are not differentiable (forget MC sampling)
%\end{itemize}

\end{frame}

\begin{frame}{When do we have intractable likelihood?}

{\bf Unsupervised problems} contain unobserved random variables\\ 
\begin{equation*}
p_\theta(x, z) = \overbrace{p(z)}^{\text{latent variable model}} \underbrace{p_\theta(x|z)}_{\text{observation model}}
\end{equation*}

~ \pause

thus assessing the marginal likelihood requires \alert{marginalisation of latent variables} 
\begin{equation*}
p_\theta(x) = \int p_\theta(x, z) \dd{z} = \int p(z)p_\theta(x|z) \dd{z} 
\end{equation*}


\end{frame}

\begin{frame}{Examples of latent variable models}

Discrete latent variable, continuous observation
\begin{itemize}
	\item too many forward passes
	\begin{small}
	\begin{equation*}
	p_\theta(x) = \sum_{c=1}^K \Cat(c|\pi_1, \ldots, \pi_K) \underbrace{\mathcal N(x|\mu_\theta(c), \sigma_\theta(c)^2)}_{\text{forward pass}}
	\end{equation*}
	\end{small}
\end{itemize}
	\pause
	
Continuous latent variable, discrete observation
\begin{itemize}
	\item infinitely many forward passes
	\begin{small}
	\begin{equation*}
	p_\theta(x) = \int \mathcal N(z|0, I) \underbrace{\Cat(x|\pi_\theta(z))}_{\text{forward pass}} \mathrm{d}z
	\end{equation*}
	\end{small}
\end{itemize}

\end{frame}

\begin{frame}{Deep generative models}

Joint distribution with {\bf deep observation model}
\begin{equation*}
p_\theta(x, z) = \underbrace{p(z)}_{\text{prior}} \underbrace{p_\theta(x|z)}_{\text{likelihood}}
\end{equation*}
~ {\small mapping from latent variable $z$ to $p(x|z)$ is a NN with parameters $\theta$}

~ \pause

Marginal likelihood (or evidence)
\begin{equation*}
p_\theta(x) = \int p_\theta(x, z) \dd{z} = \int p(z)p_\theta(x|z) \dd{z} 
\end{equation*}
~ \alert{intractable} in general



\end{frame}

\begin{frame}[plain]{Gradient}

Exact gradient is intractable
\begin{small}
\begin{equation*}
\begin{aligned}
\grad_\theta \log p_\theta(x) \pause &= \grad_\theta \log \underbrace{\int p_\theta(x, z) \dd{z}}_{\text{marginal}} \\ \pause
&= \underbrace{\frac{1}{\int p_\theta(x, z) \dd{z}} \int \grad_\theta p_\theta(x,z) \dd{z}}_{\text{chain rule}} \\ \pause
&= \frac{1}{p_\theta(x)} \int \underbrace{p_\theta(x,z) \grad_\theta \log p_\theta(x,z)}_{\text{log-identity for derivatives}} \dd{z} \\ \pause
&= \int \underbrace{\frac{p_\theta(x,z)}{p_\theta(x)}}_{\text{posterior}} \grad_\theta \log p_\theta(x,z) \dd{z} \\ \pause
&= \int p_\theta(z|x) \grad_\theta \log p_\theta(x,z) \dd{z} \\ \pause
&= \underbrace{\mathbb E_{p_\theta(z|x)} \left[ \grad_\theta \log p_\theta(x,Z) \right]}_{\text{expected gradient :)}}
\end{aligned}
\end{equation*}
\end{small}



\end{frame}

\begin{frame}{Can we get an estimate?}

\begin{equation*}
\begin{aligned}
\grad_\theta \log p_\theta(x) 
 &= \mathbb E_{p_\theta(z|x)} \left[ \grad_\theta \log p_\theta(x,Z) \right] \\ \pause
 &\overset{\text{MC}}{\approx} \frac{1}{K} \sum_{k=1}^K \grad_\theta \log p_\theta(x,z^{(k)})  \\
 & z^{(k)} \sim p_\theta(Z|x)
\end{aligned}
\end{equation*}


 \pause

MC estimate of gradient requires sampling from posterior
\begin{small}
\begin{equation*}
p_\theta(z|x) = \frac{p(z)p_\theta(x|z)}{\alert{p_\theta(x)}}
\end{equation*}
\end{small}
~ unavailable due to the intractability of the marginal

\end{frame}


\begin{frame}{Summary}

\begin{itemize}
	\item We like probabilistic models because can make explicit modelling assumptions \pause
	\item We want complex observation models 
	parameterised by NNs \pause
	\item But we cannot use backprop for parameter estimation 
\end{itemize}

\pause

We need \alert{approximate inference} techniques!

\end{frame}
